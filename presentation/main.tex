% author: Ondrej Platek 2012
%settings are located in begin_settins.tex and end_settings.tex files
%do not remove!
\input{begin_settings}
\input{python_style} % do not remove

\maketitle
% \note[item]{Good afternoon, ... DO NOT READ IT, POINT AT IT!}


\section{Task} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{Goals of thesis} 
    Improve speech recognition for Alex Spoken Dialogue Systems \\
    Particularly public transport information application (800 899 998).
    \begin{exampleblock}{Goals of the thesis were:}
        \begin{itemize}
            \item to build acoustic models using the Kaldi toolkit,
            \item to develop new real-time recogniser which supports incremental speech recognition,
            \item to integrate the~recogniser into our Alex SDS.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame} \frametitle{Content} \tableofcontents \end{frame}

\section{ASR introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{ASR components} 
    \input{images/asr-components}
    % Architecture of statistical speech recognizer\cite{ney1990acoustic}
\end{frame}

\begin{frame}\frametitle{Acoustic features, features preprocessing} 
    \input{images/mfcc_window}
    \input{images/mfcc-delta}
    % \caption{\ac{PLP} or \ac{MFCC} features are computed every 10 ms seconds in 25 ms windows.
    % Audio length is $(frames-1)*shift + win\_len = 85ms$}
    % \caption{Typical setup with 39 features using \ac{MFCC}.}
\end{frame}

\begin{frame}\frametitle{Continuous Speech recognition} 
    \begin{block}{Pattern matching}
        HMM --- speech time series modelling (phones/triphones for words)
        \begin{itemize}
            \item We trained several HMM acoustic models.
        \end{itemize}
    \end{block}
    \begin{exampleblock}{Graph search - decoding}
        Viterbi algorithm --- dynamic programming
        \begin{itemize}
            \item We search for best parameters (beam, max-active-states).
            \item Normalise its output.
            \item Change interface.
        \end{itemize}
    \end{exampleblock}
\end{frame}


\begin{frame}[fragile]\frametitle{Output formats} 

\begin{verbatim}
    0.5 hi how are you
    0.2 hi where are you
    0.1 bey how are you
\end{verbatim}

% Example of 3-best list output with posterior probability for each path. 
% N-best list in Kaldi can be easily extracted from lattices. 
    \begin{center}
        \includegraphics[width=30em]{toy_lattice.ps}
    \end{center}
    % \caption{Word posterior lattice. 
    %     Common parts of hypotheses are effectively represented. 
    %     All outgoing arcs for each node sum to 1.0. }
\end{frame}

\begin{frame}\frametitle{Evaluation measures} 
    \begin{itemize}
        \item Real Time Factor (RTF) of decoding -- the~ratio of the~recognition time to the~duration of the~audio input,
        \item Latency -- the~delay between utterance end and the~availability of the~recognition results,
        \item Word Error Rate (WER).
    \end{itemize}
\end{frame}


\section[Evaluation in PTI]{Evaluation in Public Transport Information domain}%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{On-line vs batch decoding} 
    \begin{center}
        \includegraphics[scale=0.5]{lat_cloud_kaldi.png}
    \end{center}
    % \caption{Almost constant latency of on-line decoder (OnlineLatgenRecogniser) and linearly growing latency of cloud based speech recogniser (Google ASR service) for increasing utterance length.}
\end{frame}

\begin{frame}\frametitle{Public Transport Information domain - results} 
    \includegraphics[scale=0.4]{beam_vs_rtfwer.pdf.ps}
    \includegraphics[scale=0.4]{latbeam_vs_latwer.pdf.ps}
    % \caption{The~upper graph (a) shows that WER decreases with increasing \term{beam} and the~average RTF linearly grows with the~beam.
    %     The~growth of the~95th RTF percentile is limited at 0.6 by setting \term{max-active-states} to 2000, because the~\term{max-active-states} parameters influence presumably the~worst cases with large search space.
    % The~lower graph (b) shows latency growth in response to increasing \term{lattice-beam}.}
\end{frame}


\section{On-line recogniser} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{Components for on-line decoding} 
    \begin{center}
        \input{images/online_pipeline}
    \end{center}
\end{frame}

\begin{frame}\frametitle{(Py)OnlineLatgenRecogniser interface} 
    \begin{itemize}
        \item \term{AudioIn} -- queueing new audio for pre-processing
        \item \term{Decode} -- decoding a~fixed number of audio frames
        \item \term{PruneFinal} -- preparing internal data structures for lattice extraction
        \item \term{GetLattice} -- extracting a~word posterior lattice
        \item \term{GetBestPath} -- extracting a~one best word sequence
        \item \term{Reset} -- preparing the~recogniser for a~new utterance
    \end{itemize}
\end{frame}

\section{Acoustic modelling} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{Acoustic modeling} 
    \input{images/asr-components-am}
    % Architecture of statistical speech recognizer\cite{ney1990acoustic}
\end{frame}

\begin{frame}\frametitle{Acoustic models training} 
    \begin{center}
        \input{images/am-deps}
        \small{\begin{tabular}{lll}
        \hline
        Training method name & Script shortcut \\
        \hline
        Monophone & mono \\
        Triphone  & tri1 \\
        $\Delta + \Delta\Delta$ & tri2a  \\
        LDA+MLLT & tri2b  \\
        LDA+MLLT+MMI & tri2b\_mmi \\
        LDA+MLLT+bMMI & tri2b\_bmmi \\
        LDA+MLLT+MPE & tri2b\_mpe \\
        \hline
        \end{tabular}}
        % \caption{Training partial order among \ac{AM} in our training scripts}
    \end{center}
\end{frame}

\begin{frame}\frametitle{Vystadial dataset} 
    \begin{center}
    Collected by UFAL Dialogue system group.\\
    \begin{tabular}{lrrr}
        \hline
        dataset & audio[hour] & \# sentences & \# words \\
        \hline
        \textbf{English} & & & \\
                training & 41:30 & 47,463 & 178,110 \\
                development & 01:45 & 2,000 & 7,376 \\
                test & 01:46 & 2,000 & 7,772 \\
        \hline
        \textbf{Czech} & & & \\
                training & 15:25 & 22,567 & 126,333 \\
                development & 01:23 & 2,000 & 11,478 \\
                test & 01:22 & 2,000 & 11,204 \\
        \hline
		\end{tabular}
    % \caption{Size of the~data: length of the~audio (hours:minutes), number of sentences
        % (which is the~same as the~number of recordings), number of words in the~
    % transcriptions.\cite{korvas_2014}}
    \end{center}
\end{frame}

\begin{frame}\frametitle{ASR training results} 
    \begin{center}
    \begin{tabular}{lr}
        \theader{language/method} & \theader{bigram} \\
        \hline \\
        \theader{Czech} & \\
            tri $\Delta+\Delta\Delta$ &  56.6  \\
            tri LDA+MLLT &  53.9 \\
            tri LDA+MLLT+MMI &    49.5 \\
            tri LDA+MLLT+bMMI &   49.3 \\
            tri LDA+MLLT+MPE &    49.2 \\
        \hline \hline \\
        \theader{English} & \\ 
            tri $\Delta+\Delta\Delta$ &   16.2 \\
            tri LDA+MLLT &    15.8 \\
            tri LDA+MLLT+MMI &    10.4  \\
            tri LDA+MLLT+bMMI &   10.2 \\
            tri LDA+MLLT+MPE &    11.1 \\
        \hline
    \end{tabular}
    % \caption{Word error rates for zerogram and bigram LM for different training triphone methods.
    %     The~`tri~$\Delta+\Delta\Delta$' row shows results for a~generative model which is comparable to the~model trained using the~HTK scripts.
    % }
    \end{center}
\end{frame}

\begin{frame}\frametitle{HTK and Kaldi acoustic models} 
    % \begin{block}{HTK results}
        \begin{tabular}{lrr}
            \hline
            \theader{HTK method} & \theader{bigram} \\
            \hline
            \theader{Czech}& \\
                tri $\Delta+\Delta\Delta$  & 60.4\\
            \hline
            \theader{English}& & \\
               tri $\Delta+\Delta\Delta$   & 17.5 \\
            \hline
        \end{tabular}
    % \end{block}
    % \begin{exampleblock}
        \begin{tabular}{lrr}
            \hline
            \theader{Kaldi method} & \theader{bigram} \\
            \hline
            \theader{Czech}& & \\
                tri $\Delta+\Delta\Delta$  &   56.6  \\
            \hline
            \theader{English}& & \\
               tri $\Delta+\Delta\Delta$  &   16.2 \\
            \hline
        \end{tabular}
    % \end{exampleblock}
  % \caption{HTK results: Word error rates on test set are obtained by both a~zerogram and a~bigram LM. The~\acp{AM} can be compared with the~basic \term{tri} $\Delta+\Delta\Delta$ Kaldi setup in~Table~\ref{tab:best}.}
\end{frame}

\begin{frame}\frametitle{Acoustic model accuracy based training data size} 
    \begin{center}
        \includegraphics[scale=0.7]{images/partial-zerogram.ps}
        % \caption{The~figure displays improving performance of Czech generative AM based on growing size of training data for acoustic modelling. The~zerogram LM allows to evaluate only acoustic modelling, but causes a~high WER. }
    \end{center}
\end{frame}

\begin{frame}\frametitle{Speech recognition accuracy based on LM training data size} 
    \begin{center}
        \includegraphics[scale=0.7]{images/partial-lm-tri2b-bmmi.ps}
        % \caption{Influence of in-domain text size of \ac{LM} on speech recognition quality. The~\ac{AM} \term{tri2b\_bmmi} and parameters are fixed and only \ac{LM} training size varies.}
    \end{center}
\end{frame}


\section{Summary} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{Achievements} 
    \begin{itemize}
        \item Working real-time on-line speech recogniser
        \item Developed acoustic modeling scripts for Czech and English - accepted to Kaldi svn trunk
        \item Integration of ASR into Alex Dialogue Systems Framework
        \item Improved speech recognition for toll-free line 800 899 998
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Results} 
    \begin{itemize}
        \item WER 22, latency under 200 ms on Public Transport Information domain (Czech)
        \item WER 50 for Czech on Vystadial dataset (Czech - complex domain)
        \item WER 12 for English on Vystadial dataset 
    \end{itemize}
\end{frame}


\section{Details} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}\frametitle{Functional (Py)OnlineLatgenRecogniser demo} 
    \begin{center}
        \lstinputlisting[style=Python]{pykaldi_usage.py}
    \end{center}
\end{frame}

\begin{frame}\frametitle{Speed - RTF and Latency} 
    Fast enough for 95 \% of utterances. \\
    \includegraphics[scale=0.4]{frtf_vs_prc.pdf.ps}
    \includegraphics[scale=0.4]{lat_vs_prc.pdf.ps}
%     \caption{The~percentile graphs show RTF and Latency scores for test data for \term{max-active-sates}=2000, \term{beam}=13, \term{lattice-beam}=5.
% Note that 95 \% of utterances were decoded with the~latency lower that 200ms.}
\end{frame}


\begin{frame}\frametitle{Problem} 
    Spoken dialogue systems needs speech recognition \\
    OpenJulius --- crashes, PocketSphinx --- no posteriors, RWTH decoder --- license \\
    Cloud based services Google and Nuance --- no customisation + license issues
\end{frame}

\begin{frame} \frametitle{Semiring}
\begin{tabular}{lrrrrr}
\hline
Name & $\mathcal{K}$ & $\oplus$ & $ \otimes$ & $\bar{0}$ & $\bar{1}$ \\ 
\hline
Real        & $[0,\infty)$        &  +                     &  * &  0        &  1  \\
Log         & $(-\infty, \infty)$ & $-log(e^{-x} + e^{-y})$ & + &  $\infty$ &  0  \\
Tropical    & $(-\infty, \infty)$ &  min                   &  + &  $\infty$ &  0  \\
\hline
\end{tabular}
% \caption{Semirings used in speech recognition.\cite{openfst_web}}
\end{frame}

\begin{frame}\frametitle{Beam search - Viterbi}
    \begin{center}
        \includegraphics[width=30em]{viterbi_decoding.png}
    \end{center}
    \tiny{\url{http://www.ee.columbia.edu/ln/LabROSA/doc/HTKBook21/node8.html  - viterbi_decoding.gif}}
    % \begin{center}
    %     \includegraphics[width=30em]{viterbi_decoding_2.jpg}
    % \end{center}
    % \tiny{http://www.askkia.com/articles/what-is-a-convolutional-encoder-viterbi-algorithm.html - viterbi_decoding_2.jpg}
\end{frame}

\begin{frame}\frametitle{Links and references}
    Thank you for your attention! \\
    \begin{exampleblock}{Related links}
        \begin{itemize}
            \item Thesis and this slides \\ {\bf \url{https://github.com/oplatek/kaldi-thesis}}
            \item OnlineLatgenRecogniser implementation and AM training scripts \\ {\bf \url{https://github.com/UFAL-DSG/pykaldi}} 
            \item Alex implementation \\ {\bf \url{https://github.com/UFAL-DSG/alex}} 
            % \item Alex web presentation \\ {\bf \url{http://ufal.mff.cuni.cz/alex-dialogue-systems-framework/}} 
            \item Contact \& CV \\ {\bf \url{http://www.linkedin.com/in/ondrejplatek}}
        \end{itemize}
    \end{exampleblock}
\end{frame}

\end{document}  % do not remove!
