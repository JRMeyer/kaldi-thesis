% !TEX root = main.tex
\chapter{Real time decoder}
\label{cha:decoder}

This chapter describes the implementation
of the new {\it OnlLatgenFasterDecoder}\/ and 
the modification of an online speech parametrisation and feature transformations.

We limited our task, so it fits our requirements for real time decoding
in our \ac{SDS} Alex.
We need a real time decoder with sufficient accuracy,
which does not apply any speaker adaptive transformations.
We strongly prefer n-best list or lattice output formats over one-best hypothesis.
In addition, we require latency under 0.2 seconds after the end of user speech.

We also wanted to reuse existing code as much as possible,
so the decoder will benefit from constant development on Kaldi toolkit.
As a bonus of the minimal changes we can compare our {\it OnlLatgenFasterDecoder}\/ 
with {\it LatgenFasterDecoder}\/ which is used through binary executable {\it gmm-latgen-faster}.
Correctly setup, the decoders produce exactly the same results.


\section{Online pre-processing} 
\label{sec:onl_preprocess}
The pre-processing consist of audio signal buffering, \ac{MFCC} feature extraction and
applying feature transformation. 
The pre-processing and the decoder are bridged by the {\it decodable} 
interface as illustrated in Figure~\ref{fig:online_pipeline}.
We briefly describe each step:
\begin{itemize}
    \item Audio buffering
    \item Computation of \ac{MFCC} features
    \item Applying feature transformation. 
        \begin{itemize}
            \item $\Delta + \Delta\Delta$ \todo{How many frames requires}
            \item The $LDA+MLLT$ is computed using context,
                which by default is set to four previous and four future frames.
        \end{itemize}
        Note, that the $LDA+MLLT$ and the $\Delta+\Delta\Delta$ transformations are complementary.
    \item The {\it Decodable}\/ interface in our case the its {\it OnlDecodableDiagGmmScaled}\/ implementation
        queries the \ac{AM} for the probability for acoustic features $a$ and given state.
        Note that the acoustic features $a$ are the output of the previous steps.
    \item The decoder itself performs the search in state level space 
        having the probabilities from the {\it Decodable}\/ interface. 
        States represents triphones where for some thriphones the parameters are shared.
\end{itemize}

\begin{figure}[!htp]
    \begin{center}
        \input{images/online_pipeline}
        \caption{Components for online decoding}
    \label{fig:online_pipeline} 
    \end{center}
\end{figure}

Each step in~Figure~\ref{fig:online_pipeline} is implemented as a class.
In Figure~\ref{fig:classes} you may see, that additional helper class is also used.
During decoding each class is instantiated only once at the beginning.
The pre-processing classes are aggregated together. 
The {\it OnlBuffSource}\/ is setup as member of {\it OnlFeInput<Mfcc>}\/.
The {\it OnlFeInpute<Mfcc>}\/ is setup as member of either {\it OnlLdaInput}\/ or {\it OnlDeltaInput}\/.
One of the transformations is setup as member to {\it OnlFeatureMatrix}\/ instance.

The {\it OnlLatticeFasterDecoder}\/ performs forward decoding frame by frame using the Viterbi beam search.
The forward decoding is performed on request by calling the method {\it decode(int max\_frames)}.
It returns the number of frames which were actually decoded, which is always smaller than {\it max\_frames}.
In order to evaluate the probability of acoustic features for the new frame
the decoder query the {\it Decodable}\/ interface.

\begin{figure}[!htp]
    \begin{center}
        \input{images/pykaldi-classes}
    \caption{Aggregation of classes for serial input processing}
    \label{fig:classes} 
    \end{center}
\end{figure}

In case of {\it OnlDecodableDiagGmmScaled}, the online implementation, can trigger chain reaction.
If {\it OnlFeatureMatrix}\/ does not contain the acoustic features for the new frame, it asks
the previous component to compute the features, namely {\it OnlDeltaInput}\/ or {\it OnlLdaInput}\/.
If the previous component can not provide the features, it return default empty value indicating,
that no features are not available at the moment. It triggers the message that no data is available
and the decoder's method {\it decode(..)}, returns zero, meaning that no frames were decoded.

\subsubsection{History and work implemented}
\label{ssub:history}
We did not implemented the classes {\it OnlDeltaInput, OnlLdaInput, OnlFeInput}\/ and {\it OnlFeatureMatrix}\/
from scratch. We started the work with Mathias Pawlik implementation and we implemented few buffering details
in the classes, but mainly we removed the built in timeouts and changed the interface.
Note that we also added the {\it OnlBuffSource}\/ class, which just allows buffer the raw \ac{PCM} audio.

Let us argue why using timeouts is not convenient in the speech recognizing code
and should be let to the code which uses the speech recoginition code.
Our \ac{API} immediatly reports there are no frames available.
On the other hand in the code with timetouts, you have to wait the {\it timeout}\/ time
in order to figure out that there are no frames available.
In addition, if you have timetouts in multiple preprocessing classes the timeouts time may chain.
Finally, the timeouts require reinitialization of the preporcessing classes with frame context,
because the user did not respond during the {\it timeout}\/. 
Obviously, the delay could be caused by other reasons. For example if the application, 
which uses the speech recognition engine sends the audio over network, the network can be
the cause of exceeding the {\it timeout}\/.

\section{online decoder interface} 
\label{sec:improve}

\todo{How I split the LatgenFasterDecoder}

\todo{desscribe the behaviour of all functions especially the decode function}


\section{Post-processing the state lattice}
\label{sec:postprocess}

\todo{How I converted the state lattice to word posterior lattice}
\todo{decoding lattices and normal outputs -- describe again the problems}

\todo{The trick with discarding alignments}

\section{Objected oriented interface to speech recognition}
\label{sec:ooi}

The real time use of the decoder force us to design responsive \acl{API}.
We  
Note that the transformations need history of several frames

\code{Example of the decoder \ac{API} which is mapped in Python}{Python}{snippets/pykaldi_usage.py}

\todo{How I wrapped up the code instead of creating a binary executable}

\section{The setup for real-time decoding}

\todo{WER - with equivalent setup as gmm-latgen-faster, describe the relation ship. Maybe reference pykaldi/binutils}

\todo{The WER evaluation in SDS setup described in next chapter}

\todo{Which parameters influence decoding, which setup on which machine we use it}

\todo{profiling -cpu, spent time in which methods, memory profiling}

\todo{dej do evaluation, ze rozeberes nejenom presnost 
ale i rychlosty dekoderu, a rohodnes ktery dekoder z Kaldi je vhodny a jake nastaveni potrebuje pro RT performance.}

% \section[Comparison of real time decoders]{Comparison of real time decoders in Alex dialog system} 
% \label{sec:comparison_of_real_time_decoders_in_alex_dialog_system}
% Further, in this section we will briefly compare the main qualities of used decoders.
% Namely, we will compare: 
% \begin{itemize}
%     \item Kaldi online decoder - Acoustic models were trained by training scripts for Kaldi described at~\ref{cha:models}.
%     \item HDecode - HDecode is \ac{HTK} decoder and it decodes from acoustic models trained by \ac{HTK}.
% \end{itemize}


\section{Summary}
\label{sec:onl_summary}
The {\it OnlLatgenFasterDecoder}\/ is able to perform real-time decoding.
Its parameters for real-time decoding, 
can be setup based on its reference batch decoder {\it LatgenFasterDecoder}\/ used through {\it gmm-latgen-faster}\/ executable.

The minimal interface for speech parametrisation and feature transformations is not usable in general case,
but works very well for the implemented setup.
Our setup support \ac{MFCC} speech parametrisation, $\Delta+\Delta\Delta$ or \ac{LDA} transformation, with
classic Viterbi trained \ac{AM} or \ac{AM} trained using \ac{bMMI}.
The setup yields the best results for non-speaker adaptive methods.

\subsection{Future improvements}
\label{sub:onl_future}
As described, we support a narrow range of feature transformation,
so it would be natural to implement more complicated online interface,
which could support more feature transformations, 
which are available in Kaldi toolkit for batch mode. 

In next work, we would like to focus on acoustic modeling with \acl{DNN}.
The \ac{DNN} provide more accurate acoustic probabilities, 
which should lead to faster beam search,
because the search is more informed.\cite{TODO_DNN} 
We see the challenge in implementation of \acl{DNN} evaluation in real time, probably using \ac{GPU}.
Note that the Kaldi toolkit already support \ac{DNN} training and evaluation.
\todo{Write Karel Vesely and ask him how fast is the evaluation}
