% !TEX root = main.tex
\chapter{Real time decoder}
\label{cha:decoder}
% \todo{SIGDIAL BEGIN}

The significant drawback of Kaldi toolkit was the~lack of an on-line recognition support.
Therefore, it could not be used directly in applications such as spoken dialogue systems.
We implemented a~lightweight modification of the~\term{LatticeFasterDecoder} from the~Kaldi toolkit and created an on-line recogniser with an interface that is suitable for statistical dialogue systems.
The~Kaldi toolkit as well as the~on-line recogniser is distributed under the~Apache 2.0 license\footnote{\url{http://www.apache.org/licenses/LICENSE-2.0}}.

Kaldi included an on-line recognition application; however, hard-wired timeout exceptions, audio source fixed to a~sound card, and a specialised 1-best decoder limit its use to demonstration of Kaldi recognition capabilities only.

Our on-line recogniser may use acoustic models trained using the~state-of-the-art techniques, 
such as Linear Discriminant Analysis (LDA), Maximum Likelihood Linear Transform (MLLT), Boosted Maximum Mutual Information (BMMI), Minimum Phone Error (MPE).
It produces word posterior lattices which can be easily converted into high quality n-best lists.
The~recogniser's speed and latency can be effectively controlled off-line by optimising a~language model and during decoding by beam thresholds.

Section \ref{sec:impl} describes the~implementation of the~\term{OnlineLatgenRecogniser}.
% Section \ref{sec:train} outlines the~evaluation data and the~acoustic model used in the~evaluation.
Section \ref{sec:eval} evaluates the~accuracy and speed of the~recogniser.
\todo{fill other sections}
Finally, Section \ref{sec:conclusion} concludes this work.

\section{OnlineLatgenRecogniser}
\label{sec:impl}

The~standard Kaldi interface between the~components of the toolkit is based on a~batch processing paradigm, where the~components assume that whole audio signal is available when recognition starts.
However, when performing on-line recognition, one would like to take advantage of the~fact that the signal appears in small chunks and can be processed incrementally.
When properly implemented, this significantly reduces recogniser output latency.

\subsection{C++ implementation}
\label{sub:verb_c_}

To achieve this, we implemented Kaldi's \term{DecodableInterface} supporting incremental speech pre-processing, which includes speech parameterisation, feature transformations, and likelihood estimation.
In addition, we subclassed \term{LatticeFasterDecoder} and split the~original batch processing interface.

The~\term{OnlineLatgenRecogniser} makes use of the~new incremental speech pre-processing and modified \term{LatticeFasterDecoder}.
It implements the~following interface:
\begin{itemize}
\item \term{AudioIn} -- queueing new audio for pre-processing,
\item \term{Decode} -- decoding a~fixed number of audio frames,
\item \term{PruneFinal} -- preparing internal data structures for lattice extraction,
\item \term{GetLattice} -- extracting a word posterior lattice and returning log likelihood of processed audio,
\item \term{Reset} -- preparing the~recogniser for a~new utterance,
\end{itemize}

The~\verb!C++! example in Listing~\ref{snippets/rec_usage.cc} shows a typical use of the~\term{OnlineLatgenRecogniser} interface.
When audio data becomes available, it is queued into the~recogniser's buffer (line 11) and immediately decoded (lines 12-14).
If the~audio data is supplied in small enough chunks, the~decoding of queued data is finished before new data arrives.
When the recognition is finished, the~recogniser prepares for lattice extraction (line 16).
Line 20 shows how to obtain word posterior lattice as an OpenFST object.
The~\term{getAudio()} function represents a~separate process supplying speech data.
Please note that the~recogniser's latency is mainly determined by the~time spent in the~\term{GetLattice} function.

\code{Example of the~decoder usage}{C}{snippets/rec_usage.cc}

The~source code of the~\term{OnlineLatgenRecogniser} is available in Kaldi repository\footnote{\url{https://sourceforge.net/p/kaldi/code/HEAD/tree/sandbox/oplatek2/src/dec-wrap/}}.

\subsection{Python extension}
\label{sec:pyext}

\todo{Describe what is necessary to do for wrapping decoder, easy}

\todo{Describe what is necessary to wrap C++ -> hard but easy with pyfst -> extended}

In addition, we developed a~Python extension exporting the~\term{OnlineLatgenRecogniser} C++ interface.
This can be used as an example of bringing Kaldi's on-line speech recognition functionality to higher-level programming languages.
This Python extension is used in the~Alex Dialogue Systems Framework \cite{asdf2014url}.

In Listing~\ref{snippets/pykaldi_usage.py} we illustrate the usage of the \term{GmmLatgenWrapper}
respectively its Python wrapper \term{PyGmmLatgenWrapper}.

\code{Example of the decoder \ac{API} which is mapped in Python}{Python}{snippets/pykaldi_usage.py}

\section{Online pre-processing} 
\label{sec:onl_preprocess}
The pre-processing consist of audio signal buffering, \ac{MFCC} feature extraction and
applying feature transformation. 
The pre-processing and the decoder are bridged by the \term{decodable} 
interface as illustrated in Figure~\ref{fig:online_pipeline}.
We briefly describe each step:
\begin{itemize}
    \item Audio buffering
    \item Computation of \ac{MFCC} features
    \item Applying feature transformation. 
        \begin{itemize}
            \item $\Delta + \Delta\Delta$ \todo{How many frames requires}
            \item The $LDA+MLLT$ is computed using context,
                which by default is set to four previous and four future frames.
        \end{itemize}
        Note, that the $LDA+MLLT$ and the $\Delta+\Delta\Delta$ transformations are complementary.
    \item The \term{Decodable} interface in our case the its \term{OnlDecodableDiagGmmScaled} implementation
        queries the \ac{AM} for the probability for acoustic features $a$ and given state.
        Note that the acoustic features $a$ are the output of the previous steps.
    \item The decoder itself performs the search in state level space 
        having the probabilities from the \term{Decodable} interface. 
        States represents triphones where for some thriphones the parameters are shared.
\end{itemize}

\begin{figure}[!htp]
    \begin{center}
        \input{images/online_pipeline}
        \caption{Components for on-line decoding}
    \label{fig:online_pipeline} 
    \end{center}
\end{figure}

Each step in~Figure~\ref{fig:online_pipeline} is implemented as a class.
During decoding each class is instantiated only once at the beginning.
The pre-processing classes are aggregated together. 

The \term{OnlLatticeFasterDecoder} performs forward decoding frame by frame using the Viterbi beam search.
The forward decoding is performed on request by calling the method \term{decode(int max\_frames)}.
It returns the number of frames which were actually decoded, which is always smaller than \term{max\_frames}.
In order to evaluate the probability of acoustic features for the new frame
the decoder query the \term{Decodable} interface.

In case of \term{OnlDecodableDiagGmmScaled}, the on-line implementation, can trigger chain reaction.
If \term{OnlFeatureMatrix} does not contain the acoustic features for the new frame, it asks
the previous component to compute the features, namely \term{OnlDeltaInput} or \term{OnlLdaInput}.
If the previous component can not provide the features, it return default empty value indicating,
that no features are not available at the moment. It triggers the message that no data is available
and the decoder's method \term{decode(..)}, returns zero, meaning that no frames were decoded.


We did not implement the classes \term{OnlDeltaInput, OnlLdaInput, OnlFeInput} and \term{OnlFeatureMatrix}
from scratch. We started the work with Mathias Pawlik implementation and we implemented few buffering details
in the classes, but mainly we removed the built in timeouts and changed the interface.
In addition, we suggest that a~higher program logic e.g.\ timeouts should not be embedded into speech recogniser.
It slows the~decoding and it limits the~usage of such decoder.
Note that we also added the \term{OnlBuffSource} class, which just allows buffer the raw \ac{PCM} audio.


% \todo{SIGDIAL END}

% \section{Online decoder interface} 
% \label{sec:improve}
% We started developing the real-time speech recognizer by preparing the preprocessing
% pipeline as describe in~Section~\ref{sec:onl_preprocess}, after that we focused 
% on implementation the real-time decoder. In the first step we changed 
% the batch \term{LatticeFasterDecoder} only by reorganizing its methods into new interface.
% It turned out that the decoder with the new interface is 
% fast enough and sufficiently accurate for \ac{SDS}. 
% The evaluation in \ac{SDS} Alex is described in~Section~\ref{sec:evalution}.
% 
% Let us describe the details how we reorganized the \term{LatticeFasterDecoder} 
% into \term{OnlLatticeFasterDecoder}.
% 
% We implemented the new interface by inheriting the \term{OnlLatticeFasterDecoder} from \term{LatticeFasterDecoder} 
% and splitting the original \term{bool Decode(DecodableInterface *d)} function 
% into three simpler public functions.
% \begin{itemize}
%     \item \term{size\_t Decode(DecodableInterface *d, size\_t max\_frames)} 
%         Decodes using the function calls:
%         \begin{itemize}
%             \item \term{ProcessEmitting(decodbale, frame\_)} process non $\epsilon$ arcs.
%             \item \term{ProcessNonemitting(frame\_)} process $\epsilon$ arcs.
%             \item \term{PruneActiveTokens(frame\_, lattice\_beam * 0.1)} Prune still-alive tokens. 
%         \end{itemize}
% 
%     \item \term{void PruneFinal()} calls \term{PruneActiveTokensFinal(frame\_- 1)}, which starts
%         with probability of the final tokens and go backwards pruning the tokens.
%     \item \term{void Reset()} empties the data structures and turns the decoder into
%         initial state.
% \end{itemize}

% \subsection*{Online interface use case}
% The \term{OnlLatticeFasterDecoder} performs forward decoding using 
% the~\term{Decode(DecodableInterface *, size\_t max\_frames)}.
% The function \term{PruneFinal} performs the final pruning and prepares the active tokens from beam,
% to be transformed to state level lattice using the backward decoding.
% We use the \term{GetLattice(fst::MutableFst<CompactLatticeArc> *ofst)} function,
% because the \term{CompactLattice} it efficiently performs 
% state level lattice determinisation.\cite{povey2012generating}.
% 
% The \term{Reset()} function may be useful, if there is too many unprocessed frames,
% so we want to discard all the buffered audio. It is typically the case,
% when we did not have enough CPU resources and new user input appears and we want to process
% the latest audio input. If a \acl{SDS} has to choose which audio chunk to process, typically the last audio
% chunk is the most important for a conversation and the previous chunks are simply discarded.
% 
% \code{Usage of the~\term{OnlLatticeFasterDecoder} in its wrapper}{C}{snippets/onl_lat_dec_usage.cc}

\section{Post-processing the state lattice}
\label{sec:postprocess}
% \todo{The trick with discarding alignments}
% \todo{How I converted the state lattice to word posterior lattice}
% \todo{decoding lattices and normal outputs -- describe again the problems}
The \term{CompactLattice} determinised at state level still may contain
multiple paths for each word sequence encoded in the lattice.
In fact, typically there is number of state level alternatives for each word sequence.
We need to realize that the phones are not represented individually at the state level
and we does not concatenate the phone labels on path in order to obtain words and sentences.
The words label is located on the first arc of the word \ac{HMM}. 
The rest of the arcs in the graph are $\epsilon$ transition.

\ml{alignment}
Note that \ac{HMM} states are time synchronous, so traversing one arc represent a fixed time slot.
The time slot corresponds to frame shift as introduced in~Subsection~\ref{sub:param}.
The mapping between each word and the number of arcs from beginning of the utterance is called
alignment.

The \term{CompactLattice} distinguishes each path not only according word labels on the path,
but also according the alignments.
In order to obtain only the word lattice we discard the alignments.
In next steps we convert the \term{CompactLattice} to standard OpenFST object.\footnote{Kaldi implements a special semiring for \term{CompactLattice}\cite{povey2012generating}.}  

The following steps convert the weights on the lattice from representing likelihood
to posterior probabilities. The posterior probabilities are of course approximation of
true posterior probabilities for individual paths in lattice. 

The first approximation has a very small impact. The first approximation is 
that we are using only the lattice constructed from the active states in beam search
and not the complete search space. 
The discarded hypotheses have very low posterior probabilities, so it does a little harm.

More serious problem is violating the assumption that the \ac{AM} and \ac{LM} models are 
computing the true likelihood. 
In generative models we are training and improving the models so the likelihood match
the reality as much as possible.
On contrary, the discriminative \ac{AM} models deliberately favor the most probable hypothesis,
so they are not computing the true likelihood.
The discriminative behaviour can be compensated, but we do not solve the problem
in the decoder, because the decoder has no information, which \ac{AM} was used.

\code{Converting \term{CompactLattice} to posterior word lattice}{C}{snippets/compact2wordpost.cc}

The computing of the posterior probabilities is done through
standard forward-backward algorithm\cite{todo}.
We reused Kaldi function for computing helper $\alpha$ and $\beta$ data structures,
but we implemented the \term{MovePostToArcs} function for updating the lattice weights 
from likelihood to posterior probabilities based on $\alpha$ and $\beta$.

\section{Batch interface versus object-oriented on-line interface}
\label{sec:ooi}

\todo{
We also wanted to reuse existing code as much as possible,
so the decoder will benefit from constant development on Kaldi toolkit.
}

\todo{
As a bonus of the minimal changes we can compare our \term{OnlLatticeFasterDecoder} 
with \term{LatticeFasterDecoder} which is used through binary executable \term{gmm-latgen-faster}.
Correctly setup, the decoders produce exactly the same results.
The components of the Kaldi library such as decoder, or \ac{MLLT} transformation are designed using 
the \ac{OOP} design patterns.
On the other hand, the functionality of the Kaldi library is accessed by Kaldi executables, 
which are typically organized into single function.
The Kaldi executables are thin wrappers around the classes and 
are further combined together using system tools e.g.\ system pipes
and storing intermediate results to files. 
Such setup is convenient for speech recognition experiments, because the intermediate results are  
effectively reused by different experiments.
}

\todo{
Out task is to implement real-time speech recogniser into \acl{SDS}.
We also experimented and searched for the best setup as described in~Section~\ref{sec:evaluation},
but from the \ac{SDS} point of view we want simple component which accepts audio and produce \ac{ASR} hypothesis.
We profit from the reference \term{LatticeFasterDecoder}, where the Kaldi preprocessing executables
can be scripted in a such manner, so that the results of \term{LatticeFasterDecoder} 
and wrapped \term{OnlLatticeFasterDecoder} are identical.
We used the Kaldi executables to find the best experiment and once fixed,
we created object wrapper around the whole recognition pipeline.
}

\todo{
Let us described the Kaldi executable in our best experiment,
then we will describe the wrapper class for the speech recognition.
We chose \ac{MFCC} speech parametrisation, \ac{LDA} and \ac{MLLT} feature transformations with 
acoustic model trained by \ac{bMMI} as our best setup.
In our experiments, we used Kaldi binaries, which are listed below, 
for computing the one best \ac{ASR} hypothesis:
% \todo{How I wrapped up the code instead of creating a binary executable}
\begin{enumerate}
    \item compute-mfcc-feats
    \item copy-feats
    \item splice-feats
    \item transform-feats with trained \ac{LDA} and \ac{MLLT} matrix as parameter
    \item \label{enum:latgen} gmm-latgen-faster
    \item lattice-best-path
\end{enumerate}
}

\todo{
The \term{GmmLatgenWrapper} class wraps the whole recognition process
and provides \ac{API} convenient for real-time decoding.
We designed the \ac{API} of the preprocessing and the decoder 
so it can be nicely plugged into the \term{GmmLatgenWrapper} implementation.
}

\todo{
The  \term{GmmLatgenWrapper} instance accepts the audio in the~\term{FrameIn(\ldots)}
function and passes the audio chunk to the \term{OnlBuffSource} instance.
At the end of the pipeline is the \term{OnlDecodableDiagGmmScaled} instance,
which implements the~\term{DecodableInterface} and is queried by the \term{OnlLatticeFasterDecoder}
for the probability of each frame.
}

\todo{
Let us remind that audio buffered in~\term{OnlBuffSource} is 
segmented to 25ms frames which are shifted by 10ms\footnote{See Subsection~\ref{sub:param}}.
The \ac{MFCC} features are computed for each frame, the \ac{LDA}\&\ac{MLLT} matrix
is applied on features from 9 spliced frames.\footnote{Four frames from left 
and four frames from right context are used}. 
Finally, the \term{DecodableInterface} queries the \acl{AM} for the probability of the acoustic features.
}
% \code{The real-time decoding \ac{API} of \term{GmmLatgenWrapper}}{C}{snippets/gmmLatgenWrapper_simple.cc}

\todo{
Note, that the action of computing the new features are initialized by the \term{OnlLatticeFasterDecoder},
when the \term{Decode(DecodableInterface *d, size\_t max\_frames)} is called.
If one of the preprocessing components can not compute any features, 
because it does not has sufficient data, it returns its default $Null$ value. 
Consequently, all subsequent components returns its default $Null$ value and 
the \term{Decode(DecodableInterface *d, size\_t max\_frames)} returns zero,
indicating that zero frames were forward-decoded.
}

As a side effect of creating the class \term{GmmLatgenWrapper} representing speech recognition process
we easily implemented Python wrapper for the speech recognition. 
We just needed to implement a wrapper of a single class and the data structures transfered from and into the
\term{GmmLatgenWrapper} instance.

From Listing~\ref{snippets/pykaldi_usage.py} should be obvious that 
the forward decoding can be performed as the user speaks.
If the \ac{RTF} is smaller than one, the forward decoding can be performed faster
than the user produces the audio, so the end of the utterance
the speech recognition engine has to perform only backward decoding using 
the \term{get\_lattice} function.
If we have small enough \ac{RTF} and we supply the audio to the speech recognizer
in reasonably small chunks, the latency is influenced only by the~time of decoding the last chunk
and the~time of backward decoding and computing the word posterior lattice.

\section{The setup for real-time decoding}
\label{sec:real-setup}

In this section we focus on parameters of the speech recognizer \term{GmmLatgenWrapper},
which affect speed of decoding.
The parameters of the speech recognizer are passed to one of the components,
which are wrapped by the \term{GmmLatgenWrapper} class.
Most of the parameters for speech parametrisation and feature transformations 
does not effect speed of decoding. 
The frame width (set to 25 ms), the frame shift (set to 10 ms) and
the frame splicing (nine frames are spliced) 
are the only "preprocessing" parameters which could significantly affect the forward decoding speed.
We did not experiment with setting those and we used the mentioned recommended values.

On the other hand, there is a number \term{OnlLatticeFasterDecoder} parameters,
which directly affect the speed of forward decoding and there is a \term{lattice-beam} parameter,
which affect the speed of backward decoding and building a lattice.
The forward decoding parameters are listed below\footnote{Full list of parameters is listed in Section~\ref{sec:gmmlatgenwrapper}}:
\begin{enumerate}
    \item \term{beam} \todo{default value and description}
    \item \term{max-active} \todo{default value and description}
    \item \term{max-mem} \todo{default value and description}
    \item \term{prune-interval} \todo{default value and description}
\end{enumerate}

We estimated the forward decoding parameters using the \term{LatticeFasterDecoder}
wrapped by \term{gmm-latgen-faster} in the training scripts,
which are described in~Chapter~\ref{cha:train}. The decoding part of the scripts evaluates 
the \ac{RTF}, based on the \ac{RTF} we setup the parameters. 
Note that the scripts uses precomputed \ac{MFCC}, but the time spent on \ac{MFCC} evaluation
in \term{GmmLatgenWrapper} is so tiny, that we ignore the difference.

\todo{Which parameters influence decoding, which setup on which machine we use it}
In our current settings tuned for our machine\footnote{\todo{specify machine CPU, ... parameters.}}
we keep most of the default values and sets only four parameters.
\begin{verbatim}
--max-mem=10000000000 --lat-lm-scale=10 --beam=12.0 
--lattice-beam=6.0 --max-active=5000
\end{verbatim}
Three of the parameters affect the decoding speed  for other parameters.
The \term{lat-lm-scale} loosely resembles the $w_{lm}$ from 
Equation~\ref{eq:am_lm}.\footnote{In fact in \term{GmmLatgenWrapper} 
    a \ac{LM} weight is used for the first time in beam search,
    but it is neutralized before using the \term{lat-lm-scale} for scaling 
the \ac{AM} versus the \ac{LM} in the state level lattice.}

In Figure~\ref{fig:cpu_profile} see profiling.\footnote{For full log see Section~\ref{sec:profiling}}
\todo{Profiling describe}.

\begin{figure}[!htp]
    \begin{center}
    \begin{verbatim}
Hits Time  Per Hit   %  Line Contents
==============================================================
                        def decode(d, pcm_chunks):
13   0.00    0.00  0.0    for audio_chunk in pcm_chunks:
12  0.002    0.00  0.1      d.frame_in(audio_chunk)
12   0.70    0.06 37.4      dec_t = d.decode(max_frames=10)
47   0.00    0.00  0.0      while dec_t > 0:
35   0.83    0.02 43.9        dec_t = d.decode(max_frames=10)
 1   0.02    0.02  0.8      d.prune_final()
 1   0.33    0.33 17.6      lik, lat = d.get_lattice()
 1  0.002   0.002  0.1      d.reset(keep_buffer_data=False)
 1      3     3.0  0.0    return (lat, lik, decoded_frames)
        
    \end{verbatim}
    \caption{System time profiling of forward and backward decoding.
First column is number of row hits in \term{decode} function. Second column
is total system spent in the row in seconds, the third column shows per hit time. 
The last fourth column assigns the relative amount of time to each row. 
It is expressed in percent.}
    \label{fig:cpu_profile} 
    \end{center}
\end{figure}


\section{Summary}
\label{sec:onl_summary}
The \term{OnlLatticeFasterDecoder} is able to perform real-time decoding.
Its parameters for real-time decoding, 
can be setup based on its reference batch decoder \term{LatticeFasterDecoder} used through \term{gmm-latgen-faster} executable.

The minimal interface for speech parametrisation and feature transformations is not usable in general case,
but works very well for the implemented setup.
Our setup support \ac{MFCC} speech parametrisation, $\Delta+\Delta\Delta$ or \ac{LDA} transformation, with
classic Viterbi trained \ac{AM} or \ac{AM} trained using \ac{bMMI}.
The setup yields the best results for non-speaker adaptive methods.

\todo{GmmLatgenWrapper could be template class, to much work for us, we want minimalistic decoder}

\todo{dej do evaluation, ze rozeberes nejenom presnost ale i rychlosty dekoderu, a rohodnes ktery dekoder z Kaldi je vhodny a jake nastaveni potrebuje pro RT performance.}

\subsection{Future improvements}
\label{sub:onl_future}
As described, we support a narrow range of feature transformation,
so it would be natural to implement more complicated on-line interface,
which could support more feature transformations, 
which are available in Kaldi toolkit for batch mode. 

In next work, we would like to focus on acoustic modeling with \acl{DNN}.
The \ac{DNN} provide more accurate acoustic probabilities, 
which should lead to faster beam search,
because the search is more informed.\cite{TODO_DNN} 
We see the challenge in implementation of \acl{DNN} evaluation in real time, probably using \ac{GPU}.
Note that the Kaldi toolkit already support \ac{DNN} training and evaluation.
\todo{Write Karel Vesely and ask him how fast is the evaluation}
