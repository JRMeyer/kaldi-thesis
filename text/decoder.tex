% !TEX root = main.tex
\chapter{Real time recogniser}
\label{cha:decoder}
% \todo{SIGDIAL BEGIN}

We implemented a~lightweight modification of the~\term{LatticeFasterDecoder} from the~Kaldi toolkit, improved on-line speech parametrisation and feature processing in order to create an \term{OnlineLatgenRecogniser}.
The Kaldi \term{OnlineLatgenRecogniser} implements on-line interface which allows incremental speech processing which is able to process the incoming speech in small chunks incrementally.
As a result, the speech decoding is performed as user speaks and the ASR output is obtained with minimal latency.
% The~Kaldi toolkit as well as the~on-line recogniser is distributed under the~Apache 2.0 license\footnote{\url{http://www.apache.org/licenses/LICENSE-2.0}}.

The implementation of the recogniser was motivated by the~lack of an on-line recognition support in Kaldi toolkit.
Therefore, the toolkit decoders could not be used in applications such as spoken dialogue systems.
Although Kaldi included an on-line recognition application; but hard-wired timeout exceptions, audio source fixed to a~sound card, and a specialised 1-best decoder limit its use to demonstration of Kaldi recognition capabilities only.

Our on-line recogniser may use acoustic models trained using the~state-of-the-art techniques, 
such as Linear Discriminant Analysis (LDA), Maximum Likelihood Linear Transform (MLLT), Boosted Maximum Mutual Information (BMMI), Minimum Phone Error (MPE).
It produces word posterior lattices which can be easily converted into high quality n-best lists.
The~recogniser's speed and latency can be effectively controlled off-line by optimising a~language model. 
At runtime the speed of decoding is controlled by a beam threshold.
The latency which corresponds to the amount of time spent on word posterior lattice extraction depends on level of approximations used.

Section~\ref{sec:rec} describes the~implementation of the~\term{OnlineLatgenRecogniser}.
\term{OnlLatticeFasterDecoder} is the core component of the speech recogniser and it greatly influences the interface of \term{OnlineLatgenRecogniser}.
First, we describe the \term{OnlineLatgenRecogniser} in Subsection~\ref{sub:dec}. 
Later, we present the new interface of audio buffering, speech parametrisation and feature transformations in Subsection~\ref{sub:preprocess}, and Subsection~\ref{sub:postprocess} discuss word posterior lattice extraction.
Next Section~\ref{sec:pyext} describes, \term{PyOnlineLatgenRecogniser}, an extension of \term{OnlineLatgenRecogniser} into Python.
Finally, Section~\ref{sec:conclusion} summarize how we extended the Kaldi library.

\section{OnlineLatgenRecogniser}
\label{sec:rec}

The~standard Kaldi interface between the~components of the toolkit is based on a~batch processing paradigm, where the~components assume that whole audio signal is available when recognition starts.
However, when performing on-line recognition, one would like to take advantage of the~fact that the signal appears in small chunks and can be processed incrementally.
When properly implemented, this significantly reduces recogniser output latency.

\subsection{\term{OnlLatticeFasterDecoder}}
\label{sub:dec}
We did not implement no new functionality \term{OnlLatticeFasterDecoder}, but we only reorganised the code of base class \term{LatticeFasterDecoder}.
We splited the LatticeFasterDecoder::Decode which performed two separate tasks into two other functions.
The \term{LatticeFasterDecoder::Decode} function runs a beam search from frame 0 to the end of each utterance.
In addition, a pruning is triggered periodically in the function.
Namely, we control the beam search by following functions: 
\begin{itemize}
\item \term{Decode} -- decoding a~fixed number of audio frames instead of decoding whole utterance, pruning is triggered periodically,
\item \term{PruneFinal} -- run final pruning and so prepare the internal data structures for lattice extraction,
\item \term{Reset} -- preparing the~recogniser for a~new utterance.
\end{itemize}

In the \term{PruneFinal} function, called at the end of an utterance, the states selected by beam search are pruned with the knowledge that no further search will be performed so more states can be safely discarded.

Extracting the \ac{ASR} is performed by method of \term{LatticeFasterDecoder}, namely:
\begin{itemize}
    \item \term{GetRawLattice} returns state-level lattice,
    \item \term{GetLattice} extracts from state-level lattice word lattice which is returned,
    \item \term{GetBestPath} returns just one-best bath.
\end{itemize}

The state-level lattice returned from the \term{GetRawLattice} method can be understand as lattice on triphone level.
In the state-level lattice, single word hypothesis is typically represented as multiple state-level hypotheses due to different word alignments, i.e., the same words sequences were pronounced with different timing.

\subsection{\term{OnlineLatgenRecogniser} interface}
\label{sub:verb_c_}

We implemented Kaldi's \term{DecodableInterface} supporting incremental speech pre-processing, which includes speech parameterisation, feature transformations, and likelihood estimation.
In addition, we subclassed \term{LatticeFasterDecoder} and split the~original batch processing interface.

The~\term{OnlineLatgenRecogniser} makes use of the~new incremental speech pre-processing and modified \term{LatticeFasterDecoder}.
It implements the~following interface:
\begin{itemize}
\item \term{AudioIn} -- queueing new audio for pre-processing,
\item \term{Decode} -- decoding a~fixed number of audio frames,
\item \term{PruneFinal} -- preparing internal data structures for lattice extraction,
\item \term{GetLattice} -- extracting a word posterior lattice and returning log likelihood of processed audio,
\item \term{Reset} -- preparing the~recogniser for a~new utterance,
\end{itemize}

The~\verb!C++! example in Listing~\ref{snippets/rec_usage.cc} shows a typical use of the~\term{OnlineLatgenRecogniser} interface.
When audio data becomes available, it is queued into the~recogniser's buffer (line 11) and immediately decoded (lines 12-14).
If the~audio data is supplied in small enough chunks, the~decoding of queued data is finished before new data arrives.
When the recognition is finished, the~recogniser prepares for lattice extraction (line 16).
Line 20 shows how to obtain word posterior lattice as an OpenFST object.
The~\term{getAudio()} function represents a~separate process supplying speech data.
Please note that the~recogniser's latency is mainly determined by the~time spent in the~\term{GetLattice} function.

\code{Example of the~decoder usage}{C}{snippets/rec_usage.cc}

The~source code of the~\term{OnlineLatgenRecogniser} is available in Kaldi repository\footnote{\url{https://sourceforge.net/p/kaldi/code/HEAD/tree/sandbox/oplatek2/src/dec-wrap/}}.

\subsection{On-line feature pre-processing} 
\label{sub:preprocess}
This section describes of audio signal buffering, \ac{MFCC} feature extraction and feature transformation. 
\todo{The pre-processing and the decoder are bridged by the \term{decodable} interface as illustrated in Figure~\ref{fig:online_pipeline}.}
We briefly describe each step:
\begin{itemize}
    \item Audio buffering \todo{describe}
    \item Computation of \ac{MFCC} features on overlapping audio window.
    \item Applying feature transformation on top of \ac{MFCC} features. 
        \begin{itemize}
            \item $\Delta + \Delta\Delta$ requires at least two previous frames. 
            \item The $LDA+MLLT$ is computed using context,
                which by default is set to four previous and four future frames.
        \end{itemize}
        Note, that the $LDA+MLLT$ and the $\Delta+\Delta\Delta$ transformations are complementary.
    \item The \term{Decodable} interface in our case the its \term{OnlDecodableDiagGmmScaled} implementation
        queries the \ac{AM} for the probability for acoustic features $a$ and given state.
        Note that the acoustic features $a$ are the output of the previous steps.
    \item The decoder itself performs the search in state level space 
        having the probabilities from the \term{Decodable} interface. 
        States represents triphones where for some thriphones the parameters are shared.
\end{itemize}

\begin{figure}[!htp]
    \begin{center}
        \input{images/online_pipeline}
        \caption{Components for on-line decoding}
    \label{fig:online_pipeline} 
    \end{center}
\end{figure}

Each step in~Figure~\ref{fig:online_pipeline} is implemented as a class.
During decoding each class is instantiated only once at the beginning.
The pre-processing classes are aggregated together. 

The \term{OnlLatticeFasterDecoder} performs forward decoding frame by frame using the Viterbi beam search.
The forward decoding is performed on request by calling the method \term{decode(int max\_frames)}.
It returns the number of frames which were actually decoded, which is always smaller than \term{max\_frames}.
In order to evaluate the probability of acoustic features for the new frame
the decoder query the \term{Decodable} interface.

In case of \term{OnlDecodableDiagGmmScaled}, the on-line implementation, can trigger chain reaction.
If \term{OnlFeatureMatrix} does not contain the acoustic features for the new frame, it asks
the previous component to compute the features, namely \term{OnlDeltaInput} or \term{OnlLdaInput}.
If the previous component can not provide the features, it return default empty value indicating,
that no features are not available at the moment. It triggers the message that no data is available
and the decoder's method \term{decode(..)}, returns zero, meaning that no frames were decoded.


We did not implement the classes \term{OnlDeltaInput, OnlLdaInput, OnlFeInput} and \term{OnlFeatureMatrix}
from scratch. We started the work with Mathias Pawlik implementation and we implemented few buffering details
in the classes, but mainly we removed the built in timeouts and changed the interface.
In addition, we suggest that a~higher program logic e.g.\ timeouts should not be embedded into speech recogniser.
It slows the~decoding and it limits the~usage of such decoder.
Note that we also added the \term{OnlBuffSource} class, which just allows buffer the raw \ac{PCM} audio.


% \todo{SIGDIAL END}

% \section{Online decoder interface} 
% \label{sec:improve}
% We started developing the real-time speech recognizer by preparing the preprocessing
% pipeline as describe in~Section~\ref{sec:onl_preprocess}, after that we focused 
% on implementation the real-time decoder. In the first step we changed 
% the batch \term{LatticeFasterDecoder} only by reorganizing its methods into new interface.
% It turned out that the decoder with the new interface is 
% fast enough and sufficiently accurate for \ac{SDS}. 
% The evaluation in \ac{SDS} Alex is described in~Section~\ref{sec:evalution}.
% 
% Let us describe the details how we reorganized the \term{LatticeFasterDecoder} 
% into \term{OnlLatticeFasterDecoder}.
% 
% We implemented the new interface by inheriting the \term{OnlLatticeFasterDecoder} from \term{LatticeFasterDecoder} 
% and splitting the original \term{bool Decode(DecodableInterface *d)} function 
% into three simpler public functions.
% \begin{itemize}
%     \item \term{size\_t Decode(DecodableInterface *d, size\_t max\_frames)} 
%         Decodes using the function calls:
%         \begin{itemize}
%             \item \term{ProcessEmitting(decodbale, frame\_)} process non $\epsilon$ arcs.
%             \item \term{ProcessNonemitting(frame\_)} process $\epsilon$ arcs.
%             \item \term{PruneActiveTokens(frame\_, lattice\_beam * 0.1)} Prune still-alive tokens. 
%         \end{itemize}
% 
%     \item \term{void PruneFinal()} calls \term{PruneActiveTokensFinal(frame\_- 1)}, which starts
%         with probability of the final tokens and go backwards pruning the tokens.
%     \item \term{void Reset()} empties the data structures and turns the decoder into
%         initial state.
% \end{itemize}

% \subsection*{Online interface use case}
% The \term{OnlLatticeFasterDecoder} performs forward decoding using 
% the~\term{Decode(DecodableInterface *, size\_t max\_frames)}.
% The function \term{PruneFinal} performs the final pruning and prepares the active tokens from beam,
% to be transformed to state level lattice using the backward decoding.
% We use the \term{GetLattice(fst::MutableFst<CompactLatticeArc> *ofst)} function,
% because the \term{CompactLattice} it efficiently performs 
% state level lattice determinisation.\cite{povey2012generating}.
% 
% The \term{Reset()} function may be useful, if there is too many unprocessed frames,
% so we want to discard all the buffered audio. It is typically the case,
% when we did not have enough CPU resources and new user input appears and we want to process
% the latest audio input. If a \acl{SDS} has to choose which audio chunk to process, typically the last audio
% chunk is the most important for a conversation and the previous chunks are simply discarded.
% 
% \code{Usage of the~\term{OnlLatticeFasterDecoder} in its wrapper}{C}{snippets/onl_lat_dec_usage.cc}

\subsection{Post-processing the lattice}
\label{sec:postprocess}
\todo{What is CompactLattice}
The \term{CompactLattice} determinised at state level still may contain
multiple paths for each word sequence encoded in the lattice.
In fact, typically there is number of state level alternatives for each word sequence.
We need to realize that the phones are not represented individually at the state level
and we does not concatenate the phone labels on path in order to obtain words and sentences.
The words label is located on the first arc of the word \ac{HMM}. 
The rest of the arcs in the graph are $\epsilon$ transition.

\ml{alignment}
Note that \ac{HMM} states are time synchronous, so traversing one arc represent a fixed time slot.
The time slot corresponds to frame shift as introduced in~Subsection~\ref{sub:param}.
The mapping between each word and the number of arcs from beginning of the utterance is called
alignment.

The \term{CompactLattice} distinguishes each path not only according word labels on the path,
but also according the alignments.
In order to obtain only the word lattice we discard the alignments.
In next steps we convert the \term{CompactLattice} to standard OpenFST object.\footnote{Kaldi implements a special semiring for \term{CompactLattice}\cite{povey2012generating}.}  

The following steps convert the weights on the lattice from representing likelihood
to posterior probabilities. The posterior probabilities are of course approximation of
true posterior probabilities for individual paths in lattice. 

The first approximation has a very small impact. The first approximation is 
that we are using only the lattice constructed from the active states in beam search
and not the complete search space. 
The discarded hypotheses have very low posterior probabilities, so it does a little harm.

More serious problem is violating the assumption that the \ac{AM} and \ac{LM} models are 
computing the true likelihood. 
In generative models we are training and improving the models so the likelihood match
the reality as much as possible.
On contrary, the discriminative \ac{AM} models deliberately favor the most probable hypothesis,
so they are not computing the true likelihood.
The discriminative behaviour can be compensated, but we do not solve the problem
in the decoder, because the decoder has no information, which \ac{AM} was used.

\code{Converting \term{CompactLattice} to posterior word lattice}{C}{snippets/compact2wordpost.cc}

The computing of the posterior probabilities is done through standard forward-backward algorithm. %\cite{??}.
We reused Kaldi function for computing helper $\alpha$ and $\beta$ data structures, but we implemented the \term{MovePostToArcs} function for updating the lattice weights from likelihood to posterior probabilities based on $\alpha$ and $\beta$.

% \section{Batch interface versus object-oriented on-line interface}
% \label{sec:ooi}
% 
% \todo{
% We also wanted to reuse existing code as much as possible,
% so the decoder will benefit from constant development on Kaldi toolkit.
% }
% 
% \todo{
% As a bonus of the minimal changes we can compare our \term{OnlLatticeFasterDecoder} 
% with \term{LatticeFasterDecoder} which is used through binary executable \term{gmm-latgen-faster}.
% Correctly setup, the decoders produce exactly the same results.
% The components of the Kaldi library such as decoder, or \ac{MLLT} transformation are designed using 
% the \ac{OOP} design patterns.
% On the other hand, the functionality of the Kaldi library is accessed by Kaldi executables, 
% which are typically organized into single function.
% The Kaldi executables are thin wrappers around the classes and 
% are further combined together using system tools e.g.\ system pipes
% and storing intermediate results to files. 
% Such setup is convenient for speech recognition experiments, because the intermediate results are  
% effectively reused by different experiments.
% }
% 
% \todo{
% Out task is to implement real-time speech recogniser into \acl{SDS}.
% We also experimented and searched for the best setup as described in~Section~\ref{sec:evaluation},
% but from the \ac{SDS} point of view we want simple component which accepts audio and produce \ac{ASR} hypothesis.
% We profit from the reference \term{LatticeFasterDecoder}, where the Kaldi preprocessing executables
% can be scripted in a such manner, so that the results of \term{LatticeFasterDecoder} 
% and wrapped \term{OnlLatticeFasterDecoder} are identical.
% We used the Kaldi executables to find the best experiment and once fixed,
% we created object wrapper around the whole recognition pipeline.
% }
% 
% \todo{
% Let us described the Kaldi executable in our best experiment,
% then we will describe the wrapper class for the speech recognition.
% We chose \ac{MFCC} speech parametrisation, \ac{LDA} and \ac{MLLT} feature transformations with 
% acoustic model trained by \ac{bMMI} as our best setup.
% In our experiments, we used Kaldi binaries, which are listed below, 
% for computing the one best \ac{ASR} hypothesis:
% % \todo{How I wrapped up the code instead of creating a binary executable}
% \begin{enumerate}
%     \item compute-mfcc-feats
%     \item copy-feats
%     \item splice-feats
%     \item transform-feats with trained \ac{LDA} and \ac{MLLT} matrix as parameter
%     \item \label{enum:latgen} gmm-latgen-faster
%     \item lattice-best-path
% \end{enumerate}
% }
% 
% \todo{
% The \term{GmmLatgenWrapper} class wraps the whole recognition process
% and provides \ac{API} convenient for real-time decoding.
% We designed the \ac{API} of the preprocessing and the decoder 
% so it can be nicely plugged into the \term{GmmLatgenWrapper} implementation.
% }
% 
% \todo{
% The  \term{GmmLatgenWrapper} instance accepts the audio in the~\term{FrameIn(\ldots)}
% function and passes the audio chunk to the \term{OnlBuffSource} instance.
% At the end of the pipeline is the \term{OnlDecodableDiagGmmScaled} instance,
% which implements the~\term{DecodableInterface} and is queried by the \term{OnlLatticeFasterDecoder}
% for the probability of each frame.
% }
% 
% \todo{
% Let us remind that audio buffered in~\term{OnlBuffSource} is 
% segmented to 25ms frames which are shifted by 10ms\footnote{See Subsection~\ref{sub:param}}.
% The \ac{MFCC} features are computed for each frame, the \ac{LDA}\&\ac{MLLT} matrix
% is applied on features from 9 spliced frames.\footnote{Four frames from left 
% and four frames from right context are used}. 
% Finally, the \term{DecodableInterface} queries the \acl{AM} for the probability of the acoustic features.
% }
% % \code{The real-time decoding \ac{API} of \term{GmmLatgenWrapper}}{C}{snippets/gmmLatgenWrapper_simple.cc}
% 
% \todo{
% Note, that the action of computing the new features are initialized by the \term{OnlLatticeFasterDecoder},
% when the \term{Decode(DecodableInterface *d, size\_t max\_frames)} is called.
% If one of the preprocessing components can not compute any features, 
% because it does not has sufficient data, it returns its default $Null$ value. 
% Consequently, all subsequent components returns its default $Null$ value and 
% the \term{Decode(DecodableInterface *d, size\_t max\_frames)} returns zero,
% indicating that zero frames were forward-decoded.
% }
% 
% As a side effect of creating the class \term{GmmLatgenWrapper} representing speech recognition process
% we easily implemented Python wrapper for the speech recognition. 
% We just needed to implement a wrapper of a single class and the data structures transfered from and into the
% \term{GmmLatgenWrapper} instance.
% 
% From Listing~\ref{snippets/pykaldi_usage.py} should be obvious that 
% the forward decoding can be performed as the user speaks.
% If the \ac{RTF} is smaller than one, the forward decoding can be performed faster
% than the user produces the audio, so the end of the utterance
% the speech recognition engine has to perform only backward decoding using 
% the \term{get\_lattice} function.
% If we have small enough \ac{RTF} and we supply the audio to the speech recognizer
% in reasonably small chunks, the latency is influenced only by the~time of decoding the last chunk
% and the~time of backward decoding and computing the word posterior lattice.

\section{PyOnlineLatgenRecogniser}
\label{sec:pyext}

We developed a~Python extension, \term{PyOnlineLatgenRecogniser}, exporting the~\term{OnlineLatgenRecogniser} C++ interface.
This can be used as an example of bringing Kaldi's on-line speech recognition functionality to higher-level programming languages.
This Python extension is used in the~Alex Dialogue Systems Framework \cite{asdf2014url}.

\term{PyOnlineLatgenRecogniser} is a thin wrapper around \term{OnlineLatgenRecogniser} implemented using Cython\cite{cython2014url}.
The Cython is well known for its speed when interfacing Python and C++.
In addition, we extended PyFST library\cite{pyfst2014url} which interfaces OpenFST library into Python because we need to use in Python the OpenFST lattices produced by \term{OnlineLatgenRecogniser}.
Consequently, the recogniser as well as its input and output can be seamlessly used from Python.

We also implemented conversion of the word posterior lattices which are returned by \term{PyOnlineLatgenRecogniser} to an~n-best list. 
The implementation is efficient since the~OpenFST shortest path algorithm is used on small lattices.

The minimalistic Python example in Listing~\ref{snippets/pykaldi_usage.py} shows usage of the \term{PyOnlineLatgenRecogniser} and the~decoding of a single utterance.

The audio is passed to the recogniser in small chunks (line 4), so the decoding (line 5 and 8) can be performed as user speaks.
When no more audio data is available a likelihood and a word posterior lattice is extracted from the recogniser(line 10).
\code{Fully functional example of the \term{PyOnlineLatgenRecogniser} interface}{Python}{snippets/pykaldi_usage.py}

Note that \term{PyOnlineLatgenRecogniser} and \term{OnlineLatgenRecogniser} are initialised by string vector of arguments in command line format.
The parameters are parsed using Kaldi's command line parser and options affect behaviour speech parametrisation, feature transformations and  the \term{OnlLatticeFasterDecoder}.
In addition, exactly the same parameters can be parsed by standard Kaldi utilities. We created scripts which use exactly the same speech recognition parameters and performs speech recognition with both standard Kaldi utilities and our \term{PyOnlineLatgenRecogniser} where the alternatives produce exactly the same results.

\section{Decoder parameters}
\label{sec:real-setup}
\todo{Write about influence of beam search on Lattice extraction}

In this section we focus on parameters of the \term{OnlineLatgenRecogniser},
which affect speed of decoding.
The parameters of the speech recognizer are passed to speech parametrisation, feature transformations or \term{OnlLatticeFasterDecoder}.
Most of the parameters for speech parametrisation and feature transformations does not effect speed of decoding. 
The frame width (set to 25 ms), the frame shift (set to 10 ms) and the frame splicing (nine frames are spliced) are the only "preprocessing" parameters which could significantly affect the forward decoding speed.
We did not experiment with setting those and we used the recommended values.

On the other hand, the \term{beam}, \term{lattice-beam} and \term{max-active-states} parameters directly affect the speed of speech recognition. 
The \term{beam} and \term{max-active-states} parameters affect speed of beam search, whereas \term{lattice-beam} influence speed of lattice extraction.

\section{Summary}
\label{sec:onl_summary}
The \term{OnlLatticeFasterDecoder} is able to perform on-line speech recognition.
Its parameters for real-time decoding, can be setup based on its reference batch decoder \term{LatticeFasterDecoder} used through \term{gmm-latgen-faster} executable.

The minimal interface works very well for supported \ac{MFCC} speech parametrisation, $\Delta-\Delta\Delta$ feature transformation or \ac{LDA}+\ac{MLLT}
and both generative training and discriminative training using \ac{bMMI} and \ac{MPE}.
The setup yields the best results for non-speaker adaptive methods.

% \subsection*{Future improvements}
% \label{sub:onl_future}
% As described, we support a narrow range of feature transformation, so it would be natural to implement more sophisticated on-line interface, which could support more feature transformations available in Kaldi.
% 
% In next work, we would like to focus on acoustic modeling with \acl{DNN}.
% The \ac{DNN} provide more accurate acoustic probabilities, which should lead to faster beam search, because the search is more informed.\cite{zhang2014improving} 
% We see the challenge in implementation of \acl{DNN} evaluation in real time, probably using \ac{GPU}.
