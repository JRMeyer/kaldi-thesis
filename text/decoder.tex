% !TEX root = main.tex
\chapter{Real time recogniser}
\label{cha:decoder}
% \todo{SIGDIAL BEGIN}

We implemented a~lightweight modification of the~\term{LatticeFasterDecoder} from the~Kaldi toolkit, improved on-line speech parametrisation and feature processing in order to create an \term{OnlineLatgenRecogniser}.
The Kaldi \term{OnlineLatgenRecogniser} implements on-line interface which allows incremental speech processing, and it is able to process the incoming speech in small chunks incrementally.
As a result, the speech decoding is performed as user speaks and the ASR output is obtained with minimal latency.
% The~Kaldi toolkit as well as the~on-line recogniser is distributed under the~Apache 2.0 license\footnote{\url{http://www.apache.org/licenses/LICENSE-2.0}}.

The implementation of the recogniser was motivated by the~lack of an on-line recognition support in Kaldi toolkit.
Therefore, the toolkit decoders could not be used in applications such as spoken dialogue systems.
Although Kaldi included an on-line recognition application; but hard-wired timeout exceptions, audio source fixed to a~sound card, and a specialised 1-best decoder limit its use only to demonstration of Kaldi recognition capabilities.

Our on-line recogniser uses acoustic models trained using the~state-of-the-art techniques, 
such as Linear Discriminant Analysis (LDA), Maximum Likelihood Linear Transform (MLLT), Boosted Maximum Mutual Information (BMMI) and Minimum Phone Error (MPE).
It produces word posterior lattices which can be easily converted into high quality n-best lists.

The~recogniser's speed and latency can be effectively controlled off-line by optimising a~language model. 
At runtime the speed of decoding is controlled by a beam threshold.
The latency which corresponds to the amount of time spent on word posterior lattice extraction depends on level of approximations used.

Section~\ref{sec:rec} describes the~implementation of the~\term{OnlineLatgenRecogniser}.
\term{OnlLatticeFasterDecoder} is the core component of the speech recogniser and it greatly influences the interface of the recogniser.
First, we describe the \term{OnlineLatgenRecogniser} in Subsection~\ref{sub:dec}. 
Next Section~\ref{sec:pyext} describes, \term{PyOnlineLatgenRecogniser}, an extension of \term{OnlineLatgenRecogniser} into Python.
Finally, Section~\ref{sec:conclusion} summarize how we extended the Kaldi library.


\section{OnlineLatgenRecogniser}
\label{sec:rec}

The~standard Kaldi interface between the~components of the toolkit is based on a~batch processing paradigm, where the~components assume that whole audio signal is available when recognition starts.
However, when performing on-line recognition, one would like to take advantage of the~fact that the signal appears in small chunks and can be processed incrementally.
When properly implemented, this significantly reduces recogniser output latency.
We reimplemented incremental speech parameterisation and feature transformations, and likelihood estimation to be more responsive.
In addition, we subclassed \term{LatticeFasterDecoder} and reorganized the~original batch interface to on-line interface.
First, we present the interface of \term{OnlineLatgenRecogniser} and in next subsections we introduce its components.
The Subsection~\ref{sub:dec} describes the decoder, the core component.
Subsection~\ref{sub:preprocess} introduces on-line speech parametrisation and feature transformations and the Subsection~\ref{sub:postprocess} discuss word posterior lattice extraction.

\subsection{\term{OnlineLatgenRecogniser} interface}
\label{sub:verb_c_}
The~\term{OnlineLatgenRecogniser} makes use of the~new incremental speech pre-processing and modified \term{LatticeFasterDecoder}.
It implements the~following interface:
\begin{itemize}
\item \term{AudioIn} -- queueing new audio for pre-processing,
\item \term{Decode} -- decoding a~fixed number of audio frames,
\item \term{PruneFinal} -- preparing internal data structures for lattice extraction,
\item \term{GetLattice} -- extracting a word posterior lattice and returning log likelihood of processed audio,
\item \term{GetBestPath} -- extracting a one best word sequence,
\item \term{Reset} -- preparing the~recogniser for a~new utterance,
\end{itemize}

The~\verb!C++! example in Listing~\ref{snippets/rec_usage.cc} shows a typical use of the~\term{OnlineLatgenRecogniser} interface.
When audio data becomes available, it is queued into the~recogniser's buffer (line 11) and immediately decoded (lines 12-14).
If the~audio data is supplied in small enough chunks, the~decoding of queued data is finished before new data arrives.
When the recognition is finished, the~recogniser prepares for lattice extraction (line 16).
Line 20 shows how to obtain word posterior lattice as an OpenFST object.
The~\term{getAudio()} function represents a~separate process supplying speech data.
Please note that the~recogniser's latency is mainly determined by the~time spent in the~\term{GetLattice} function.

\code{Example of the~decoder usage}{C}{snippets/rec_usage.cc}

We desinged the interface with two things in mind:
\begin{itemize}
    \item Passing the audio in the recogniser should be very responsive and robust to any size of audio input.
    \item Decoding should be very responsive even if no audio is available.
    \item Decoding can be triggered at any time.
    \item Decoding should be called frequently on small chunks which guaranties quick response times of the \term{Decode} method.
\end{itemize}

Consequently, the interface does not block a process to either load audio or decode an utterance.
The loading of audio and the decoding can be easily alternate back and forth.
Obviously, the decoding of single utterance can be separated into many parts and other tasks can be run in single process with speech recognition in order to allow an application to be responsive.
We use the interface to decode the utterance as user speaks.

On the other hand, extracting the word posterior lattice may block the process since it is very computationally demanding.
It lasts several tens of milliseconds, but is called only at the end of each utterance.
Extracting one best word sequence is much faster and can be called at any time.

\subsection{\term{OnlLatticeFasterDecoder}}
\label{sub:dec}
We did not implement no new functionality \term{OnlLatticeFasterDecoder}, but we only reorganised the code of base class \term{LatticeFasterDecoder}.
We splited the LatticeFasterDecoder::Decode method which performed several tasks into more functions.
The \term{LatticeFasterDecoder::Decode} function runs a beam search from frame 0 to the end of each utterance.
In addition, a pruning is triggered periodically in the function.
In \term{OnlineLatgenRecogniser},  we control the beam search by following functions: 
\begin{itemize}
\item \term{Decode} -- decoding a~fixed number of audio frames instead of decoding whole utterance, pruning is triggered periodically,
\item \term{PruneFinal} -- run final pruning and so prepare the internal data structures for lattice extraction,
\item \term{Reset} -- preparing the~recogniser for a~new utterance.
\end{itemize}

In the \term{PruneFinal} function, which is called at the end of an utterance, the states are pruned by beam search with the knowledge that no further search will be performed so more states can be safely discarded.

The \term{OnlLatticeFasterDecoder} performs forward decoding frame by frame using the Viterbi beam search.
The forward decoding is performed on request by calling the method \term{decode(int max\_frames)}.
It returns the number of frames which were actually decoded, which is always smaller than the \term{max\_frames} passed as input parameter.

However, extracting the \ac{ASR} output is still performed by method of \term{LatticeFasterDecoder}, namely:
\begin{itemize}
    \item \term{GetRawLattice} returns state-level lattice,
    \item \term{GetLattice} extracts from state-level lattice word lattice which is returned,
    \item \term{GetBestPath} returns just one-best bath.
\end{itemize}

The state-level lattice returned from the \term{GetRawLattice} method can be understand as lattice on triphone level.
In the state-level lattice, single word hypothesis is typically represented as multiple state-level hypotheses due to different word alignments, i.e., the same words sequences were pronounced with different timing.


\subsection{On-line feature pre-processing} 
\label{sub:preprocess}
This section describes of audio signal buffering, \ac{MFCC} feature extraction and feature transformation. 
\todo{The pre-processing and the decoder are bridged by the \term{decodable} interface as illustrated in Figure~\ref{fig:online_pipeline}.}
We briefly describe each step:
\begin{itemize}
    \item Audio buffering which accepts audio of any size without problem. 
    \item Computation of \ac{MFCC} features on overlapping audio window.
    \item Applying feature transformation on top of \ac{MFCC} features. 
        \begin{itemize}
            \item $\Delta + \Delta\Delta$ requires at least two previous frames. 
            \item The $LDA+MLLT$ is computed using context,
                which by default is set to four previous and four future frames.
        \end{itemize}
        Note, that the $LDA+MLLT$ and the $\Delta+\Delta\Delta$ transformations are complementary.
    \item The \term{Decodable} interface in our case the its \term{OnlDecodableDiagGmmScaled} implementation
        queries the \ac{AM} for the probability for acoustic features $a$ and given state.
        Note that the acoustic features $a$ are the output of the previous steps.
    \item The decoder itself performs the search in state level space 
        having the probabilities from the \term{Decodable} interface. 
        States represents triphones where for some thriphones the parameters are shared.
\end{itemize}

\begin{figure}[!htp]
    \begin{center}
        \input{images/online_pipeline}
        \caption{Components for on-line decoding}
    \label{fig:online_pipeline} 
    \end{center}
\end{figure}

Each step in~Figure~\ref{fig:online_pipeline} is implemented as a C++ class.
\term{OnlineLatticeRecogniser} instantiate each class only once during setup.
The pre-processing classes are aggregated together.

In case of \term{OnlDecodableDiagGmmScaled}, the on-line implementation, can trigger chain reaction.
If \term{OnlFeatureMatrix} does not contain the acoustic features for the new frame, it asks
the previous component to compute the features, namely \term{OnlDeltaInput} or \term{OnlLdaInput}.
If the previous component can not provide the features, it return default empty value indicating,
that no features are not available at the moment. It triggers the message that no data is available
and the decoder's method \term{decode(..)}, returns zero, meaning that no frames were decoded.


\todo{The frame width (set to 25 ms), the frame shift (set to 10 ms) and the frame splicing (nine frames are spliced) are the only "preprocessing" parameters which could significantly affect the forward decoding speed.
We did not experiment with setting those and we used the recommended values.
}

\todo{
On the other hand, the \term{beam}, \term{lattice-beam} and \term{max-active-states} parameters directly affect the speed of speech recognition. 
The \term{beam} and \term{max-active-states} parameters affect speed of beam search, whereas \term{lattice-beam} influence speed of lattice extraction.
}


% We did not implement the classes \term{OnlDeltaInput, OnlLdaInput, OnlFeInput} and \term{OnlFeatureMatrix}
% from scratch. We started the work with Mathias Pawlik implementation and we implemented few buffering details
% in the classes, but mainly we removed the built in timeouts and changed the interface.
% In addition, we suggest that a~higher program logic e.g.\ timeouts should not be embedded into speech recogniser.
% It slows the~decoding and it limits the~usage of such decoder.
% Note that we also added the \term{OnlBuffSource} class, which just allows buffer the raw \ac{PCM} audio.


% We implemented the new interface by inheriting the \term{OnlLatticeFasterDecoder} from \term{LatticeFasterDecoder} 
% and splitting the original \term{bool Decode(DecodableInterface *d)} function 
% into three simpler public functions.
% \begin{itemize}
%     \item \term{size\_t Decode(DecodableInterface *d, size\_t max\_frames)} 
%         Decodes using the function calls:
%         \begin{itemize}
%             \item \term{ProcessEmitting(decodbale, frame\_)} process non $\epsilon$ arcs.
%             \item \term{ProcessNonemitting(frame\_)} process $\epsilon$ arcs.
%             \item \term{PruneActiveTokens(frame\_, lattice\_beam * 0.1)} Prune still-alive tokens. 
%         \end{itemize}
% 
%     \item \term{void PruneFinal()} calls \term{PruneActiveTokensFinal(frame\_- 1)}, which starts
%         with probability of the final tokens and go backwards pruning the tokens.
%     \item \term{void Reset()} empties the data structures and turns the decoder into
%         initial state.
% \end{itemize}

% \subsection*{Online interface use case}
% The \term{OnlLatticeFasterDecoder} performs forward decoding using 
% the~\term{Decode(DecodableInterface *, size\_t max\_frames)}.
% The function \term{PruneFinal} performs the final pruning and prepares the active tokens from beam,
% to be transformed to state level lattice using the backward decoding.
% We use the \term{GetLattice(fst::MutableFst<CompactLatticeArc> *ofst)} function,
% because the \term{CompactLattice} it efficiently performs 
% state level lattice determinisation.\cite{povey2012generating}.
% 
% The \term{Reset()} function may be useful, if there is too many unprocessed frames,
% so we want to discard all the buffered audio. It is typically the case,
% when we did not have enough CPU resources and new user input appears and we want to process
% the latest audio input. If a \acl{SDS} has to choose which audio chunk to process, typically the last audio
% chunk is the most important for a conversation and the previous chunks are simply discarded.

\subsection{Post-processing the lattice}
\label{sec:postprocess}
The \term{OnlineLatgenRecogniser} not only extracts word lattice using \term{OnlLatticeFasterDecoder::GetLattice} function, but also computes posterior probabilities for the word lattice.
The \term{OnlLatticeFasterDecoder} returns word lattice with alignments in form of \term{CompactLattice}.
The \term{CompactLattice} determinised at state level still may contain multiple paths for each word sequence encoded in the lattice.
The \term{CompactLattice} distinguishes each path not only according word labels on the path, but also according the alignments.
In order to obtain only the word lattice we discard the alignments because it significantly reduces the lattice size and also it allow us to compute the posterior lattice.

% \todo{
% In fact, typically there is number of state level alternatives for each word sequence.
% We need to realize that the phones are not represented individually at the state level and we does not concatenate the phone labels on path in order to obtain words and sentences.
% The words label is located on the first arc of the word \ac{HMM}. 
% The rest of the arcs in the graph are $\epsilon$ transition.
%
% \ml{alignment}
% Note that \ac{HMM} states are time synchronous, so traversing one arc represent a fixed time slot.
% The time slot corresponds to frame shift as introduced in~Subsection~\ref{sub:param}.
% The mapping between each word and the number of arcs from beginning of the utterance is called
% alignment.
% }


The steps of converting \term{CompactLattice} to word posterior lattice are listed below. For implementation details see Listing~\ref{snippets/compact2wordpost.cc}:
\begin{itemize}
    \item Joining multiple word sequences which differer in word alignments is performed in two steps:
    \begin{itemize}
        \item Discarding the alignments from \term{CompactLattice}.
        \item Converting the lattice to its minimal lattice representation with no alternatives for one word hypothesis. 
    \end{itemize}
    \item The computing of the posterior probabilities through standard forward-backward algorithm, which is implemented in two steps:
    \begin{itemize}
        \item Computing $\alpha$ and $\beta$ data structures for which a Kaldi implementation is reused.
        \item Updating the lattice weights from likelihood to posterior probabilities based on $\alpha$ and $\beta$, which we implemented in the \term{MovePostToArcs} function.
    \end{itemize}
\end{itemize}

\code{Converting \term{CompactLattice} to posterior word lattice}{C}{snippets/compact2wordpost.cc}

The word posterior probability is converted from the likelihood of the words in word lattice. 
The word lattice obviously contains alternatives which were explored by the beam search during decoding the utterance.
Consequently, the posterior probability is an approximation because the very low probable alternatives discarded by beam search are not considered.
On the other hand, the discarded alternatives are so improbable so they almost does not influence the posterior probability.

Presumably the word posterior values are more impacted by inaccurate likelihood values taken from \acl{AM}.
In generative models we are training and improving the models so the likelihood match the reality as much as possible.
On the other hand, the discriminative \ac{AM} models deliberately favor the most probable hypothesis, so they are boosting the likelihood of the most probable hypothesis.
As a result, the word posterior probabilities for the best hypothesis are also artificially boosted.
At the moment, we do not renormalise the word posterior probabilities in extracted lattices.

% \section{Batch interface versus object-oriented on-line interface}
% \label{sec:ooi}
% 
% \todo{
% We also wanted to reuse existing code as much as possible,
% so the decoder will benefit from constant development on Kaldi toolkit.
% }
% 
% \todo{
% As a bonus of the minimal changes we can compare our \term{OnlLatticeFasterDecoder} 
% with \term{LatticeFasterDecoder} which is used through binary executable \term{gmm-latgen-faster}.
% Correctly setup, the decoders produce exactly the same results.
% The components of the Kaldi library such as decoder, or \ac{MLLT} transformation are designed using 
% the \ac{OOP} design patterns.
% On the other hand, the functionality of the Kaldi library is accessed by Kaldi executables, 
% which are typically organized into single function.
% The Kaldi executables are thin wrappers around the classes and 
% are further combined together using system tools e.g.\ system pipes
% and storing intermediate results to files. 
% Such setup is convenient for speech recognition experiments, because the intermediate results are  
% effectively reused by different experiments.
% }
% 
% \todo{
% Out task is to implement real-time speech recogniser into \acl{SDS}.
% We also experimented and searched for the best setup as described in~Section~\ref{sec:evaluation},
% but from the \ac{SDS} point of view we want simple component which accepts audio and produce \ac{ASR} hypothesis.
% We profit from the reference \term{LatticeFasterDecoder}, where the Kaldi preprocessing executables
% can be scripted in a such manner, so that the results of \term{LatticeFasterDecoder} 
% and wrapped \term{OnlLatticeFasterDecoder} are identical.
% We used the Kaldi executables to find the best experiment and once fixed,
% we created object wrapper around the whole recognition pipeline.
% }
% 
% \todo{
% Let us described the Kaldi executable in our best experiment,
% then we will describe the wrapper class for the speech recognition.
% We chose \ac{MFCC} speech parametrisation, \ac{LDA} and \ac{MLLT} feature transformations with 
% acoustic model trained by \ac{bMMI} as our best setup.
% In our experiments, we used Kaldi binaries, which are listed below, 
% for computing the one best \ac{ASR} hypothesis:
% % \todo{How I wrapped up the code instead of creating a binary executable}
% \begin{enumerate}
%     \item compute-mfcc-feats
%     \item copy-feats
%     \item splice-feats
%     \item transform-feats with trained \ac{LDA} and \ac{MLLT} matrix as parameter
%     \item \label{enum:latgen} gmm-latgen-faster
%     \item lattice-best-path
% \end{enumerate}
% }
% 
% \todo{
% The \term{GmmLatgenWrapper} class wraps the whole recognition process
% and provides \ac{API} convenient for real-time decoding.
% We designed the \ac{API} of the preprocessing and the decoder 
% so it can be nicely plugged into the \term{GmmLatgenWrapper} implementation.
% }
% 
% \todo{
% The  \term{GmmLatgenWrapper} instance accepts the audio in the~\term{FrameIn(\ldots)}
% function and passes the audio chunk to the \term{OnlBuffSource} instance.
% At the end of the pipeline is the \term{OnlDecodableDiagGmmScaled} instance,
% which implements the~\term{DecodableInterface} and is queried by the \term{OnlLatticeFasterDecoder}
% for the probability of each frame.
% }
% 
% \todo{
% Let us remind that audio buffered in~\term{OnlBuffSource} is 
% segmented to 25ms frames which are shifted by 10ms\footnote{See Subsection~\ref{sub:param}}.
% The \ac{MFCC} features are computed for each frame, the \ac{LDA}\&\ac{MLLT} matrix
% is applied on features from 9 spliced frames.\footnote{Four frames from left 
% and four frames from right context are used}. 
% Finally, the \term{DecodableInterface} queries the \acl{AM} for the probability of the acoustic features.
% }
% % \code{The real-time decoding \ac{API} of \term{GmmLatgenWrapper}}{C}{snippets/gmmLatgenWrapper_simple.cc}
% 
% \todo{
% Note, that the action of computing the new features are initialized by the \term{OnlLatticeFasterDecoder},
% when the \term{Decode(DecodableInterface *d, size\_t max\_frames)} is called.
% If one of the preprocessing components can not compute any features, 
% because it does not has sufficient data, it returns its default $Null$ value. 
% Consequently, all subsequent components returns its default $Null$ value and 
% the \term{Decode(DecodableInterface *d, size\_t max\_frames)} returns zero,
% indicating that zero frames were forward-decoded.
% }
% 
% 
% If the \ac{RTF} is smaller than one, the forward decoding can be performed faster
% than the user produces the audio, so the end of the utterance
% the speech recognition engine has to perform only backward decoding using 
% the \term{get\_lattice} function.
% If we have small enough \ac{RTF} and we supply the audio to the speech recognizer
% in reasonably small chunks, the latency is influenced only by the~time of decoding the last chunk
% and the~time of backward decoding and computing the word posterior lattice.

\section{PyOnlineLatgenRecogniser}
\label{sec:pyext}

We developed a~Python extension, \term{PyOnlineLatgenRecogniser}, exporting the~\term{OnlineLatgenRecogniser} C++ interface to Python.
It can be used as an example of bringing Kaldi's on-line speech recognition functionality to higher-level programming languages.
We extended also PyFST library\cite{pyfst2014url} which interfaces OpenFST C++ template library into Python because we need to process further the OpenFST lattices produced by \term{PyOnlineLatgenRecogniser} in Python.
Consequently, the recogniser as well as its input and output can be seamlessly used both from C++ and Python.
% This Python extension is used in the~Alex Dialogue Systems Framework \cite{asdf2014url}.

\term{PyOnlineLatgenRecogniser} is a thin wrapper around \term{OnlineLatgenRecogniser} implemented using Cython\cite{cython2014url}.
The Cython compiler is well known for generating fast code when interfacing Python and C++ and the wrapper causes no measurable overhead.

We implemented conversion of the word posterior lattices to an~n-best list.
The implementation is efficient since the~OpenFST shortest path algorithm is used on small lattices.

The minimalistic Python example in Listing~\ref{snippets/pykaldi_usage.py} shows usage of the \term{PyOnlineLatgenRecogniser} and the~decoding of a single utterance.

The audio is passed to the recogniser in small chunks (line 4), so the decoding (line 5 and 8) can be performed as user speaks.
When no more audio data is available a likelihood and a word posterior lattice is extracted from the recogniser(line 10).
\code{Fully functional example of the \term{PyOnlineLatgenRecogniser} interface}{Python}{snippets/pykaldi_usage.py}

Note that \term{PyOnlineLatgenRecogniser} and \term{OnlineLatgenRecogniser} are initialised by string vector of arguments in command line format.
The parameters are parsed using Kaldi's command line parser and options affect behaviour speech parametrisation, feature transformations and  the \term{OnlLatticeFasterDecoder}.
In addition, exactly the same parameters can be parsed by standard Kaldi utilities. 
We created demos\footnote{\url{https://github.com/UFAL-DSG/pykaldi/tree/master/egs/vystadial/online_demo}} which use the same parameters for speech recognition using:
\begin{itemize}
    \item standard Kaldi executables and scripts
    \item \term{PyOnlineLatgenRecogniser} 
    \item \term{OnlineLatgenRecogniser}
\end{itemize}
The alternatives produces exactly the same results.

\section{Summary}
\label{sec:onl_summary}
The \term{OnlLatticeFasterDecoder} is able to perform on-line speech recognition.
We suggest exploit the \term{OnlineLatgenRecogniser} and decode the utterance in small chunks and pass the audio to the recogniser immediately as it is available.
The speech recognition parameters are initialized with reasonable default values and the parameters are the same as used in Kaldi executables. 
As a result, one can use the parameters from any Kaldi recipe to obtain the exactly same high quality results but with on-line speech recognition.

The implemented minimal on-line interface which supports \ac{MFCC} speech parametrisation, $\Delta-\Delta\Delta$ feature transformation or \ac{LDA}+\ac{MLLT} and both generative training and discriminative training using \ac{bMMI} and \ac{MPE}.
The \ac{MFFC}, \ac{LDA}+\ac{MLLT} and \ac{bMMI} is one of the best setup for non-speaker adaptive speech recognition.
To conclude, we reimplemented Kaldi batch speech recognition so it can perform on-line real-time speech recognition and maintain its quality.
The next Chapter~\ref{cha:integration} evaluates the recognisers performance in real-time usage in detail.
