% !TEX root = main.tex
\chapter{Kaldi \acs{ASR} in Alex \acs{SDS}}
\label{cha:integration}

This chapter discuss the details of deploying \term{OnlineLatgenRecogniser} into Alex dialogue system written in Python.
The \term{OnlineLatgenRecogniser} is used in Alex dialogue system for Czech Public Transport Domain available on public toll-free (+420) 800 899 998 line.

First, the architecture of Alex \acf{SDS} is described.
Second, Section~\ref{sec:asrsds} presents how the Python wrapper \term{PyOnlineLatgenRecogniser} was integrated into~\ac{SDS} Alex.
Finally, Section~\ref{sec:eval} evaluates the decoder in Alex dialogue system on Czech \ac{PTI} domain. 

\section{Alex dialogue system architecture} 
\label{sec:arch}
The Alex dialogue system is developed in Python programming language and consist of~six major components. 
\begin{enumerate}
    \item Voice Activity Detection (VAD)
    \item Automatic Speech Recognition (ASR) 
    \item Spoken Language Understanding (SLU)
    \item Dialog Manager (DM)
    \item Natural Language Generation (NLG)
    \item Text To Speech (TTS)
\end{enumerate}
The~Alex dialogue system has a~speech to speech user interface. 
The~system interacts with the user in \term{turns}. 
During a~single turn the dialogue system waits for a~user spoken input, processes the speech and generates the spoken response.
The~data~flow in a single turn is depicted in~Figure~\ref{fig:alex}.

Each of the Alex's component runs in separate process in order parallelize the input data processing and output data generation.
The components communicates among themselves through system pipes.

\begin{figure}
    \begin{center}
    \input{images/ds-diagram}
    \caption{Single turn in Alex dialogue system}
    \label{fig:alex} 
    \end{center}
\end{figure}

The Alex's framework is organised into several logical parts:
\begin{itemize}
    \item The core library is located at \term{alex/components/}. The library is domain and language independent. All components in Figure~\ref{fig:alex} are implemented in this core library.
    \item Settings and scripts for specific domain applications are located in \term{alex/applications/}.
        For example, there is a bash scripts \term{alex/applications/PublicTransportInfoCS/vhub\_live\_kaldi} which starts the Alex \ac{SDS} service which provides \acl{PTI} at the 800 899 998 toll free line.
    \item The general scripts which use external tools or data can be found in:
        \begin{itemize}
            \item \term{alex/corpustools/} directory which focuses on formatting and organising the collected data,
            \item and \term \term{alex/tools/} directory which stores code for modelling \ac{VAD}, \ac{ASR}. It also stores code for evaluation and \term{SIP} client.
        \end{itemize}
    \item Integration tests are stored in \term{alex/tests/}.
    \item The \term{alex/utils/} directory contains simple utilities for various purposes.
\end{itemize}

The components depicted in Figure~\ref{fig:alex} are represented as 
modules under \term{alex/components/} e.g. \term{alex/components/asr/},  \term{alex/components/tts/}, etc..
The~source code of the~components is very modular.
For example the \ac{ASR} component currently supports several \ac{ASR} engines wrapped in modules, which implement the base class \term{ASRInterface} methods. 
See Listing~\ref{snippets/asr_interface.py}.
The supported engines are:
\begin{itemize}
    \item OpenJulius \term{alex/components/asr/julius.py} interfaces OpenJulius decoder through sockets for on-line recognition.
    \item Google \term{alex/components/asr/google.py} uses cloud service for batch decoding.
    \item Kaldi \term{alex/components/asr/kaldi.py} imports \term{PyOnlineLatgenRecogniser} class and uses its methods and the utilities for on-line decoding.
\end{itemize}

Implementations of components can be easily change using simple configuration file. 
For example, by specifying the \ac{ASR} tool, \ac{AM} and other parameters one can choose between \ac{ASR} cloud based service, local OpenJulius or Kaldi speech recognition.

In order to prepare a specific application one need to train appropriate models for classifiers or create setup for cloud services, which implements Alex components functionality.
As an example, a public phone call service\footnote{On telephone number (+420) 800 899 998}, which provides \ac{PTI} and weather information in Czech language,  is developed in directory \term{alex/applications/PublicTransportInfoCS/}.
The Alex's framework provides source code for domain dependent training for models in \ac{VAD}, \ac{ASR}, \ac{SLU} and also \ac{DM}.



\section[Kaldi integration into \acs{SDS} framework]{Kaldi integration into Alex's \acl{SDS} framework}
\label{sec:asrsds}

Integration of the~Kaldi real-time recognizer into Alex's framework requires implementing new features:
\begin{enumerate}
    \item The \term{kaldi.py} module is implemented so the Alex's \ac{ASR} component can use \term{PyOnlineLatgenRecogniser} decoder through \term{ASRInterface}.
        See Subsection~\ref{sub:asr_component}.
    \item  The training scripts for \acp{AM} were prepared as described in Chapter~\ref{cha:train}.
    \item  The scripts for building custom decoding graph \term{HCLG} and evaluation of Alex's \ac{ASR} unit were developed.
        See Subsection~\ref{sub:hclg} and Section~\ref{sec:eval}.
\end{enumerate}

The decoding graph \term{HCLG} is necessary in order to run any Kaldi decoder, and the evaluation scripts are used to find out decoding parameters which balance quality and speed of speech recognition.

\subsection{\term{PyOnlineLatgenRecogniser} in Alex}
\label{sub:asr_component}
The \ac{ASR} component in the~Alex dialogue system runs as separate process, and the speech recognition is triggered based on \ac{VAD} decisions.

If \ac{VAD} detects starts of speech in input audio stream, it passes the audio data to \ac{ASR} component and \term{rec\_in} method is called. 
See Listing~\ref{snippets/asr_interface.py}.
The audio data are passed to the ASR component as instance of class \term{Frame} in \term{rec\_in} method.
The audio is forward decoded using beam search as user speaks.
The method \term{rec\_in} adds gradually the new audio in \term{PyOnlineLatgenRecogniser} buffer and calls forward decoding on new buffered audio.
The buffering and forward decoding could have been split into two steps, but the current setup works well.\footnote{If the \ac{ASR} component is busy with forward decoding and is not able to accept the audio from \ac{VAD} component the audio just waits in \ac{VAD} buffer instead of in \term{PyOnlineLatgenRecogniser}'s buffer.}

If \ac{VAD} recognises end of speech, no more data are sent to \ac{ASR} engine and \term{hyp\_out} method is called in order to extracted word lattice.
Extracting the word lattice is also referred as backward decoding since the data structures are traversed in time-synchronous backward.
In case of \term{PyOnlineLatgenRecogniser} word posterior probabilities are computed using forward-backward algorithm.
Then, the word posterior lattice in \term{PyFst} format is converted to an~n-best list represented as proprietary \term{UtteranceNBlist} n-best list.\footnote{In the future, we would like to implement keyword spotting using directly \term{pyfst} lattices in our \ac{SLU} unit.}
The conversion from a lattice to an~n-best list is very fast because the~\term{OpenFST} shortest path algorithm is used on lattices which usually contain only tens of words.

The \term{flush} method is used only if the speech recogniser wants to throw away the buffered audio input and reset the decoding.

\code{ASRInterface}{Python}{snippets/asr_interface.py}

The method \term{rec\_wav\_file} nicely illustrates how the two methods \term{rec\_in} and \term{hyp\_out} are used for decoding. 
Since the method is used only for testing purposes, it sends all input audio to the speech recogniser at once.
However, in real-time application the audio is passed to \term{PyOnlineLatgenRecogniser} in small chunks, so the forward decoding can run as a user speaks. 

In~the Kaldi real-time settings, latency of \ac{ASR} unit depends mostly on~the time spent in \term{hyp\_out} method.
In~the \term{hyp\_out} method a~word posterior lattice is extracted using the \term{PyOnlineLatgenRecogniser::GetLattice} method as described in~Subsection~\ref{sec:pyext}. 
For most cases the latency is well below 200 ms for our settings as illustrated in Figure~\ref{fig:prc}.

\todo{SIMPLE HIGH LEVEL POINT OF VIEW:
The integration of \term{PyOnlineLatgenRecogniser} is simple yet effective.
However, be aware of triggering backward decoding too often. 
It may seem that a user should wait to response of the Alex dialogue system before he speaks again and consequently new audio is buffered to recognition.
However in practice, users speak spontaneously; quite often an user finishes utterance and immediately starts speaking in order to change his request.
The spontaneous speech may cause delays of \ac{ASR} unit since the forward and backward decoding run in single process.
In case of very short consecutive utterances, the processor time which is meant for backward decoding should also be used to forward decoding.
}

We notices the problem for chains of noises detected in \ac{VAD} components as multiple short utterances.
The backward decoding (\term{rec\_in} method) was extracting hypothesis while the audio input was still buffering the noisy speech and want to call \term{rec\_in}.
The~backward decoding (\term{hyp\_out} method) was called so often that almost no forward decoding was performed.
The problem was solved by improving \ac{VAD} so the~\term{hyp\_out} method is not triggered so often.
It is useful to target settings for a~forward decoding \ac{RTF} smaller than 1.0 e.g. 0.6, because with reduced RTF the~forward decoding can catch up the~delay caused by backward decoding the~previous utterance.
See Section~\ref{sec:eval} for \ac{RTF} definition.

\subsection{Building in-domain decoding graph}
\label{sub:hclg}
We developed scripts for building a decoding graph and other necessary files for decoding with \term{PyOnlineLatgenRecogniser}.\footnote{The evaluation scripts and \term{HCLG} scripts are located in \term{alex/applications/PublicTransportInfoCS/hclg}.}
In order to build the decoding graph the best \ac{AM} obtained by training scripts  and an in-domain \ac{LM} are used.
The scripts are based on standard Kaldi utilities.
If new \ac{LM} or \ac{AM} needs to be tested, new \term{HCLG} graph has to be built.
The decoding graph also needs to specified in configuration file before launching Alex dialogue system with Kaldi \ac{ASR} engine.

\subsubsection*{Acoustic and language models used}
\label{sec:ptilm}
The~\term{OnlineLatgenRecogniser} is evaluated on a~corpus of audio data from the~Public Transport Information (PTI) domain.
% The phone call audio data were collected from Alex's users, so the test data simulates the real setup perfectly.
In PTI, users can interact in Czech language with a~telephone-based dialogue system to find public transport connections \cite{ptics2014url}.
The~PTI corpus consist of approximately 12k user utterances with a length varying between 0.4 s and 18 s with median around 3 s.
The~data were divided into training, development, and test data where the~corresponding data sizes were 9496, 1188, 1188 respectively.
For evaluation, a~domain specific class-based language model with a vocabulary size of approximately 52k  and 559k n-grams was estimated from the~training data.
Named entities e.g., cities or bus stops, in class-based language model are expanded before building a~decoding graph.
The~perplexity of the~resulting language model evaluated on the~development data is about 48.

Since the~PTI acoustic data amounts to less then 5 hours, the~acoustic training data was extended by additional 15 hours of telephone out-of-domain data from VYSTADIAL 2013 - Czech corpus \cite{korvas_2014}.
The~acoustic models were obtained by BMMI discriminative training with LDA and MLLT feature transformations.
% The~scripts used to train the~acoustic models are publicly available in ASDF \cite{asdf2014url} as well as in Kaldi \cite{kaldi2014url} and 
A~detailed description of the~training procedure is given in Chapter~\ref{cha:train}. 




\section{Evaluation of \term{PyOnlineLatgenRecogniser} in Alex}
\label{sec:eval}
We focus on evaluating the~speed of the \term{OnlineLatgenRecogniser} and its relationship with the accuracy of the~decoder.
% The decoding graph built from \ac{LM} and \ac{AM} described in Subsection~\ref{sec:ptilm}.
We evaluate following measures:
\begin{itemize}
    \item Real Time Factor (RTF) of decoding -- the ratio of the~recognition time to the~duration of the~audio input,
    \item Latency -- the~delay between utterance end and the~availability of the~recognition results,
    \item Word Error Rate (WER).
\end{itemize}

% DONE \todo{only non speed parameter LMW}
Accuracy and speed of the~\term{OnlineLatgenRecogniser} are controlled by the \term{max-active-states},   \term{beam}, and \term{lattice-beam} parameters \cite{povey2011kaldi}.
\term{Max-active-states} limits the~maximum number of active tokens during decoding.
\term{Beam} is used during graph search to prune ASR hypotheses at the~state level.
\term{Lattice-beam} is used when producing word level lattices after the~decoding is finished.
It is crucial to tune these parameters optimally to obtain good results.

In general, one aims for a \ac{RTF} smaller than 1.0.
Moreover, in practice, it is useful if the~RTF is even smaller because other processes running on the machine can influence the~amount of available computational resources.
Therefore, we target the~RTF of 0.6 in our setup.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{beam_vs_rtfwer.pdf.ps}
    \includegraphics[scale=0.5]{latbeam_vs_latwer.pdf.ps}
    \caption{The~upper graph (a) shows that WER decreases with increasing \term{beam} and the~average RTF linearly grows with the~beam.
    Setting the~maximum number of active states to 2000 stops the~growth of the~95th RTF percentile at 0.6, indicating that even in the~worst case, we can guarantee an~RTF around 0.6.
    The~lower graph (b) shows how latency grows in response to increasing \term{lattice-beam}.}
    \label{fig:wer} 
    \end{center}
\end{figure*}

We used grid search on the~test set to identify optimal parameters.
Figure~\ref{fig:wer} (a) shows the~impact of the~\term{beam} on the~WER and RTF measures.
In this case, we set \term{max-active-states} to 2000 in order to limit the~worst case RTF to 0.6.
Observing Figure~\ref{fig:wer} (a), we set \term{beam} to 13 as this setting balances the~WER.%, 95th RTF percentile, and the~average RTF.
% Therefore, for Figure~\ref{fig:wer} (b) as this setting
Figure~\ref{fig:wer} (b) shows the~impact of the~\term{lattice-beam} on WER and latency when \term{beam} is fixed to 13.
We set \term{lattice-beam} to 5 based on Figure~\ref{fig:wer} (b) to obtain the~95th latency percentile of 200 ms, which is considered natural in a~dialogue \cite{skantze2009incremental}.
\term{Lattice-beam} does not affect WER, but larger \term{lattice-beam} improves the~oracle WER of generated lattices~\cite{povey2012generating}.
Richer lattices may improve \ac{SLU} performance.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{frtf_vs_prc.pdf.ps}
    \includegraphics[scale=0.5]{lat_vs_prc.pdf.ps}
    \caption{The~percentile graphs show RTF and Latency scores for test data for \term{max-active-sates}=2000, \term{beam}=13, \term{lattice-beam}=5.
Note that 95 \% of utterances were decoded with the~latency lower that 200ms.}
    \label{fig:prc}
    \end{center}
\end{figure*}

% Figure~\ref{fig:wer}
% The~\term{beam} and \term{lattice-beam} parameters significantly effect accuracy and speed of decoding.
% First, we set \term{max-active-states} to 2000 to limit worst case RTF to 0.6. 
% Second, we tune \term{beam} 

% We fixed other parameters or used the~default values since other parameters have just technical character or does not influence the~speed and quality of decoding.\footnote{The~only exception is language model weight which affects WER, we set it up for our LM to 15.}
% The~only exception is the~language model weight (LMW) parameter, which does not effect the~speed of the~decoding but only the~output probabilities determined by mixing AM and LM models.
% As illustrated in Figure~\ref{fig:wer}, further increasing the~\term{beam} away from Alex \term{beam} configuration would significantly slow down decoding but reduce a~little WER.
% Similarly, the~Alex \term{lattice-beam} setup balances between larger \term{lattice-beam} and quickly increasing latency.

Figure~\ref{fig:prc} shows the~percentile graph of the~RTF and latency measures over the~test set.
For example, the~95th percentile is the~value of a~measure such that 95\% of the~data has the~measure below that value.
One can see from Figure~\ref{fig:prc} that 95\% of test utterances is decoded with RTF under 0.6 and latency under 200 ms.
The~extreme values are typically caused by decoding long noisy utterances where uncertainty in decoding slows down the~recogniser.
Using this setting, the~\term{OnlineLatgenRecogniser} decodes the~test utterances with a WER of about 21\%.

% In ADSF a~Google ASR service had been used, because we did not have acoustic training data for AM training.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{lat_cloud_kaldi.pdf.ps}
    \caption{Shorter latency of custom on-line decoder (OnlineLatgenRecogniser) over batch decoding with cloud service (Google ASR service).}
    \label{fig:wer} 
    \end{center}
\end{figure*}

In addition, we have also evaluated Google ASR service as we used it previously in Alex \ac{SDS}.
The~Google ASR service decoded the~test utterances from the~PTI domain with 95\% latency percentile of 1900ms and it reached WER about 48\%.
The~high latency is presumably caused by the~batch processing of audio data and network latency, and the~high WER is likely caused by a mismatch between Google's acoustic and language models and the~test data.

\subsection*{Results}
\label{sec:results}
Based on evaluation we selected the~best~setup\footnote{Setup: \term{beam} 12, \term{lattice-beam} 5, \term{max-active-states} 2000.} for ASR component in Alex Dialogue System Framework with  WER under 22 \%, latency less tha 200 ms and RTF under 0.6 on \ac{PTI} domain.
To conclude, the~\term{OnlineLatgenRecogniser} performs significantly better than our previous \ac{ASR} engines: OpenJulius decoder or queries to cloud Google ASR service.

