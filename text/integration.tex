% !TEX root = main.tex
\chapter{Kaldi \acs{ASR} in Alex \acs{SDS}}
\label{cha:integration}

This chapter discuss the details of deploying \term{OnlineLatgenRecogniser} into Alex dialogue system.
The \term{OnlineLatgenRecogniser} is used in Alex dialogue system for Czech Public Transport Domain available on public toll-free (+420) 800 899 998 line.

First, the architecture of Alex \acf{SDS} is described.
Second, Section~\ref{sec:asrsds} presents how the wrapper \term{PyOnlineLatgenRecogniser} is integrated into~\ac{SDS} Alex.
Finally, Section~\ref{sec:eval} evaluates the decoder in Alex dialogue system on Czech \ac{PTI} domain. 

\section{Alex dialogue system architecture} 
\label{sec:arch}

The~Alex dialogue system has a~speech to speech user interface. 
The Alex dialogue system is developed in Python programming language and consists of~six major components. 
\begin{enumerate}
    \item \acf{VAD} 
    \item \acf{ASR}
    \item \acf{SLU}
    \item \acf{DM}
    \item \acf{NLG}
    \item \acf{TTS}
\end{enumerate}
The~system interacts with the user in \term{turns}. 
The schema in~Figure~\ref{fig:alex} illustrates how the user's input is processed in single turn.
The spoken input is passed to \ac{ASR} component which generates corresponding textual representation.
\ac{SLU} extracts semantic features from the text and \ac{DM} decides which response to present.
The \ac{NLG} component generates textual response from an internal representation of \ac{DM} and finally the \ac{TTS} read the text with human voice.

Each of the Alex's component runs in separate process in order parallelize the input data processing and output data generation.
The components communicates among themselves through system pipes.

\begin{figure}
    \begin{center}
    \input{images/ds-diagram}
    \caption{Single turn in Alex dialogue system}
    \label{fig:alex} 
    \end{center}
\end{figure}

In order to prepare \ac{ASR} unit for \term{PyOnlineLatgenRecogniser} we have implemented not only the wrapper itself, but also preparation scripts and useful utilities. 
Let us introduce the Alex \ac{SDS} framework organisation, so we can better explain how our scripts are used.
The framework is separated into several logical parts:
\begin{itemize}
    \item The core library is located at \term{alex/components/}. The library is domain and language independent. All components in Figure~\ref{fig:alex} are implemented in this core library.
    \item Settings and scripts for specific domain applications are located in \term{alex/applications/}.
        For example, application for \ac{PTI} domain can be found in \term{alex/applications/PublicTransportInfoCS/} directory.
        % and started by simple scripts \term{vhub\_live\_kaldi} which starts the Alex \ac{SDS} service which provides \acl{PTI} at the 800 899 998 toll free line.
    \item The scripts which use external tools or data can be found in:
        \begin{itemize}
            \item \term{alex/corpustools/} directory which focuses on formatting and organising the collected data,
            \item and \term \term{alex/tools/} directory which stores code for modelling \ac{VAD}, \ac{ASR}, \term{SIP} client, etc.
        \end{itemize}
    \item Integration tests are stored in \term{alex/tests/}.
    \item The \term{alex/utils/} directory contains simple utilities for various purposes.
\end{itemize}

The components depicted in Figure~\ref{fig:alex} are represented as Python modules under \term{alex/components/} e.g. \term{alex/components/asr/},  \term{alex/components/tts/}, etc..
The~source code of the~components is very modular.
For example the \ac{ASR} component currently supports several \ac{ASR} recognisers.
The recognisers are used from dedicated Python modules which implement the common interface of a base class \term{ASRInterface}. 
See Listing~\ref{snippets/asr_interface.py}.
The supported speech recognisers are:
\begin{itemize}
    \item OpenJulius \term{alex/components/asr/julius.py} interfaces OpenJulius decoder through sockets for on-line recognition.
    \item Google \term{alex/components/asr/google.py} uses cloud service for batch decoding.
    \item Kaldi \term{alex/components/asr/kaldi.py} imports \term{PyOnlineLatgenRecogniser} class and uses its methods and the utilities for on-line decoding.
\end{itemize}

One can easily choose an \ac{ASR} recogniser in Alex configuration file.
The configuration file is also the right place to specify \ac{AM} and \ac{LM} and the speech recognition parameters if necessary.
% For example, by specifying the \ac{ASR} tool, \ac{AM} and other parameters one can choose between \ac{ASR} cloud based service, local OpenJulius or Kaldi speech recognition.

In order to prepare a specific application with \term{PyOnlineLatgenRecogniser} one need to train \ac{AM} and \ac{LM} and also train \ac{SLU} unit based on the \ac{ASR} unit outputs.
The \ac{LM} model training and consequently \ac{SLU} training is very domain specific so the scripts are deployed for each application separately.
For example, the scripts for \ac{LM} and \ac{SLU} model training for \ac{PTI} domain are located under directory \term{alex/applications/PublicTransportInfoCS/} in directories \term{lm/} and \term{slu/}.

\section[Kaldi integration into \acs{SDS} framework]{Kaldi integration into Alex's \acl{SDS} framework}
\label{sec:asrsds}

Integration of the~Kaldi real-time recognizer into Alex's framework requires implementing following features:
\begin{enumerate}
    \item The \term{kaldi.py} module which exploits functionality of \term{PyOnlineLatgenRecogniser} and implements the abstract \term{ASRInterface}.
    \item The training scripts for \acp{AM}.
    \item The scripts for building custom decoding graph \term{HCLG}, which is Kaldi effective representation for \ac{AM} and \ac{LM} used for decoding. 
    \item Evaluation of the \ac{ASR} recogniser in Alex, so the best one can be selected.
\end{enumerate}

The \term{PyOnlineLatgenRecogniser} integration in \term{kaldi.py} is described in Subsection~\ref{sub:asr_component}.
The training scripts for training \acp{AM} were described in Chapter~\ref{cha:train}.
However, note that we adjusted their directory structure and copied them into \term{alex/tools/kaldi} directory so they nicely integrates with Alex tools.
The scripts for building the \term{HCLG} decoding graph are introduced in~Subsection~\ref{sub:hclg}.
They are stored at \term{alex/applications/PublicTransportInfoCS/hclg/}.
Finally, the Section~\ref{sec:eval} evaluates the performance of \term{PyOnlineLatgenRecogniser} in Alex \ac{SDS} and briefly compares it with module which uses Google speech recognition service.

\subsection{\term{PyOnlineLatgenRecogniser} in Alex}
\label{sub:asr_component}
The \ac{ASR} component in the~Alex dialogue system runs as separate process, and the speech recognition is triggered based on \ac{VAD} decisions.

If \ac{VAD} detects start of speech in input audio stream, it passes the speech signal to \ac{ASR} component and \term{rec\_in} method is called. 
See Listing~\ref{snippets/asr_interface.py}.
% The audio data are passed to the ASR component as instance of class \term{Frame} in \term{rec\_in} method.
The audio is decoded using beam search as user speaks.
The method \term{rec\_in} gradually adds the new audio in \term{PyOnlineLatgenRecogniser} buffer and immediately decodes it.
The buffering and decoding could have been split into two steps, but the current setup works well.\footnote{If the \ac{ASR} component is busy with decoding and is not able to accept the audio from \ac{VAD} component the audio just waits in \ac{VAD} buffer instead of in \term{PyOnlineLatgenRecogniser}'s buffer.}

If \ac{VAD} recognises end of speech, no more data are sent to \ac{ASR} engine and \term{hyp\_out} method is called in order to extracted word lattice.
Extracting the word lattice is also referred as backward decoding since the data structures are traversed in time-synchronous backward.
In case of \term{PyOnlineLatgenRecogniser} word posterior probabilities are computed using forward-backward algorithm.
Then, the word posterior lattice in \term{PyFst} format is converted to an~n-best list.\footnote{We would like to implement direct keyword spotting from \term{pyfst} lattices in Alex \ac{SLU} unit in future.}
%represented as proprietary \term{UtteranceNBlist} n-best list.

The \term{flush} method is used only if the speech recogniser wants to throw away the buffered audio input and reset the decoding.

\code{ASRInterface}{Python}{snippets/asr_interface.py}

\todo{The method \term{rec\_wav} nicely illustrates how the two methods \term{rec\_in} and \term{hyp\_out} are used for decoding. 
Since the method is used only for testing purposes, it sends all input audio to the speech recogniser at once.
However, in real-time application the audio is passed to \term{PyOnlineLatgenRecogniser} in small chunks, so the decoding can run as a user speaks. 
}

In~the Kaldi real-time settings, latency of \ac{ASR} unit depends mostly on~the time spent in \term{hyp\_out} method.
In~the \term{hyp\_out} method a~word posterior lattice is extracted using the \term{PyOnlineLatgenRecogniser::GetLattice} method as described in~Subsection~\ref{sec:pyext}. 
For most cases the latency is well below 200 ms for our settings as illustrated in Figure~\ref{fig:prc}.

\todo{SIMPLE HIGH LEVEL POINT OF VIEW:
The integration of \term{PyOnlineLatgenRecogniser} is simple yet effective.
However, be aware of triggering backward decoding too often. 
It may seem that a user should wait to response of the Alex dialogue system before he speaks again and consequently new audio is buffered to recognition.
However in practice, users speak spontaneously; quite often an user finishes utterance and immediately starts speaking in order to change his request.
The spontaneous speech may cause delays of \ac{ASR} unit since the speech decoding and lattice extraction run in a single process.
In case of very short consecutive utterances, the processor time which is meant for lattice extraction should also be used to decoding.
}

\todo{
We notices the problem for chains of noises detected in \ac{VAD} components as multiple short utterances.
The backward decoding (\term{rec\_in} method) was extracting hypothesis while the audio input was still buffering the noisy speech and want to call \term{rec\_in}.
The~backward decoding (\term{hyp\_out} method) was called so often that almost no forward decoding was performed.
The problem was solved by improving \ac{VAD} so the~\term{hyp\_out} method is not triggered so often.
It is useful to target settings for a~forward decoding \ac{RTF} smaller than 1.0 e.g. 0.6, because with reduced RTF the~forward decoding can catch up the~delay caused by backward decoding the~previous utterance.
See Section~\ref{sec:eval} for \ac{RTF} definition.
}

\subsection{Building in-domain decoding graph}
\label{sub:hclg}
We developed scripts for building a decoding graph. 
A decoding graph is a graph represented as an OpenFst object it stores all the \ac{LM} model information and part of information for acoustic modelling. 
The decoding graph is necessary for decoding with Kaldi decoders.
We build the \term{HCLG} graph using standard OpenFst operations which are implemented in Kaldi utilities. 

We designed our scripts so they automatically update newly built \acp{AM} and \acp{LM} and create all files necessary for decoding with \term{OnlineLatgenRecogniser}.
The same files can also be used with standard Kaldi decoders or \term{PyOnlineLatgenRecogniser}.

The \term{HCLG} build script requires:
\begin{itemize}
    \item \todo{Fill the files}
\end{itemize}

The \term{HCLG} build script produces follwing files which are required for decoding with \term{PyOnlineLatgenRecogniser}:
\begin{itemize}
    \item \todo{Fill the files}
\end{itemize}

We also developed evaluation scripts which simply computes the statistics which are evaluated given \ac{AM}, \ac{LM} and parameters.
The evaluation itself is describe Section~\ref{sec:eval}.
Both the evaluation scripts and build \term{HCLG} script are located in the~\term{alex/applications/PublicTransportInfoCS/hclg/} directory.

\subsubsection*{Acoustic and language models for \acs{PTI} domain}
\label{sec:ptilm}
The~\term{OnlineLatgenRecogniser} is evaluated on a~corpus of audio data from the~Public Transport Information (PTI) domain.
% The phone call audio data were collected from Alex's users, so the test data simulates the real setup perfectly.
In PTI, users can interact in Czech language with a~telephone-based dialogue system to find public transport connections \cite{ptics2014url}.
The~PTI corpus consist of approximately 12k user utterances with a length varying between 0.4 s and 18 s with median around 3 s.
The~data were divided into training, development, and test data where the~corresponding data sizes were 9496, 1188, 1188 respectively.
For evaluation, a~domain specific class-based language model with a vocabulary size of approximately 52k  and 559k n-grams was estimated from the~training data.
Named entities e.g., cities or bus stops, in class-based language model are expanded before building a~decoding graph.
The~perplexity of the~resulting language model evaluated on the~development data is about 48.

Since the~PTI acoustic data amounts to less then 5 hours, the~acoustic training data was extended by additional 15 hours of telephone out-of-domain data from VYSTADIAL 2013 - Czech corpus \cite{korvas_2014}.
The~acoustic models were obtained by BMMI discriminative training with LDA and MLLT feature transformations.
% The~scripts used to train the~acoustic models are publicly available in ASDF \cite{asdf2014url} as well as in Kaldi \cite{kaldi2014url} and 
A~detailed description of the~training procedure is given in Chapter~\ref{cha:train}. 


\section{Evaluation of \term{PyOnlineLatgenRecogniser} in Alex}
\label{sec:eval}
We focus on evaluating the~speed of the \term{OnlineLatgenRecogniser} and its relationship with the accuracy of the~decoder.
% The decoding graph built from \ac{LM} and \ac{AM} described in Subsection~\ref{sec:ptilm}.
We evaluate following measures:
\begin{itemize}
    \item Real Time Factor (RTF) of decoding -- the ratio of the~recognition time to the~duration of the~audio input,
    \item Latency -- the~delay between utterance end and the~availability of the~recognition results,
    \item Word Error Rate (WER).
\end{itemize}

% DONE \todo{only non speed parameter LMW}
Accuracy and speed of the~\term{OnlineLatgenRecogniser} are controlled by the \term{max-active-states},   \term{beam}, and \term{lattice-beam} parameters \cite{povey2011kaldi}.
\term{Max-active-states} limits the~maximum number of active tokens during decoding.
\term{Beam} is used during graph search to prune ASR hypotheses at the~state level.
\term{Lattice-beam} is used when producing word level lattices after the~decoding is finished.
It is crucial to tune these parameters optimally to obtain good results.

In general, one aims for a \ac{RTF} smaller than 1.0.
Moreover, it is useful in practice if the~RTF is even smaller because other processes running on the machine can influence the~amount of available computational resources.
Therefore, we target the~RTF the lower value of 0.6, which was estimated by informal experiments.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{beam_vs_rtfwer.pdf.ps}
    \includegraphics[scale=0.5]{latbeam_vs_latwer.pdf.ps}
    \caption{The~upper graph (a) shows that WER decreases with increasing \term{beam} and the~average RTF linearly grows with the~beam.
        The~growth of the~95th RTF percentile is limited at 0.6 by setting \term{max-active-states} to 2000, because the \term{max-active-states} parameters influence presumably the~worst cases with large search space.
    The~lower graph (b) shows latency growth in response to increasing \term{lattice-beam}.}
    \label{fig:wer} 
    \end{center}
\end{figure*}

\todo{Reread follwing paragraph-inspire in paper}
We used grid search on the~test set to identify optimal parameters.
Figure~\ref{fig:wer} (a) shows the~impact of the~\term{beam} on the~WER and RTF measures.
In this case, we set \term{max-active-states} to 2000 in order to limit the~worst case RTF to 0.6.
Observing Figure~\ref{fig:wer} (a), we set \term{beam} to 13 as this setting balances the~WER.%, 95th RTF percentile, and the~average RTF.
% Therefore, for Figure~\ref{fig:wer} (b) as this setting
Figure~\ref{fig:wer} (b) shows the~impact of the~\term{lattice-beam} on WER and latency when \term{beam} is fixed to 13.
We set \term{lattice-beam} to 5 based on Figure~\ref{fig:wer} (b) to obtain the~95th latency percentile of 200 ms, which is considered natural in a~dialogue \cite{skantze2009incremental}.
\term{Lattice-beam} does not affect WER, but larger \term{lattice-beam} improves the~oracle WER of generated lattices~\cite{povey2012generating}.
Richer lattices may improve \ac{SLU} performance.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{frtf_vs_prc.pdf.ps}
    \includegraphics[scale=0.5]{lat_vs_prc.pdf.ps}
    \caption{The~percentile graphs show RTF and Latency scores for test data for \term{max-active-sates}=2000, \term{beam}=13, \term{lattice-beam}=5.
Note that 95 \% of utterances were decoded with the~latency lower that 200ms.}
    \label{fig:prc}
    \end{center}
\end{figure*}

% Figure~\ref{fig:wer}
% The~\term{beam} and \term{lattice-beam} parameters significantly effect accuracy and speed of decoding.
% First, we set \term{max-active-states} to 2000 to limit worst case RTF to 0.6. 
% Second, we tune \term{beam} 

% We fixed other parameters or used the~default values since other parameters have just technical character or does not influence the~speed and quality of decoding.\footnote{The~only exception is language model weight which affects WER, we set it up for our LM to 15.}
% The~only exception is the~language model weight (LMW) parameter, which does not effect the~speed of the~decoding but only the~output probabilities determined by mixing AM and LM models.
% As illustrated in Figure~\ref{fig:wer}, further increasing the~\term{beam} away from Alex \term{beam} configuration would significantly slow down decoding but reduce a~little WER.
% Similarly, the~Alex \term{lattice-beam} setup balances between larger \term{lattice-beam} and quickly increasing latency.

Figure~\ref{fig:prc} shows the~percentile graphs of the~RTF and latency measures over the~test set.
The~95th percentile is the~value of a~measure such that 95\% of the~data has the~measure below that value.
One can see from Figure~\ref{fig:prc} that 95\% of test utterances is decoded with RTF under 0.6 and latency under 200 ms.
The~extreme values for 5\% of test utterances are in most cases caused by decoding long noisy utterances where uncertainty in decoding increase the search space slows down the~recogniser.
Using \term{beam} of 13, the \term{lattice-beam} of 5 and 2000 \term{max-active-states} , the~\term{OnlineLatgenRecogniser} decodes the~test utterances with a WER of about 21\%.

% In ADSF a~Google ASR service had been used, because we did not have acoustic training data for AM training.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{lat_cloud_kaldi.pdf.ps}
    \caption{Almost constant latency of on-line decoder (OnlineLatgenRecogniser) and linearly growing latency of cloud based speech recogniser (Google ASR service) for increasing utterance length.}
    \label{fig:wer} 
    \end{center}
\end{figure*}

In addition, we have also evaluated Google ASR service as we used it previously in Alex \ac{SDS}.
The~Google ASR service decoded the~test utterances from the~PTI domain with 95\% latency percentile of 1900ms and it reached WER about 48\%.
The~high latency is presumably caused by the~batch processing of audio data and network latency, and the~high WER is likely caused by a mismatch between Google's acoustic and language models and the~test data.

\subsection*{Results}
\label{sec:results}

\todo{integrated hclg, training scripts and prepared evaluation}

Based on evaluation we selected the~best~setup\footnote{Setup: \term{beam} 12, \term{lattice-beam} 5, \term{max-active-states} 2000.} for ASR component in Alex Dialogue System Framework with  WER under 22 \%, latency less tha 200 ms and RTF under 0.6 on \ac{PTI} domain.
To conclude, the~\term{OnlineLatgenRecogniser} performs significantly better than our previous \ac{ASR} engines: OpenJulius decoder or queries to cloud Google ASR service.

