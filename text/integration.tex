% !TEX root = main.tex
\chapter{Kaldi \acs{ASR} in Alex \acs{SDS}}
\label{cha:integration}

The Python wrapper of \term{OnlineLatgenRecogniser} is used as \ac{ASR} engine in Alex dialogue system for Czech Public Transport Domain available on public toll-free (+420) 800 899 998 line.
This chapter discuss the details of interfacing \term{OnlineLatgenRecogniser} in Alex dialogue system.

At first, the architecture of Alex \acf{SDS} is described.
Secondly, Section~\ref{sec:asrsds} presents how the Python wrapper \term{PyOnlineLatgenRecogniser} was integrated into~\ac{SDS} Alex.
Finally, Section~\ref{sec:eval} evaluates the decoder in Alex dialogue system on Czech \ac{PTI} domain. 

\section{Alex dialogue system architecture} 
\label{sec:arch}
The Alex dialogue system is developed in Python language and consist of~six major components. 
\begin{enumerate}
    \item Voice Activity Detection (VAD)
    \item Automatic Speech Recognition (ASR) 
    \item Spoken Language Understanding (SLU)
    \item Dialog Manager (DM)
    \item Natural Language Generation (NLG)
    \item Text To Speech (TTS)
\end{enumerate}
The~Alex dialogue system has a~speech to speech user interface. 
The~system interacts with the user in \term{turns}. 
During a~single turn the dialogue system waits for a~user spoken input, process the speech and generates the reply.
The~data~flow during single turn is depicted in~Figure~\ref{fig:dialogue_system}.

Each of the Alex's component runs in separate process in order parallelize the input data processing and data generation.
The components communicates among themselves through system pipes.

\begin{figure}
    \begin{center}
    \input{images/ds-diagram}
    \caption{Single turn in Alex dialogue system}
    \label{fig:alex} 
    \end{center}
\end{figure}

The Alex's framework is organised into several logical parts which are reflected on file system organisation:
\begin{itemize}
    \item The core domain and language independent library is stored at \term{alex/components}
    \item Implemented applications for specific domain are located at \term{alex/applications}.
        For example the bash scripts \term{alex/applications/PublicTransportInfoCS/vhub\_live\_kaldi} starts the job which handles the 800 899 998 toll free line.
    \item The source code using external tools or data is stored at:
        \begin{itemize}
            \item \term{alex/corpustools} directory which focuses on formatting and organising the collected data.
            \term \term{alex/tools} which stores code for modelling \ac{VAD}, \ac{ASR}. It also stores code for evaluation and \term{SIP} client.
        \end{itemize}
    \item Integration tests are stored in \term{alex/tests}
    \item The \term{alex/utils} directory stores simple utilities for various purposes.
\end{itemize}

The components depicted in Figure~\ref{fig:alex} are represented as 
modules under \term{alex/components/} e.g. \term{alex/components/asr},  \term{alex/components/tts}.
The source code of components are completely language and domain independent and is very modular.
For example the \ac{ASR} component currently supports several \ac{ASR} engines wrapped in several modules
implementing the \term{base class ASRInterface} methods. 
The supported engines are:
\begin{itemize}
    \item OpenJulius \term{alex/components/asr/julius.py} interfaces OpenJulius decoder through sockets for on-line recognition.
    \item Google \term{alex/components/asr/google.py} uses cloud service for batch decoding.
    \item Kaldi \term{alex/components/asr/kaldi.py} imports \term{PyOnlineLatgenRecogniser} class and uses its methods and the utilities for on-line decoding.
\end{itemize}

In order to prepare a concrete application such as a public phone call service about public transport and weather in Czech republic, you need to train appropriate models if using custom classifiers or create setup for cloud services.
The Alex's framework provides source code for domain dependent training for models in \ac{VAD}, \ac{ASR}, \ac{SLU} and also \ac{DM}.
The various models for same component can be easily change using simple configuration file. 
For example, by specifying the \ac{ASR} tool, \ac{AM} and other parameters one can choose between \ac{ASR} cloud based service, local OpenJulius or Kaldi speech recognition.



\section[Kaldi integration into \acs{SDS} framework]{Kaldi integration into Alex's \acl{SDS} framework}
\label{sec:asrsds}

Integration of Kaldi into Alex's framework consists from three tasks.
Most importantly, we implemented \term{kaldi.py} module so the Alex's \ac{ASR} component can use \term{PyOnlineLatgenRecogniser} decoder through \term{ASRInterface}.
See Subsection~\ref{sub:asr_component}.
Secondly, we prepared the training scripts for \acp{AM} as described in Chapter~\ref{cha:train}.
Last but not least, the scripts for building custom decoding graph \term{HCLG} and evaluation were developed.
The decoded graph \term{HCLG} is constructed for \ac{AM} trained using scripts described in Chapter{cha:train} and fine tuned \ac{LM} for Alex's public transport domain.
See Subsection~\ref{sub:hclg}.
The evaluation scripts are used to find out decoding parameters which balance speech recognition quality and speed as described further in Section~\ref{sec:eval}.

\subsection{\term{PyOnlineLatgenRecogniser} in Alex}
\label{sub:asr_component}
The \ac{ASR} component in Alex dialogue system runs as separate process, and the speech recognition is triggered based on \ac{VAD} decisions.

If \ac{VAD} detects starts of speech in input audio stream, it passes the data to \ac{ASR} component
and \term{rec\_in} method is called. See Listing~\ref{snippets/asr_interface.py}.
The audio data are passed to the ASR component as\term{frame} argument of \term{frame\_in} method.
The speech chunks are passed to the recogniser gradually as the \ac{VAD} is detecting spoken input.

The method \term{rec\_in} adds the new audio in \term{PyOnlineLatgenRecogniser} buffer and calls forward decoding on all buffered audio.
The buffering and forward decoding could have been split into two steps, but the current setup works well.
If the \ac{ASR} component is busy with forward decoding and is not able to accept the audio from \ac{VAD} component the audio just waits in \ac{VAD} buffer instead of in \term{PyOnlineLatgenRecogniser}'s buffer.

If \ac{VAD} recognises end of speech, no more data are sent to \ac{ASR} engine and \term{hyp\_out} method is called.
The \term{flush} method is used only if the speech recogniser wants to throw away the buffered audio input and reset the decoding.

\code{ASRInterface}{Python}{snippets/asr_interface.py}

The method \term{rec\_wav\_file} is only used for testing purposes but nicely illustrates how the two methods \term{rec\_in} and \term{hyp\_out} are used for decoding. 
However, in real-time application the audio is passed to \term{PyOnlineLatgenRecogniser} in small chunks, so the forward decoding can run as a user speaks. 

In the Kaldi real-time settings, latency of \ac{ASR} unit depends only on~the time of decoding the last audio chunk and time spent in \term{hyp\_out} method.
In the \term{hyp\_out} method the word posterior lattice is extracted using the \term{PyOnlineLatgenRecogniser::GetLattice} method as described in~Subsection~\ref{sec:pyext}. 
The word posterior lattice in \term{PyFst} format is converted to n-best list represented as proprietary \term{UtteranceNBlist}.\footnote{In the future, we would like to implement keyword spotting using directly \term{pyfst} lattices in our \ac{SLU} unit.}
The conversion from lattice to n-best list is very fast because \term{OpenFST} shortest path algorithm is used and the lattices contain usually only tens of words.
For most cases the latency is deep below 200 ms for our settings as illustrated in Figure~\ref{fig:prc}.

For Kaldi \ac{ASR} component be aware of triggering backward decoding too often. 
It may seem that a user should wait to response of the Alex dialogue system before he speaks again and consequently new audio is buffered to recognition.
However, very often in spontaneous speech user finishes utterance and immediately starts start speaking again presumably changing his requests.
The problem is that the \ac{ASR} runs in one process and the processor time which meant to be reserved for backward decoding should also be used to forward decoding.
Otherwise, the second utterance is decoded with delay.

We experienced such problem for chains of noises detected in \ac{VAD} components as multiple short utterances.
The backward decoding (\term{rec\_in} method) was extracting hypothesis while the audio input was still buffering the noisy speech and want to call \term{rec\_in}.
The~backward decoding (\term{hyp\_out} method) was called so often that almost no forward decoding was performed.
It is useful to target settings for a~forward decoding \ac{RTF} smaller than 1.0 e.g. 0.6, because with reduced RTF the~forward decoding can catch up the~delay caused by backward decoding the~previous utterance.
The problem was solved by improving \ac{VAD} so the~\term{hyp\_out} method is not triggered so often.

\subsection{Building in-domain decoding graph}
\label{sub:hclg}
The scripts for building a decoding graph prepares all necessary files for decoding with \term{PyOnlineLatgenRecogniser}.
For building the decoding graph the best \ac{AM} obtained by training scripts  and an in-domain \ac{LM} are used.
The decoding graph is used for evaluation or for running the speech recognition in the Alex dialogue system.\footnote{The scripts for building \term{HCLG} graph and for evaluation are located in \term{alex/applications/PublicTransportInfoCS/hclg}.}

\subsubsection*{Acoustic and language models used}
\label{sec:ptilm}
The~\term{OnlineLatgenRecogniser} is evaluated on a~corpus of audio data from the~Public Transport Information (PTI) domain.
In PTI, users can interact in Czech language with a~telephone-based dialogue system to find public transport connections \cite{ptics2014url}.
The~PTI corpus consist of approximately 12k user utterances with a length varying between 0.4 s and 18 s with median around 3 s.
The~data were divided into training, development, and test data where the~corresponding data sizes were 9496, 1188, 1188 respectively.
The~data were divided into training and test data, consisting of 10999 and 1223 utterances respectively.
For evaluation, a~domain specific class-based language model with a vocabulary size of approximately 52k  and 559k n-grams was estimated from the~training data.
Named entities e.g., cities or bus stops, in class-based language model are expanded before building a~decoding graph.
The~perplexity of the~resulting language model evaluated on the~development data is about 48.
% Any tunable parameters such were set on the~development data and the~reported results were obtained using the~test set.

Since the~PTI acoustic data amounts to less then 5 hours, the~acoustic training data was extended by additional 15 hours of telephone out-of-domain data from VYSTADIAL 2013 - Czech corpus \cite{korvas_2014}.
The~acoustic models were obtained by BMMI discriminative training with LDA and MLLT feature transformations.
The~scripts used to train the~acoustic models are publicly available in ASDF \cite{asdf2014url} as well as in Kaldi \cite{kaldi2014url} and a detailed description of the~training procedure is given in Korvas et al. \cite{korvas_2014}.




\section{\term{PyOnlineLatgenRecogniser} evaluation in Alex}
\label{sec:eval}
The evaluation uses the Alex's \term{ASRInterface} and the decoding graph built from \ac{LM} and \ac{AM} described in Subsection~\ref{sec:ptilm}.
We focus on evaluating the~speed of the \term{OnlineLatgenRecogniser} and its relationship with the accuracy of the~decoder, namely:
\begin{itemize}
    \item Real Time Factor (RTF) of decoding -- the ratio of the~recognition time to the~duration of the~audio input,
    \item Latency -- the~delay between utterance end and the~availability of the~recognition results,
    \item Word Error Rate (WER).
\end{itemize}

% DONE \todo{only non speed parameter LMW}
Accuracy and speed of the~\term{OnlineLatgenRecogniser} are controlled by the \term{max-active-states},   \term{beam}, and \term{lattice-beam} parameters \cite{povey2011kaldi}.
\term{Max-active-states} limits the~maximum number of active tokens during decoding.
\term{Beam} is used during graph search to prune ASR hypotheses at the~state level.
\term{Lattice-beam} is used when producing word level lattices after the~decoding is finished.
It is crucial to tune these parameters optimally to obtain good results.

In general, one aims for a setting giving RTF smaller than 1.0.
However, in practice, it is useful if the~RTF is even smaller because other processes running on the machine can influence the~amount of available computational resources.
Therefore, we target the~RTF of 0.6 in our setup.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{beam_vs_rtfwer.pdf.ps}
    \includegraphics[scale=0.5]{latbeam_vs_latwer.pdf.ps}
    \caption{The~upper graph (a) shows that WER decreases with increasing \term{beam} and the~average RTF linearly grows with the~beam.
    Setting the~maximum number of active states to 2000 stops the~growth of the~95th RTF percentile at 0.6, indicating that even in the~worst case, we can guarantee an~RTF around 0.6.
    The~lower graph (b) shows how latency grows in response to increasing \term{lattice-beam}.}
    \label{fig:wer} 
    \end{center}
\end{figure*}

We used grid search on the~test set to identify optimal parameters.
Figure~\ref{fig:wer} (a) shows the~impact of the~\term{beam} on the~WER and RTF measures.
In this case, we set \term{max-active-states} to 2000 in order to limit the~worst case RTF to 0.6.
Observing Figure~\ref{fig:wer} (a), we set \term{beam} to 13 as this setting balances the~WER, 95th RTF percentile, and the~average RTF.
% Therefore, for Figure~\ref{fig:wer} (b) as this setting
Figure~\ref{fig:wer} (b) shows the~impact of the~\term{lattice-beam} on WER and latency when \term{beam} is fixed to 13.
We set \term{lattice-beam} to 5 based on Figure~\ref{fig:wer} (b) to obtain the~95th latency percentile of 200 ms, which is considered natural in a~dialogue \cite{skantze2009incremental}.
\term{Lattice-beam} does not affect WER, but larger \term{lattice-beam} improves the~oracle WER of generated lattices~\cite{povey2012generating}.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{frtf_vs_prc.pdf.ps}
    \includegraphics[scale=0.5]{lat_vs_prc.pdf.ps}
    \caption{The~percentile graphs show RTF and Latency scores for test data for \term{max-active-sates}=2000, \term{beam}=13, \term{lattice-beam}=5.
Note that 95 \% of utterances were decoded with the~latency lower that 200ms.}
    \label{fig:prc}
    \end{center}
\end{figure*}

% Figure~\ref{fig:wer}
% The~\term{beam} and \term{lattice-beam} parameters significantly effect accuracy and speed of decoding.
% First, we set \term{max-active-states} to 2000 to limit worst case RTF to 0.6. 
% Second, we tune \term{beam} 

% We fixed other parameters or used the~default values since other parameters have just technical character or does not influence the~speed and quality of decoding.\footnote{The~only exception is language model weight which affects WER, we set it up for our LM to 15.}
% The~only exception is the~language model weight (LMW) parameter, which does not effect the~speed of the~decoding but only the~output probabilities determined by mixing AM and LM models.
% As illustrated in Figure~\ref{fig:wer}, further increasing the~\term{beam} away from Alex \term{beam} configuration would significantly slow down decoding but reduce a~little WER.
% Similarly, the~Alex \term{lattice-beam} setup balances between larger \term{lattice-beam} and quickly increasing latency.

Figure~\ref{fig:prc} shows the~percentile graph of the~RTF and latency measures over the~test set.
For example, the~95th percentile is the~value of a~measure such that 95\% of the~data has the~measure below that value.
One can see from Figure~\ref{fig:prc} that 95\% of test utterances is decoded with RTF under 0.6 and latency under 200 ms.
The~extreme values are typically caused by decoding long noisy utterances where uncertainty in decoding slows down the~recogniser.
Using this setting, the~\term{OnlineLatgenRecogniser} decodes the~test utterances with a WER of about 21\%.

% In ADSF a~Google ASR service had been used, because we did not have acoustic training data for AM training.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[scale=0.5]{lat_cloud_kaldi.pdf.ps}
    \caption{Shorter latency of custom on-line decoder (OnlineLatgenRecogniser) over batch decoding with cloud service (Google ASR service).}
    \label{fig:wer} 
    \end{center}
\end{figure*}

In addition, we have also experimented with Google ASR service on the~test data from the~PTI domain.
The~Google ASR service decodes 95\% of test utterances with latency under 1900 ms and WER is about 48\%.
The~high latency is presumably caused by the~batch processing of audio data and network latency, and the~high WER is likely caused by a mismatch between Google's acoustic and language models and the~test data.

\subsection*{Results}
\label{sec:results}
Based on evaluation we selected setup\footnote{Setup: \term{beam} 12, \term{lattice-beam} 5, \term{max-active-states} 2000.} for ASR component in Alex Dialogue System Framework with  WER under 22 \%, latency less tha 200 ms and RTF under 0.6.
To conclude, the~\term{OnlineLatgenRecogniser} performs significantly better than our previous implementation of queries to cloud Google ASR service which we were using after OpenJulius decoder.

