% !TEX root = main.tex
\chapter{Acoustic model training}
\label{cha:train}

This chapter describes the~acoustic model training.

Having a good acoustic model is the first necessary step for~a~quality real-time decoder.
In this chapter, we focus on how to train good acoustic model.

At first the~Section~\ref{sec:methods} introduces the used methods. 
The chapter continues by~presenting results in~Section~\ref{sec:exps}. 
Later, in~Section~\ref{sec:compare} we compare results obtained by Kaldi toolkit with results
trained with \ac{HTK} toolkit by Matěj Korvas\cite{lrec2014}.
The details about training scripts and its file system organization
is described in Appendix~\ref{cha:setup}.  

The training methods described further in~Section~\ref{sec:methods}
has to be trained in specific order,
because some of the more complicated models are initiated
with values of the simpler models.
The Figure~\ref{fig:am-deps} list all acoustic models trained in our recipe.
It for example show that the basic triphone \ac{AM} is initialized and retrained based
on simple monophone model.

\begin{figure}[!htp]
    \begin{center}
    \input{images/am-deps}
    \caption{Training partial order among \ac{AM} in our training recipe}
    \label{fig:am-deps} 
    \end{center}
\small{\begin{tabular}{lll}
\hline
Training method name & Script shortcut \\
\hline
Monophone & mono \\
Triphone  & tri1 \\
delta+delta-delta & tri2a  \\
\acs{LDA}+\acs{MLLT} & tri2b  \\
\acs{LDA}+\acs{MLLT}+\acs{MMI} & tri2b-mmi \\
\acs{LDA}+\acs{MLLT}+\acs{MMI}(boost) & tri2b-mmi-b0.05 \\
\acs{MPE} & tri2b-mpe \\
\end{tabular}}
\end{figure}

\section{Training methods and feature transformation} 
\label{sec:methods}
We use two types of \ac{HMM} \ac{AM} training in Vystadial recipe. 
The \acl{ML} training and discriminative training.
The discriminative training is applied on top of \ac{ML} training represented by Viterbi training.

The Viterbi training, described in~Subsection~\ref{sub:trainhmm}, approximates \ac{MLE} in setting parameters to \ac{HMM}s in \ac{AM}.
The \ac{MLE} is general attitude to setting statistical model parameters.
The maximum likelihood estimation search for best parameters $\theta^*$ 
in order to maximize the likelihood function $f$ for \ac{IID} training data illustrated in Equation~\ref{eq:maxlikelihood}.
For \ac{IID} training data holds Equation~\ref{eq:lmjoint} describing data joint probability. 
\begin{equation}\label{eq:lmjoint}
    f(x_1, x_2, x_3, \ldots, x_n | \theta) = f(x_1 | \theta) * f(x_2 | \theta) * \ldots * f(x_n | \theta)
\end{equation}
The likelihood function can be derived from Equation~\ref{eq:lmjoint} assuming training data fixed
and parameter $\theta$ free as described in Equation~\ref{eq:likelihood}.
\begin{equation}\label{eq:likelihood}
    \mathcal{L}(\theta\,|\,x_1,\ldots,x_n) = \sum_{i=1}^n f(x_i|\theta)
\end{equation}
\begin{equation}\label{eq:maxlikelihood}
    \theta^* = argmax_{\theta} \mathcal{L}(\theta\,|\,x_1,\ldots,x_n)
\end{equation}

On the other hand the discriminative training does not optimize the likelihood function.
It uses the generative \ac{AM} and posterior probability to optimize other function and change \ac{AM} parameters.
In our training recipe we use two discriminative functions.
\begin{itemize}
    \item \acl{bMMI}\cite{povey2008boosted}
    \item \acl{MPE}\cite{povey2003mmi}
\end{itemize}

The discriminative models usually yield better results than generative models, if enough data is available. 

Kaldi toolkit has support for number of feature transformations, which are applied on top
of speech parametrization (See Subsection~\ref{sub:param}).
Kaldi ransformations



As you can see in~Table~\ref{tab:disc_train}, we combine various discriminative training methods
and acoustic feature transformations in order to obtain better results. 
Let us briefly introduce improvements and alternatives to maximum likelihood training methods,
which are used in Vystadial script.

\begin{itemize}
    \item \todo{Nepatri mezi discriminativni metody - oddelit} 
        One of the historically first signal processing method which captures the dynamics of a speech
        are so called $delta$ and $delta-delta$ coefficients. The~$delta$ coefficients represent the first 
        derivative of static features and $delta-delta$ represents the second derivatives of static acoustic features.
        On Figure~\ref{fig:delta} there is typical feature extraction procedure using \ac{MFCC}\cite{ye2004speech}.
    \item \ac{LDA} is used as linear classifier for acoustic features. \todo{citace}
        It transforms the acoustic features to another feature space with reduced dimension.
    \item \ac{MLLT}\cite{psutka2007benefit} clusters and transforms full covariance matrices for Gaussian distributions used in \ac{HMM}s. Using the full covariance matrices instead of the diagonal one improves the \ac{WER} and the sharing of the covariance matrices makes the technique reasonable efficient.
    \item \ac{MMI} is different criterium to maximum likelihood. The key idea is that maximum likelihood is optimal only under rarely met conditions. \ac{MMI} optimize maximum mutual information objective function\cite{chow1990maximum}.
    \item \ac{UBM}\cite{povey2010subspace} training is the base system for \acl{SAT}. 
        By changing small number of parameters in vector of short length (50) \ac{UBM} controls all the mean and 
        weight parameters of the speech-state-specific mixture model. In addition, it introduce 
        a~"speaker vector," which can be adapted by discriminative \ac{SAT} training.
    \item \acl{SAT}\cite{povey2011kaldi} uses \ac{fMLLR} technique. Note, that in the~experiments we have 
        the~training data without speaker labels, so we suppose that each utterance is spoken by different user.
        In fact, it makes the \ac{SAT} methods hopeless in improving the \ac{WER}, 
        but we include the results for completeness and future comparison.
    \item \todo{delete} \ac{MPE}\cite{povey2007evaluation} is an objective function for discriminative models in \ac{ASR}. In \ac{MPE} experiment the Kaldi toolkit uses modified \ac{MMI} and extended Baum-Welsch algorithm together with the \ac{MPE} objective function to train \ac{HMM} parameters.
    \item \cite{povey2005fmpe} \ac{fMPE} uses \ac{MPE} objective functions to the acoustic features. \todo{check and delete}
\end{itemize}
\begin{table}[!htp]\label{tab:disc_train}
\small{\begin{tabular}{lll}
\hline
Training method name & Script shortcut & Note\\
\hline
Monophone & mono & Base for all other systems.\\
Triphone  & tri1 &  Alignments from mono. Base for others.\\
delta+delta-delta & tri2a & Alignments from tri1 \\
\ac{LDA}+\ac{MLLT} & tri2b &  Base for other experiments \\
\ac{LDA}+\ac{MLLT}+\ac{MMI} & tri2b-mmi & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{MMI}(boost) & tri2b-mmi-b0.05  & Alignments from tri2b \\
\ac{MPE} & tri2b-mpe & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{SAT} & tri3b & Alignments from tri2b. Base for others. \\
\ac{LDA}+\ac{MLLT}+\ac{SAT}+\ac{MMI} & tri3b-mmi  & Alignments from tri3b \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-b & Alignments from \ac{UBM}(based on tri3b) \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-c & tri3b-fmmi-b with changed parameters \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-d & Alignments from \ac{UBM} + \ac{MPE} on features.\\ 
\end{tabular}}
\caption{Training methods overview. All experiments train triphones except the~monophone experiment. Script shortcut is refers to names of the experiments in the "run.sh" script.}
\end{table}

% section training_method (end)

\section{Experimental design and results} 
\label{sec:exps}
\todo{Comment on 2 WER difference generative vs discriminative training}
\todo{Mention that HTK scripts are really good}
\todo{Show that we can get similar results with Kaldi as Matej did with HTK}
The experiments focus on comparing the quality of ASR hypothesis measured by~\ac{WER}.
The speed of the decoding is not important at this moment.  The {\it gmm-latgen-faster}\/ decoder is used for evaluation on testing data. 
\todo{Je to spatne - napsat presne-However, many of the methods from~Table~\ref{tab:disc_train}
require specific feature transformations, which also needs to applied before using the decoder on testing data.}

Note, that other Kaldi decoders may not support some of the trained models.
In general, if a method is slow for the {\it gmm-latgen-faster}\/ decoder, it is slow for other decoders too.
We measure the speed in terms of \ac{RTF}, because the {\it gmm-latgen-faster}\/ decoder is designed for~batch 
processing, so the measuring latency is not convenient.


\ml{datasets}
The experiments were performed on dataset called {\it Vystadial}\footnote{\todo{Add link to Vystadial dataset.}}.
However, in the~experiments we use the pretrained \ac{LM}.
The pretrained \acl{LM} was trained by our colleagues in Cambridge for the~same domain.
The pretrained \ac{LM} was also used in the~\ac{HTK} experiments and we use the~model in experiments 
for comparing comparing Kaldi and \ac{HTK} results.\footnote{The Vystadial recipe can train simple \ac{LM} from the audio transcriptions}.
The pretrained \ac{LM} contains $2802$ unigrams, $109315$ bigrams and
$118217$ trigrams. 


\begin{table}[!htp]\label{tab:exp_best}
\begin{tabular}{lrr}
\hline
\parbox[t]{6.0cm}{\ac{LM} and dictionary / Experiment} & Generated \acs{LM}  & Classic \ac{LM} \\
\hline
\ac{HTK} like parameters    &  $tri3b$-(19.59,17) & $tri2b$-(18.39,16) \\
Best with OOV               & todo & todo \\
Best without OOV            & todo & todo \\
\hline
\end{tabular}
\caption{Kaldi experiments, best Maximum Likelihood method}
\end{table}

\todo{Delete the table? The summary of best results is described~Table~\ref{tab:exp_best}}

In~the detail tables 
The tables~\ref{tab:htk_like},~\ref{tab:htk_like_gen_lm},~\ref{tab:best_oov} 
and~\ref{tab:best_nooov}\footnote{All the tables are in~Appendix~\ref{cha:results}.} presents complete overview of the results. 
Note, that the decoding on test data was performed with different \ac{LM} weights for each method and experiment. 
The we tried \ac{LM} weights $9, 10, 11, ...$ to $20$. 
The best \ac{LM} weight is always the second number in bracket.
\todo{na jakych datech jsi tu  nejlepsi wahu urcil? na test nebo devel. Pokud na test. Pak to musis nejak omluvit protoze to neni idealni
Muzes rict ze to delas aby jsi eliminoval vliv vahy LM na vysledku  a vzhledem k tomu ze netestujem kvalitu LM tak to nevadi jak ze tunis na test datech.}

Similarly to \ac{WER}, we also measured \ac{SER}. It is computed as ratio of sentences at least with one incorrectly decoded word dived by the total number of sentences. Note that \ac{SER} largely depends on length of sentences. 

\begin{table}[!htp]\label{tab:htk_like}\centering\begin{tabular}{l|rrr}
exp             & RT coef       & WER         & SER        \\ 
\hline
mono            & 0.766032      & (41.68, 12) & (67.79, 18)\\ 
tri1            & 0.6366977     & (21.97, 14) & (50.32, 20)\\ 
tri2a           & 0.4987811     & (22.34, 18) & (49.94, 18)\\ 
tri2b           & 0.4434147     & (18.39, 16) & (44.5, 19) \\ 
tri2b-mmi       & 0.50897815    & (14.25, 13) & (39.07, 13)\\ 
tri2b-mmi-b0.05 & 0.49218765    & (14.08, 14) & (38.29, 15)\\ 
tri2b-mpe       & 0.43555075    & (15.62, 16) & (41.01, 15)\\ 
tri3b           & 0.3002624     & (19.15, 17) & (47.22, 18)\\ 
tri3b-mmi       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
tri3b-fmmi-b    & 2.46556       & (14.25, 12) & (38.55, 15)\\ 
tri3b-fmmi-c    & 2.4882825     & (14.2, 14)  & (38.03, 14)\\ 
tri3b-fmmi-d    & 1.84923083333 & (14.47, 14) & (38.42, 14)
\end{tabular}
\caption{Experiment with pretrained LM and dictionary "\ac{HTK}-like".}
\end{table}  

% section introduction_to_training_acoustic_models (end)

\section[Kaldi and \acs{HTK} comparison]{Kaldi and previous \ac{HTK} results comparison} 
\label{sec:compare}

Let us remind, that we use OpenJulius decoder together with HTK models decoder in the~Alex dialog system.
The \ac{HTK} scripts were written by Filip Jurčíček and Matěj Korvas. No discriminative training was performed,
only experiments with mixing with different \acl{LM} settings were performed. 
The best obtained results are listed in~Table~\ref{tab:res}.

\begin{table}[!htp]\label{tab:htk_res}\centering\begin{tabular}{l|rr}
Experiment      & \ac{WER} & \ac{SER} \\
\hline
    Zerograms      & 50.18  & 94.34  \\
    Bigrams        & 33.67  & 68.55  \\
    Trigrams       & 20.31  & 45.10  \\
\end{tabular}
\caption{Best \ac{HTK} results}
\end{table}  

\todo{Tady jende o reportovani vysledky kete jsme dostali z HTK. 
Ale o ukazani ze pro stejnem nastaveni maji jak KALDI a HTK podobne vysledku.
Ze to je ocekavane, protoze pouzite metody jsou stejne/podobne. 
Ze to slouzi k overeni ze nove KALDI modely jsou dobre!
Idealne tady bude jedna tabulka KALDI vs. HTK}

We tried to simulate the~\ac{HTK} settings in the~Kaldi experiment~\ref{tab:htk_like}.
We choose for all experiments the~parameters according the~\ac{HTK} scripts.
The most important parameters are the~maximum Gausians number and the~number of \acl{PDF}.
\todo{pdf numbers and max gausians why 19200? and how did I compute it?} 

In the experiment~\ref{tab:htk_like} we used the available options for reproducing the~\ac{HTK} like generation
of \ac{MFCC} features.

The most comparable settings for \ac{HTK} and Kaldi are the $trigram$ \ac{HTK} results
and the $tri2a$ model in "htk-like" experiment~\ref{tab:htk_like}. 
The~\ac{WER} for this settings can be found in Table~\ref{tab:compare}.
Note we also added a discriminative model from the same "\ac{HTK}-like" experiment.
It is both more accurate and faster than current Kaldi set up.

\begin{table}[!htp]\label{tab:compare}\centering\begin{tabular}{l|rrr}
    Experiment   & \ac{RTF} & \ac{WER} & \ac{SER} \\
\hline
\hline
\ac{HTK} best, Trigrams   & Unknown  & 20.31  & 45.10  \\
Kaldi, "htk like"-$tri2a$, Table~\ref{tab:htk_like} - tri2a & 0.4987811 & 22.34 & 49.94\\
\hline
\ac{HTK}-$tri3b-mmi$       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
\end{tabular} \caption{Result comparison between Kaldi and \ac{HTK}} \end{table}  

To conclude, we find out that Kaldi recognition toolkit is capable of training acoustic models with comparable quality 
like \ac{HTK} toolkit using maximum likelihood training. In addition, Kaldi has rich set of tools for discriminative training, 
which outperforms the maximum likelihood methods.
In Chapter~\ref{cha:decoder} we will describe Kaldi decoders, which are convenient for real-time usage, 
and which also does not loose the ability to produce quality ASR hypothesis.

\todo{Discuze proc se nevenujeme LM}
We do not focus on the language modeling since \ac{LM} is usually domain specific.
We want to use the~Alex dialog system and also the Kaldi recognizer in multiple domains with minimum
setup changes. 
\todo{move to better place: We focus on AM, there two ways a) use very general LM b) train LM from transcriptions}
In future, we would like to use very general \ac{LM} 
\todo{Tohle by mohlo byt vypichnuto nekde jinde, a lepe. Delame jenom AM, LM nas nezajima, ale abychom mohli testovat nejaky potrebujeme. Jsou dva zpusoby. Primo z trenovacich dat, nebo nejaky obcnejsi.}
% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)







% chapter model_training_modelsmodels (end)
