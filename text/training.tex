% !TEX root = main.tex
\chapter{Acoustic model training}
\label{cha:train}

This chapter present acoustic new Kaldi modeling recipe for new Czech and English data.
The recipe scripts were developed as part of this thesis and are licensed under the Apache 2.0 license and publicly available in Kaldi repository\footnote{http://sourceforge.net/p/kaldi/code/HEAD/tree/sandbox/oplatek2/egs/vystadial/}.
The \ac{AM} trained using this scripts can be used for both batch training with common Kaldi decoders or with the new \term{OnlineLatgenRecognizer}, which performs on-line decoding and is described in Chapter~\ref{cha:decoder}.

The first Section~\ref{sec:methods} introduces data used. 
The chapter continues by~presenting the \acp{AM} training in~Section~\ref{sec:exps}. 
Later, in~Section~\ref{sec:compare} we evaluate results of trained \acp{AM} and compare them to generative \acp{AM} trained state of art \ac{HTK} scripts.
Note, that details about launching the scripts and file system organisation can be found in Appendix~\ref{cha:train_scripts}.  

\section{Vystadial acoustic data}
\label{sec:vystadial_acoustic_data}

The data were collected in Vystadial project\footnote{http://ufal.mff.cuni.cz/grants/vystadial} and are released under the Creative Commons Share-alike (CC-BY-SA~3.0). 
The Czech\footnote{Czech data: \url{http://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-4670-6}}and English\footnote{English data: \url{http://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-4671-4}} are available online in Lindat repository\footnote{{http://lindat.mff.cuni.cz/repository/}}.
Note that the previous version of this recipe is published with the data in Lindat repository and described in~work~\cite{korvas_2014}.

The English acoustic data are recorded phone calls among humans and statistical dialogue systems,
which was designed to provide the user with information on a suitable dining venue in the town.
Most of the data was spoken in American English.
The typical sentences recorded from users were queries for the dialogue system e.g.,
\begin{verbatim}
I NEED A CHINESE TAKE AWAY RESTAURANT IN THE CHEAP PRICE RANGE
I'M LOOKING FOR AN INTERNATIONAL RESTAURANT
I NEED TO FIND A PUB IT SHOULD ALLOW CHILDREN AND HAVE A TELEVISION
\end{verbatim}.

The Czech recordings were collected in three ways\cite{korvas_2014}:
\begin{enumerate}
    \item using a free Call Friend phone service (\term{FRIEND})
    \item using the Repeat After Me speech data collecting process,
    \item from telephone interactions with the Alex \ac{SDS} in public transport domain.
\end{enumerate}

In \term{FRIEND} service native Czech speakers were invited to make free calls.
In Repeat After Me process volunteers called a number where they were asked to repeat 
sentences synthesized by a \ac{TTS}.

Note that the user language differ significantly in dialogues with Alex and the other two settings.
% Remember we focus on training \acp{AM} for \ac{SDS} Alex and its public transport domain.
The \acp{AM} for Czech are trained on acoustic data from the three very different domains, because there is available only 2 hours of in-domain data for public transport domain, which we target with the \ac{SDS} Alex.
The summary of audio sizes in training, development and test set are presented in Table~\ref{tab:audio}.
Both Czech and English orthographic speech transcriptions were transcribed by humans.

\begin{table}[hbp]
    \centering
    \begin{tabular}{lrrr}
        \hline
            dataset & audio & \# sents & \# words \\
        \hline
            English & & & \\
                train & 41:30 & 47,463 & 178,110 \\
                dev & 01:45 & 2,000 & 7,376 \\
                test & 01:46 & 2,000 & 7,772 \\
        \hline
            Czech & & & \\
                train & 15:25 & 22,567 & 126,333 \\
                dev & 01:23 & 2,000 & 11,478 \\
                test & 01:22 & 2,000 & 11,204 \\
        \hline
		\end{tabular}
    \caption{Size of the data: length of the audio (hours:minutes), number of sentences
        (which is the same as the number of recordings), number of words in the 
    transcriptions.\cite{korvas_2014}}
    \label{tab:audio}
\end{table}


The evaluation in~Section~\ref{sec:result} is perform  on test set combined from all three domains.
The test set is mixed according the proportion of the domains in training and development set.
% On contrary, in evaluation of \ac{ASR} in Alex described in~Chapter~\ref{cha:integration}
% we use only transcribed utterances from Alex recordings, 
% so we can measure accuracy of the speech recognition task in~the dialog system.

Consider differences in Czech language between sentences from dialogues with Alex 
in first example and the sentences from the other two domains.
\begin{verbatim}
A DALŠÍ
_NOISE_
JO DĚKUJU MOC TO JSEM CHTĚL VĚDĚT
ZE ZASTÁVKY DEJVICKÁ
\end{verbatim}

\begin{verbatim}
PRYČ S TYRANY A ZRÁDCI VŠEMI
UTRHNE SI KVĚT Z KYTICE A ODCHÁZÍ
DYŤ TO JE HROŠÍ NEŽ ZVÍŘE
O LIBERALIZMU TEHDY NEBYLO ŘEČI
CO BY TAM S TEBOU DĚLALI
\end{verbatim}

Notice that the sentences in Alex public transport domain are shorter
and contain noises. The speech is spontaneous and proper names are frequently used.



\section{Training recipe \acp{AM}}
\label{sec:am_rec}

The acoustic modelling techniques focus on modelling the speech to word mapping, so the test utterances are decoded with the least error possible. 
For correctness the testing uses previously unseen utterances in training or development set, so the real conditions are well simulated.
The Figure~\ref{fig:am-deps} list all acoustic models trained in our recipe.
The advanced \ac{AM} is always initiated by audio alignments (respectively acoustic features alignments) obtained using simpler \ac{AM}.

We train a mono-phone model from flat start using the MFCCs, $\Delta$ and $\Delta \Delta$ features.
We force-align the feature vectors to HMM states for phones in the corresponding transcriptions.
We retrain the triphone \ac{AM} (\term{tri1a}).
One branch of experiment finishes by training \ac{MFCC} $\Delta + \Delta\Delta$ triphone \ac{AM} force-aligned using \term{tri1a} \ac{AM}.

On contrary, the second branch instead of $\Delta + \Delta\Delta$ transformation uses \ac{LDA}+\ac{MLLT} to train \term{tri2b} \ac{AM}.
Using the \ac{AM} with \ac{LDA}+\ac{MLLT} transformations three \acp{AM} are discriminatively trained using following objective functions:
\begin{enumerate}
    \item \acl{MMI}\cite{chow1990maximum}\footnote{Note the \ac{MMI} function is implemented as \acs{bMMI} with boosted parameter set to 0.}. The model is train in four loops.
    \item \acl{bMMI}\cite{povey2008boosted}. The model is train in four loops with parameter 0.05.
    \item \acl{MPE}\cite{povey2003mmi}. The model is retrained in four loops.
\end{enumerate}

\begin{figure}[!htp]
    \begin{center}
    \input{images/am-deps}
    \small{\begin{tabular}{lll}
    \hline
    Training method name & Script shortcut \\
    \hline
    Monophone & mono \\
    Triphone  & tri1 \\
    $\Delta + \Delta\Delta$ & tri2a  \\
    \acs{LDA}+\acs{MLLT} & tri2b  \\
    \acs{LDA}+\acs{MLLT}+\acs{MMI} & tri2b\_mmi \\
    \acs{LDA}+\acs{MLLT}+\acs{bMMI} & tri2b\_bmmi \\
    \acs{MPE} & tri2b\_mpe \\
    \hline
    \end{tabular}}
    \end{center}
    \caption{Training partial order among \ac{AM} in our training recipe}
    \label{fig:am-deps} 
\end{figure}

The acoustic models \term{mono}, \term{tri1}, \term{tri2a}
and \term{tri2b} are trained generatively.
The discriminative models yield better results than generative models, if enough data is available. 
Note that the discriminative may over-fit to train data, so models from second or third retraining loop may yield better results. 

The training scripts for Czech as well English data differs only data splitting and using different phonetic dictionary, the training itself remains exactly the same.

\todo{Described LM build from training data}
The pretrained \ac{LM} contains $2802$ unigrams, $109315$ bigrams and
$118217$ trigrams. 

\subsubsection*{\acl{LDA} and \ac{MLLT} feature transformation}
The $\Delta + \Delta\Delta$ was introduced Section~\label{sub:param} and triples the number of features by by computing also first and second derivatives from \ac{MFCC} coefficients.
The \ac{LDA}+\ac{MLLT} is an alternative setup.
\ac{LDA} is used as linear classifier for acoustic features.
Based on vectors of length 13 representing static \ac{MFCC} 
spliced together in for each context window the \ac{LDA} tries to 
find the best dynamic information, replacing the $\Delta+\Delta\Delta$ coefficients.

The combination of \ac{LDA} and \ac{MLLT} reduce the feature dimension and
rotates the feature space so the classes after \ac{LDA} has more diagonal covariance in two steps.\cite{gopinath1998maximum}
The \ac{HLDA} performs dimension reduction and space transformation in one step.\cite{gales1999semi}
The combination \ac{HLDA} and \ac{MLLT} perform very similar feature transformation
and obtain similar improvements over $\Delta+\Delta\Delta$ transformation 
as \ac{HLDA}\cite{gales1999semi}\cite{gopinath1998maximum}.
We choose to use \ac{LDA} and \ac{MLLT}, because Kaldi toolkit offers scripts 
for this combination and we gain substantial improvement over $\Delta+\Delta\Delta$ transformation.
See Table~\ref{tab:TODO_results}

In our experiments we use the context window with the default length of 9 frames, 
four frames from left context and four frames from right context.
The \ac{LDA} transforms uses and $\Delta$ and $\Delta\Delta$ dynamic features as prior.


% DONE \todo{described setup, alignments, dependencies (again)}



% \subsection{Experimental setup} 
% \label{sec:exps}
% In this section we describe the organization of experiments,
% although we skip the implementation detail, which can be found in~Appendix~\ref{cha:setup}.
% The~Appendix~\ref{cha:setup} contains tutorial how to run the scripts, on the other hand
% this section briefly discuss the data and setup used in the scripts. 

% For training acoustic models we used Czech and English acoustic data
% collected under Vystadial project which are publicly available.\cite{korvas_2014}



% Note, that other Kaldi decoders may not support some of the trained models.
% In general, if a method is slow for the \term{gmm-latgen-faster} decoder, it is slow for other decoders too.
% We measure the speed in terms of \ac{RTF}, because the \term{gmm-latgen-faster} decoder is designed for~batch 
% processing, so the measuring latency is not convenient.


\section{Evaluation}
\label{sec:am_eval}

The experiments focus on comparing the quality of ASR hypothesis measured by~\ac{WER}.
The speed of the decoding is not important at this moment.  
The \term{gmm-latgen-faster} decoder is used for evaluation on testing data.
We are able to exactly reproduce the results of \term{gmm-latgen-faster} decoder with our \term{OnlineLatgenRecogniser}.
The \term{gmm-latgen-faster} was used for evaluation in the scripts, so the Kaldi users do not have to install our extension.
The \term{gmm-latgen-faster} decoder generates a state level lattice for each utterance.
The one best hypothesis is extracted from the decoded lattice and evaluated by \ac{WER} and \ac{SER} metrics agains against the reference transcription.
The \term{gmm-latgen-faster} decoder is used with all \acp{AM} trained and wth the training bigram \ac{LM}.
The \ac{AM}, \ac{LM} phonetic dictionary and few helper files are used to built \term{HCLG} decoding graph.
The parameters are set to default the exceptions are \term{beam=12.0, lattice-beam=6.0, max-active-states=14000}. See Section~\ref{sec:real-setup} to find out what the parameters mean.


The results were compared against results obtained well-written \ac{HTK} scripts by Keith Vertanen and improved by Matěj Korvas \cite{korvas_2014}.

\todo{Note, that the decoding on test data was performed with different \ac{LM} weights for each method and experiment. 
The we tried \ac{LM} weights $9, 10, 11, ...$ to $20$. 
The best \ac{LM} weight is always the second number in bracket.
}

\todo{na jakych datech jsi tu  nejlepsi wahu urcil? na test nebo devel. Pokud na test. Pak to musis nejak omluvit protoze to neni idealni
Muzes rict ze to delas aby jsi eliminoval vliv vahy LM na vysledku  a vzhledem k tomu ze netestujem kvalitu LM tak to nevadi jak ze tunis na test datech.}

Similarly to \ac{WER}, we also measured \ac{SER}. It is computed as ratio of sentences at least with one incorrectly decoded word dived by the total number of sentences. Note that \ac{SER} largely depends on length of sentences. 
\todo{best models, why best}

\subsection[Kaldi and \acs{HTK} comparison]{Kaldi and previous \ac{HTK} results comparison} 
\label{sec:compare}
\todo{Mention that HTK scripts are really good}

\todo{Show that we can get similar results with Kaldi as Matej did with HTK}

\todo{Run experiments with every 50 30 20 10 5 1 shows tri2a, tri2b, tri2b-bmmi, trib2-mpe, graph and table only full data (every 1)}

Let us remind, that we use OpenJulius decoder together with HTK models decoder in the~Alex dialog system.
The \ac{HTK} scripts were written by Filip Jurčíček and Matěj Korvas. No discriminative training was performed,
only experiments with mixing with different \acl{LM} settings were performed. 
The best obtained results are listed in~Table~\ref{tab:res}.

\todo{Add HTK results from paper Korvas}
\begin{table}[!htp]\label{tab:htk_res}\centering\begin{tabular}{l|rr}
Experiment      & \ac{WER} & \ac{SER} \\
\hline
    Zerograms      & 50.18  & 94.34  \\
    Bigrams        & 33.67  & 68.55  \\
    Trigrams       & 20.31  & 45.10  \\
\end{tabular}
\caption{Best \ac{HTK} results}
\end{table}  

\todo{Tady jende o reportovani vysledky kete jsme dostali z HTK. 
Ale o ukazani ze pro stejnem nastaveni maji jak KALDI a HTK podobne vysledku.
Ze to je ocekavane, protoze pouzite metody jsou stejne/podobne. 
Ze to slouzi k overeni ze nove KALDI modely jsou dobre!
Idealne tady bude jedna tabulka KALDI vs. HTK}

We tried to simulate the~\ac{HTK} settings in the~Kaldi experiment~\ref{tab:htk_like}.
We choose for all experiments the~parameters according the~\ac{HTK} scripts.
The most important parameters are the~maximum Gausians number and the~number of \acl{PDF}.
\todo{pdf numbers and max gausians why 19200? and how did I compute it?} 

In the experiment~\ref{tab:htk_like} we used the available options for reproducing the~\ac{HTK} like generation
of \ac{MFCC} features.

The most comparable settings for \ac{HTK} and Kaldi are the $trigram$ \ac{HTK} results
and the $tri2a$ model in "htk-like" experiment~\ref{tab:htk_like}. 
The~\ac{WER} for this settings can be found in Table~\ref{tab:compare}.
Note we also added a discriminative model from the same "\ac{HTK}-like" experiment.
It is both more accurate and faster than current Kaldi set up.

\begin{table}[!htp]\label{tab:compare}\centering\begin{tabular}{l|rrr}
    Experiment   & \ac{RTF} & \ac{WER} & \ac{SER} \\
\hline
\hline
\ac{HTK} best, Trigrams   & Unknown  & 20.31  & 45.10  \\
Kaldi, "htk like"-$tri2a$, Table~\ref{tab:htk_like} - tri2a & 0.4987811 & 22.34 & 49.94\\
\hline
\ac{HTK}-$tri3b-mmi$       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
\end{tabular} \caption{Result comparison between Kaldi and \ac{HTK}} \end{table}  

To conclude, we find out that Kaldi recognition toolkit is capable of training acoustic models with comparable quality 
like \ac{HTK} toolkit using maximum likelihood training. In addition, Kaldi has rich set of tools for discriminative training, 
which outperforms the maximum likelihood methods.
In Chapter~\ref{cha:decoder} we will describe Kaldi decoders, which are convenient for real-time usage, 
and which also does not loose the ability to produce quality ASR hypothesis.

\todo{Discuze proc se nevenujeme LM}
We do not focus on the language modeling since \ac{LM} is usually domain specific.
We want to use the~Alex dialog system and also the Kaldi recognizer in multiple domains with minimum
setup changes. 
\todo{move to better place: We focus on AM, there two ways a) use very general LM b) train LM from transcriptions}
In future, we would like to use very general \ac{LM} 
\todo{Tohle by mohlo byt vypichnuto nekde jinde, a lepe. Delame jenom AM, LM nas nezajima, ale abychom mohli testovat nejaky potrebujeme. Jsou dva zpusoby. Primo z trenovacich dat, nebo nejaky obcnejsi.}
% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)

The experiment using \ac{MFCC}, \ac{LDA} \& \ac{MLLT} and \ac{bMMI} discriminative training is
a state of the art set up for speaker independent speech recognition\cite{morbini2013asr}.
