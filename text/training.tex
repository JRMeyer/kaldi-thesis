% !TEX root = main.tex
\chapter{Acoustic model training}
\label{cha:training}

This chapter describes the~acoustic model training.

Having a good acoustic model is the first necessary step for~a~quality real-time decoder.
In this chapter, we focus on how to train good acoustic model.

At first the~Section~\ref{sec:setup} describes how the experiments are organized.  
The~Section~\ref{sec:methods} introduces the used methods. 
The chapter continues by~presenting results in~Section~\ref{sec:exps}. 
Later, in~Section~\ref{sec:compare} we compare results obtained by Kaldi toolkit with results
trained with \ac{HTK} toolkit by Matěj Korvas\cite{lrec2014}.

\section{Experiments organisation} 
\label{sec:setup}

\todo{Organizace experimentu neni az tak dulezita, dulezite jsou pouzite metody}
\todo{popsat presne jake LM jsem pouzil, jake vysledky budu reportovat}
\ml{Vystadial recipe}
\small{The Kaldi toolkit has codified form of training scripts with predefined file system structure. 
We created new recipe called {\it kaldi-Vystadial-recipe}\/ or just {\it Vystadial recipe}. 
In~the Vystadial recipe directory there is "s5" subdirectory where all the~recipe files are located. 

The Kaldi toolkit uses small utilities which can be combined together by Perl or Bash scripts.
The scripts used for training are described below. 
The structure of the Vystadial recipe folder can be seen in~Figure~\ref{fig:s5_dir}.
\begin{itemize}
    \item The~"run.sh" script launches the training and its evaluation.
    \item Scripts in "local" directory handle data preparation and other tasks specific for Vystadial recipe.
    \item Scripts in "utils" and "steps" are the common utilities for all recipes. 
    \item The "cmd.sh", "path.sh" and files in "conf" directory store settings.
    \item The "data" directory is generated by "run.sh" script and contains the transformed input data.
    \item The "exp" directory is generated by "run.sh" script and stores the trained models and evaluation results.
\end{itemize}

\tiny\begin{verbbox}
kaldi
|-- egs
|   |-- kaldi-Vystadial-recipe
|   |   |-- s5
|   |   |   |-- cmd.sh
|   |   |   |-- conf
|   |   |   |   |-- decode.config
|   |   |   |   |-- mfcc.conf
|   |   |   |   |-- train_conf.sh
|   |   |   |-- data
|   |   |   |   |-- test 
|   |   |   |   |-- train
|   |   |   |   |-- lang
|   |   |   |-- exp
|   |   |   |-- local
|   |   |   |   |-- backup.sh
|   |   |   |   |-- results.py
|   |   |   |   |-- save_check_conf.sh
|   |   |   |   |-- score.sh
|   |   |   |   |-- Vystadial_data_prep.sh
|   |   |   |   |-- Vystadial_format_data.sh
|   |   |   |   |-- Vystadial_prepare_dict.sh
|   |   |   |-- path.sh
|   |   |   |-- run.sh
|   |   |   |-- steps -> ../../wsj/s5/steps
|   |   |   |-- utils -> ../../wsj/s5/utils
\end{verbbox}
\normalsize

\begin{figure}[!htp]
\centering \theverbbox \caption{\small{Kaldi Vystadial recipe file system structure}}
\label{fig:s5_dir}
\end{figure}

Running one experiment means that several acoustic models are trained from the same data and with the same settings. 
Each of the acoustic models is trained by different training method and it is evaluated separately.
.\footnote{The list of methods we choose to evaluate is based on~Vassil Panayotov's Voxforge recipe}.

Each experiment in Vystadial recipe use the~supplied Vystadial dataset, which is split into training and testing data.
Each of the training method from~Table~\ref{tab:disc_train} is trained on data from "$data/train$" directory
and the method is evaluated on data from "$data/test$" directory.
% section the_experiments_setup (end)

\section{Training methods} 
\label{sec:methods}
There two types of acoustic model training used in Vystadial recipe. The \acl{ML} training and discriminative training.
The \ac{ML} training of generative models is described in~Section~\ref{sec:train_ml}.
The overview of methods for discriminative training used in Vystadial recipe for training Kaldi models
can be found below.

The discriminative models usually yield better results than generative models, if it can be applied. 
% The~discriminative model can describe only the~conditional probability $P(audio|ASR hypothesis)$,
% which represents the~acoustic models.
% Note, that the~generative model it is not capable of generating samples from join probability $P(audio, ASR hypothesis)$,
% which maybe necessary in other applications. 

\todo{Muzes nasat ze discriminativni trenovani se aplikuje po generativnim trenovani a nastavuje parametry generativniho modelu tak aby ASR lepe discriminovalo mezi clasifikovanymu tridamy. To se deje tak ze se neoptimalizuje likelihood ale nejake jine dkriterium, Napr. a ted das seznam e.g. word error rare sentence error rate, phone error rat, MMI, apod.}

As you can see in~Table~\ref{tab:disc_train}, we combine various discriminative training methods
and acoustic feature transformations in order to obtain better results. 
Let us briefly introduce improvements and alternatives to maximum likelihood training methods,
which are used in Vystadial script.

\begin{figure}
    \begin{center}
    \input{images/mfcc-delta}
    \caption{Typical setup with 39 features using \ac{MFCC}.}
    \label{fig:delta} 
    \end{center}
\end{figure}

\begin{itemize}
    \item \todo{Nepatri mezi discriminativni metody - oddelit} 
        One of the historically first signal processing method which captures the dynamics of a speech
        are so called $delta$ and $delta-delta$ coefficients. The~$delta$ coefficients represent the first 
        derivative of static features and $delta-delta$ represents the second derivatives of static acoustic features.
        On Figure~\ref{fig:delta} there is typical feature extraction procedure using \ac{MFCC}\cite{ye2004speech}.
    \item \ac{LDA} is used as linear classifier for acoustic features. \todo{citace}
        It transforms the acoustic features to another feature space with reduced dimension.
    \item \ac{MLLT}\cite{psutka2007benefit} clusters and transforms full covariance matrices for Gaussian distributions used in \ac{HMM}s. Using the full covariance matrices instead of the diagonal one improves the \ac{WER} and the sharing of the covariance matrices makes the technique reasonable efficient.
    \item \ac{MMI} is different criterium to maximum likelihood. The key idea is that maximum likelihood is optimal only under rarely met conditions. \ac{MMI} optimize maximum mutual information objective function\cite{chow1990maximum}.
    \item \ac{UBM}\cite{povey2010subspace} training is the base system for \acl{SAT}. 
        By changing small number of parameters in vector of short length (50) \ac{UBM} controls all the mean and 
        weight parameters of the speech-state-specific mixture model. In addition, it introduce 
        a~"speaker vector," which can be adapted by discriminative \ac{SAT} training.
    \item \acl{SAT}\cite{povey2011kaldi} uses \ac{fMLLR} technique. Note, that in the~experiments we have 
        the~training data without speaker labels, so we suppose that each utterance is spoken by different user.
        In fact, it makes the \ac{SAT} methods hopeless in improving the \ac{WER}, 
        but we include the results for completeness and future comparison.
    \item \todo{delete} \ac{MPE}\cite{povey2007evaluation} is an objective function for discriminative models in \ac{ASR}. In \ac{MPE} experiment the Kaldi toolkit uses modified \ac{MMI} and extended Baum-Welsch algorithm together with the \ac{MPE} objective function to train \ac{HMM} parameters.
    \item \cite{povey2005fmpe} \ac{fMPE} uses \ac{MPE} objective functions to the acoustic features. \todo{check and delete}
\end{itemize}
\begin{table}[!htp]\label{tab:disc_train}
\small{\begin{tabular}{lll}
\hline
Training method name & Script shortcut & Note\\
\hline
Monophone & mono & Base for all other systems.\\
Triphone  & tri1 &  Alignments from mono. Base for others.\\
delta+delta-delta & tri2a & Alignments from tri1 \\
\ac{LDA}+\ac{MLLT} & tri2b &  Base for other experiments \\
\ac{LDA}+\ac{MLLT}+\ac{MMI} & tri2b-mmi & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{MMI}(boost) & tri2b-mmi-b0.05  & Alignments from tri2b \\
\ac{MPE} & tri2b-mpe & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{SAT} & tri3b & Alignments from tri2b. Base for others. \\
\ac{LDA}+\ac{MLLT}+\ac{SAT}+\ac{MMI} & tri3b-mmi  & Alignments from tri3b \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-b & Alignments from \ac{UBM}(based on tri3b) \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-c & tri3b-fmmi-b with changed parameters \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-d & Alignments from \ac{UBM} + \ac{MPE} on features.\\ 
\end{tabular}}
\caption{Training methods overview. All experiments train triphones except the~monophone experiment. Script shortcut is refers to names of the experiments in the "run.sh" script.}
\end{table}

% section training_method (end)

\section{Experimental design and results} 
\label{sec:exps}
\todo{Comment on 2 WER difference generative vs discriminative training}
\todo{Mention that HTK scripts are really good}
\todo{Show that we can get similar results with Kaldi as Matej did with HTK}
The experiments focus on comparing the quality of ASR hypothesis measured by~\ac{WER}.
The speed of the decoding is not important at this moment.  The {\it gmm-latgen-faster}\/ decoder is used for evaluation on testing data. 
\todo{Je to spatne - napsat presne-However, many of the methods from~Table~\ref{tab:disc_train}
require specific feature transformations, which also needs to applied before using the decoder on testing data.}

Note, that other Kaldi decoders may not support some of the trained models.
In general, if a method is slow for the {\it gmm-latgen-faster}\/ decoder, it is slow for other decoders too.
We measure the speed in terms of \ac{RTF}, because the {\it gmm-latgen-faster}\/ decoder is designed for~batch 
processing, so the measuring latency is not convenient.


\ml{datasets}
The experiments were performed on dataset called {\it Vystadial}\footnote{\todo{Add link to Vystadial dataset.}}.
However, in the~experiments we use the pretrained \ac{LM}.
The pretrained \acl{LM} was trained by our colleagues in Cambridge for the~same domain.
The pretrained \ac{LM} was also used in the~\ac{HTK} experiments and we use the~model in experiments 
for comparing comparing Kaldi and \ac{HTK} results.\footnote{The Vystadial recipe can train simple \ac{LM} from the audio transcriptions}.
The pretrained \ac{LM} contains $2802$ unigrams, $109315$ bigrams and
$118217$ trigrams. 


\begin{table}[!htp]\label{tab:exp_best}
\begin{tabular}{lrr}
\hline
\parbox[t]{6.0cm}{\ac{LM} and dictionary / Experiment} & Generated \acs{LM}  & Classic \ac{LM} \\
\hline
\ac{HTK} like parameters    &  $tri3b$-(19.59,17) & $tri2b$-(18.39,16) \\
Best with OOV               & todo & todo \\
Best without OOV            & todo & todo \\
\hline
\end{tabular}
\caption{Kaldi experiments, best Maximum Likelihood method}
\end{table}

\todo{Delete the table? The summary of best results is described~Table~\ref{tab:exp_best}}

In~the detail tables 
The tables~\ref{tab:htk_like},~\ref{tab:htk_like_gen_lm},~\ref{tab:best_oov} 
and~\ref{tab:best_nooov}\footnote{All the tables are in~Appendix~\ref{cha:results}.} presents complete overview of the results. 
Note, that the decoding on test data was performed with different \ac{LM} weights for each method and experiment. 
The we tried \ac{LM} weights $9, 10, 11, ...$ to $20$. 
The best \ac{LM} weight is always the second number in bracket.
\todo{na jakych datech jsi tu  nejlepsi wahu urcil? na test nebo devel. Pokud na test. Pak to musis nejak omluvit protoze to neni idealni
Muzes rict ze to delas aby jsi eliminoval vliv vahy LM na vysledku  a vzhledem k tomu ze netestujem kvalitu LM tak to nevadi jak ze tunis na test datech.}

Similarly to \ac{WER}, we also measured \ac{SER}. It is computed as ratio of sentences at least with one incorrectly decoded word dived by the total number of sentences. Note that \ac{SER} largely depends on length of sentences. 

\begin{table}[!htp]\label{tab:htk_like}\centering\begin{tabular}{l|rrr}
exp             & RT coef       & WER         & SER        \\ 
\hline
mono            & 0.766032      & (41.68, 12) & (67.79, 18)\\ 
tri1            & 0.6366977     & (21.97, 14) & (50.32, 20)\\ 
tri2a           & 0.4987811     & (22.34, 18) & (49.94, 18)\\ 
tri2b           & 0.4434147     & (18.39, 16) & (44.5, 19) \\ 
tri2b-mmi       & 0.50897815    & (14.25, 13) & (39.07, 13)\\ 
tri2b-mmi-b0.05 & 0.49218765    & (14.08, 14) & (38.29, 15)\\ 
tri2b-mpe       & 0.43555075    & (15.62, 16) & (41.01, 15)\\ 
tri3b           & 0.3002624     & (19.15, 17) & (47.22, 18)\\ 
tri3b-mmi       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
tri3b-fmmi-b    & 2.46556       & (14.25, 12) & (38.55, 15)\\ 
tri3b-fmmi-c    & 2.4882825     & (14.2, 14)  & (38.03, 14)\\ 
tri3b-fmmi-d    & 1.84923083333 & (14.47, 14) & (38.42, 14)
\end{tabular}
\caption{Experiment with pretrained LM and dictionary "\ac{HTK}-like".}
\end{table}  

% section introduction_to_training_acoustic_models (end)

\section[Kaldi and \acs{HTK} comparison]{Kaldi and previous \ac{HTK} results comparison} 
\label{sec:compare}

Let us remind, that we use OpenJulius decoder together with HTK models decoder in the~Alex dialog system.
The \ac{HTK} scripts were written by Filip Jurčíček and Matěj Korvas. No discriminative training was performed,
only experiments with mixing with different \acl{LM} settings were performed. 
The best obtained results are listed in~Table~\ref{tab:res}.

\begin{table}[!htp]\label{tab:htk_res}\centering\begin{tabular}{l|rr}
Experiment      & \ac{WER} & \ac{SER} \\
\hline
    Zerograms      & 50.18  & 94.34  \\
    Bigrams        & 33.67  & 68.55  \\
    Trigrams       & 20.31  & 45.10  \\
\end{tabular}
\caption{Best \ac{HTK} results}
\end{table}  

\todo{Tady jende o reportovani vysledky kete jsme dostali z HTK. 
Ale o ukazani ze pro stejnem nastaveni maji jak KALDI a HTK podobne vysledku.
Ze to je ocekavane, protoze pouzite metody jsou stejne/podobne. 
Ze to slouzi k overeni ze nove KALDI modely jsou dobre!
Idealne tady bude jedna tabulka KALDI vs. HTK}

We tried to simulate the~\ac{HTK} settings in the~Kaldi experiment~\ref{tab:htk_like}.
We choose for all experiments the~parameters according the~\ac{HTK} scripts.
The most important parameters are the~maximum Gausians number and the~number of \acl{PDF}.
\todo{pdf numbers and max gausians why 19200? and how did I compute it?} 

In the experiment~\ref{tab:htk_like} we used the available options for reproducing the~\ac{HTK} like generation
of \ac{MFCC} features.

The most comparable settings for \ac{HTK} and Kaldi are the $trigram$ \ac{HTK} results
and the $tri2a$ model in "htk-like" experiment~\ref{tab:htk_like}. 
The~\ac{WER} for this settings can be found in Table~\ref{tab:compare}.
Note we also added a discriminative model from the same "\ac{HTK}-like" experiment.
It is both more accurate and faster than current Kaldi set up.

\begin{table}[!htp]\label{tab:compare}\centering\begin{tabular}{l|rrr}
    Experiment   & \ac{RTF} & \ac{WER} & \ac{SER} \\
\hline
\hline
\ac{HTK} best, Trigrams   & Unknown  & 20.31  & 45.10  \\
Kaldi, "htk like"-$tri2a$, Table~\ref{tab:htk_like} - tri2a & 0.4987811 & 22.34 & 49.94\\
\hline
\ac{HTK}-$tri3b-mmi$       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
\end{tabular} \caption{Result comparison between Kaldi and \ac{HTK}} \end{table}  

To conclude, we find out that Kaldi recognition toolkit is capable of training acoustic models with comparable quality 
like \ac{HTK} toolkit using maximum likelihood training. In addition, Kaldi has rich set of tools for discriminative training, 
which outperforms the maximum likelihood methods.
In Chapter~\ref{cha:decoder} we will describe Kaldi decoders, which are convenient for real-time usage, 
and which also does not loose the ability to produce quality ASR hypothesis.

\todo{Discuze proc se nevenujeme LM}
We do not focus on the language modeling since \ac{LM} is usually domain specific.
We want to use the~Alex dialog system and also the Kaldi recognizer in multiple domains with minimum
setup changes. 
\todo{move to better place: We focus on AM, there two ways a) use very general LM b) train LM from transcriptions}
In future, we would like to use very general \ac{LM} 
\todo{Tohle by mohlo byt vypichnuto nekde jinde, a lepe. Delame jenom AM, LM nas nezajima, ale abychom mohli testovat nejaky potrebujeme. Jsou dva zpusoby. Primo z trenovacich dat, nebo nejaky obcnejsi.}
% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)



% chapter model_training_modelsmodels (end)
