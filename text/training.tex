% !TEX root = main.tex
\chapter{Acoustic model training}
\label{cha:training}

This chapter describes the~acoustic model training.
Having a good acoustic model is the first necessary step in embedding 
quality real-time decoder into a dialog system.
In this chapter, we focus on how to train as good acoustic model as possible
and we do not care too much for the speed of decoding.

The~Section~\ref{sec:setup} introduces the used methods and describes 
how the experiments are organized. The section continues by~presenting our results in~Section~\ref{sec:exps}. 
Later, in in Section~\ref{sec:compare} we briefly compare the obtained 
results with results trained with \ac{HTK} toolkit.
Finally, we sum up the results and we briefly judge the Kaldi toolkit.

\section{The script structure and training methods used} 
\label{sec:setup}

The experiments described further in this chapter were performed on two data sets.
The first dataset can not be published due to licensing reason, 
the~second dataset will be published for free use in autumn.
\ml{classic data}\ml{vystadial data}
Let us name the first one {\it classic data} and the second one {\it vystadial data}.
All the results presented in the~thesis were obtain using vystadial data.
However, for some experiments we use the bigger language model trained 
on~classic data.\footnote{\todo{SHOULD WE PUBLISH THEM?}. The results classic data experiments 
can be found in the enclosed source code.}

\small{The Kaldi toolkit has codified form of training scripts with predefined file system structure. 
In my recipe directory I have subdirectory "s5" where all the files are located. All the recipes in Kaldi have 
 one or more subdirectories named s1, s2, s3, s4 or s5 with predefined structure.

The most important file in the recipe is the "run.sh" file. It launches the training and its evaluation.
It uses other scripts. The scripts special to vystadial recipe are located in subdirectory "local".
The tools shared among all the Kaldi recipes are stored in symbolically linked directories "steps" and "utils".
The files "cmd.sh", "path.sh" and files in "conf" directory stores the short settings.

The directories "data" and "exp" are generated by running the "run.sh" script.
The "exp" directory stores the trained models and evaluation results the "data" 
directory is used for transformed input data.}

\tiny\begin{verbbox}
kaldi
|-- egs
|   |-- kaldi-vystadial-recipe
|   |   |-- s5
|   |   |   |-- cmd.sh
|   |   |   |-- conf
|   |   |   |   |-- decode.config
|   |   |   |   |-- mfcc.conf
|   |   |   |   |-- train_conf.sh
|   |   |   |-- data
|   |   |   |   |-- test 
|   |   |   |   |-- train
|   |   |   |   |-- lang
|   |   |   |-- exp
|   |   |   |-- local
|   |   |   |   |-- backup.sh
|   |   |   |   |-- results.py
|   |   |   |   |-- save_check_conf.sh
|   |   |   |   |-- score.sh
|   |   |   |   |-- vystadial_data_prep.sh
|   |   |   |   |-- vystadial_format_data.sh
|   |   |   |   |-- vystadial_prepare_dict.sh
|   |   |   |-- path.sh
|   |   |   |-- run.sh
|   |   |   |-- steps -> ../../wsj/s5/steps
|   |   |   |-- utils -> ../../wsj/s5/utils
\end{verbbox}
\normalsize

\begin{figure}[!htp]
\centering \theverbbox \caption{\small{Kaldi Vystadial recipe file system structure}}
\label{fig:ascii-box}
\end{figure}

If we say that we run one experiment, it means we train and evaluate from ten to twelve training methods 
for the same data and the same settings.\footnote{The list of methods we choosed to evaluate is based Vassil Panayotov's Voxforge recipe}.

The maximum likelihood training of generative models is described in~Section~\ref{sec:train_ml}.
The overview of methods for discriminative training used in vystadial recipe for training Kaldi models
can be found below.

Let us remind that discriminative model can describe the probability $P(audio|ASR hypothesis)$.
In contrast to generative model it is not capable to generate samples from join probability $P(audio, ASR hypothesis)$. However, the discriminative models usually yield better results than generative models. 

As you can see in~Table~\ref{tab:disc_train}, we combine various discriminative training methods
and feature transformations in order to obtain better results. 
Let us briefly introduce improvements and alternatives to maximum likelihood training methods,
which are used in vystadial script.

\begin{figure}
    \begin{center}
    \input{images/mfcc-delta}
    \caption{Typical setup with 39 features using \ac{MFCC}.}
    \label{fig:delta} 
    \end{center}
\end{figure}

\begin{itemize}
    \item One of the historically first signal processing method which captures the dynamics of a speech
        are so called $delta$ and $delta-delta$ coefficients. The~$delta$ coefficients represent the first 
        derivative of static features and $delta-delta$ represents the second derivatives of static acoustic features.
        On Figure~\ref{fig:delta} there is typical feature extraction procedure using \ac{MFCC}\cite{ye2004speech}.
    \item \ac{LDA} is used as linear classifier for acoustic features. 
        It transforms the acoustic features to another feature space with reduced dimension.
    \item \ac{MLLT}\cite{psutka2007benefit} clusters and transforms full covariance matrices for Gaussian distributions used in \ac{HMM}s. Using the full covariance matrices instead of the diagonal one improves the \ac{WER} and the sharing of the covariance matrices makes the technique reasonable efficient.
    \item \ac{MMI} is different criterium to maximum likelihood. The key idea is that maximum likelihood is optimal only under rarely met conditions. \ac{MMI} optimize maximum mutual information objective function\cite{chow1990maximum}.
    \item \ac{UBM}\cite{povey2010subspace} training is the base system for \acl{SAT}. 
        By changing small number of parameters in vector of short length (50) \ac{UBM} controls all the mean and 
        weight parameters of the speech-state-specific mixture model. In addition, it introduce 
        a~"speaker vector," which can be adapted by discriminative \ac{SAT} training.
    \item \acl{SAT}\cite{povey2011kaldi} uses \ac{fMLLR} technique. Note that in our experiments we have 
        the~training data without speaker labels, so we suppose that each utterance is spoken by different user.
        In fact, it makes the \ac{SAT} methods hopeless in improving the \ac{WER}, 
        but we include the results for completeness and future comparison.
    \item \ac{MPE}\cite{povey2007evaluation} is an objective function for discriminative models in \ac{ASR}. In \ac{MPE} experiment the Kaldi toolkit uses modified \ac{MMI} and extended Baum-Welsch algorithm together with the \ac{MPE} objective function to train \ac{HMM} parameters.
    \item \cite{povey2005fmpe} \ac{fMMI} uses \ac{MPE} objective functions to the acoustic features.
\end{itemize}
\begin{table}[!htp]\label{tab:disc_train}
\small{\begin{tabular}{lll}
\hline
Training method name & Script shortcut & Note\\
\hline
Monophone & mono & Base for all other systems.\\
Triphone  & tri1 &  Alignments from mono. Base for others.\\
delta+delta-delta & tri2a & Alignments from tri1 \\
\ac{LDA}+\ac{MLLT} & tri2b &  Base for other experiments \\
\ac{LDA}+\ac{MLLT}+\ac{MMI} & tri2b-mmi & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{MMI}(boost) & tri2b-mmi-b0.05  & Alignments from tri2b \\
\ac{MPE} & tri2b-mpe & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{SAT} & tri3b & Alignments from tri2b. Base for others. \\
\ac{LDA}+\ac{MLLT}+\ac{SAT}+\ac{MMI} & tri3b-mmi  & Alignments from tri3b \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-b & Alignments from \ac{UBM}(based on tri3b) \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-c & tri3b-fmmi-b with changed parameters \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-d & Alignments from \ac{UBM} + \ac{MPE} on features.\\ 
\end{tabular}}
\caption{Training methods overview. All experiments train triphones except the~monophone experiment. Script shortcut is refers to names of the experiments in the "run.sh" script.}
\end{table}

Each of the training method from~Table~\ref{tab:disc_train} is trained on data from "$data/train$" directory.
Each of the method is also evaluated on data from "$data/test$" directory.
The $gmm-latgen-faster$ decoder is used for evaluation. 
However, many of the methods from~Table~\ref{tab:disc_train}
require specific feature transformations, which needs to applied also on testing data.
% section the_experiments_setup (end)

\section{Performed experiments and results} 
\label{sec:exps}
We designed several experiments described in~Table~\ref{tab:exp_type} at the beginning of writing this thesis.
During the year the training scripts have been improved several times. 
We present the last set of experiments setup, where we get the best result.

The experiments focus on comparing the quality of ASR hypothesis in terms of \ac{WER}.
The speed was not the main concern and other Kaldi decoders may not support some from the trained models.
In general, if a method is slow for the $gmm-latgen-faster$ decoder it is slow for others decoder too.
We measure the speed in terms of \ac{RTF}, because the $gmm-latgen-faster$ decoder is designed for~batch 
processing, so the measuring latency does not make much sense.

\begin{table}[!htp]\label{tab:exp_type}
\begin{tabular}{lrr}
\hline
Language model and dictionary / Experiment & Generated \ac{LM}  & Classic \ac{LM} \\
\hline
\ac{HTK} like parameters    & todo & todo \\
Best with OOV               & todo & todo \\
Best without OOV            & todo & todo \\
\hline
\end{tabular}
\caption{Kaldi experiments}
\end{table}

In the experiments, there are used two types of n-gram \acl{LM}. 
We store both \ac{LM} in~ARPA format\footnote{ARPA format is poorly documented at 
\href{http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html}{SRILM man page}}.
First \ac{LM}, is pre-trained from classic data. It contains $2802$ unigrams, $109315$ bigrams and
$118217$ trigrams. We will not publish the model, and we did not trained it ourselves. 
The second \ac{LM}, is just from the transcriptions of training data. It contains 
$641$ unigrams, $3707$ bigrams and $7610$ trigrams.
We do not focus on the language modeling since \ac{LM} is usually done domain specifically.
We want to use our dialog system and also the Kaldi recognizer in multiple domains with minimum
setup changes. In future, we would like to use very general \ac{LM} and reduce the dependency on~\ac{LM}
as much as possible.


The best results from each experiments are extracted to~Table~\ref{tab:exp_type}\footnote{\todo{SHOULD I ADD THIS} Complete list of the experiments can be found \href{https://redmine.ms.mff.cuni.cz/projects/vystadial/wiki/Acoustic_models/}{Vystadial Redmine Wiki}}. 

In~the detail tables 
The tables \todo{list them} presents complete overview of the results. 
Note, that the decoding on test data was performed with different \ac{LM} weights for each method and 
experiment. The we tried \ac{LM} weights $9, 10, 11, ...$ to $20$. 
The best \ac{LM} weight is always the second number in bracket.

Similarly to \ac{WER}, we also measured \ac{SER}. It is computed as ratio of sentences at least with one incorrectly decoded word dived by the total number of sentences. Note that \ac{SER} largely depends on length of sentences. 

\begin{table}[!htp]\centering\begin{tabular}{l|rrr}
Experiment      & \ac{RTF}       & \ac{WER}         & \ac{SER} \\
\hline
tri2b-mpe       & 0.88688125     & (23.54, 20) & (51.73, 20)\\
mono            & 3.2617725      & (50.3, 20)  & (75.07, 20)\\
tri3b-mmi       & 0.807879083333 & (22.73, 19) & (50.53, 15)\\
tri1            & 1.94605        & (33.34, 20) & (66.4, 20) \\
tri2b-mmi       & 1.72681375     & (23.03, 15) & (50.8, 20) \\
tri2a           & 1.962175       & (33.19, 20) & (66.0, 20) \\
tri2b           & 1.8105575      & (30.08, 19) & (63.47, 19)\\
tri3b           & 0.748003125    & (31.37, 19) & (63.47, 19)\\
tri3b-fmmi-c    & 11.8128208333  & (22.24, 16) & (49.2, 15) \\
tri3b-fmmi-b    & 8.22911708333  & (21.67, 17) & (48.27, 14)\\
tri2b-mmi-b0.05 & 1.146794625    & (22.29, 17) & (49.87, 17)\\
tri3b-fmmi-d    & 6.60894458333  & (22.51, 18) & (49.6, 15)
\end{tabular}
\caption{\todo{1.\ copy the last relevant results.}}
\end{table}  


\begin{table}[!htp]\centering\begin{tabular}{cccc}
exp             & \ac{RTF}       & \ac{WER}         & \ac{SER} \\
tri2b-mpe       & 0.88688125     & (23.54, 20) & (51.73, 20)\\
mono            & 3.2617725      & (50.3, 20)  & (75.07, 20)\\
tri3b-mmi       & 0.807879083333 & (22.73, 19) & (50.53, 15)\\
tri1            & 1.94605        & (33.34, 20) & (66.4, 20) \\
tri2b-mmi       & 1.72681375     & (23.03, 15) & (50.8, 20) \\
tri2a           & 1.962175       & (33.19, 20) & (66.0, 20) \\
tri2b           & 1.8105575      & (30.08, 19) & (63.47, 19)\\
tri3b           & 0.748003125    & (31.37, 19) & (63.47, 19)\\
tri3b-fmmi-c    & 11.8128208333  & (22.24, 16) & (49.2, 15) \\
tri3b-fmmi-b    & 8.22911708333  & (21.67, 17) & (48.27, 14)\\
tri2b-mmi-b0.05 & 1.146794625    & (22.29, 17) & (49.87, 17)\\
tri3b-fmmi-d    & 6.60894458333  & (22.51, 18) & (49.6, 15)
\end{tabular}
\caption{\todo{1.\ copy the last relevant results.}}
\end{table}  


\begin{table}[!htp]\label{tab:htk_like}\centering\begin{tabular}{l|rrr}
% oplatek@kronos:master:custom_dict_and_lm_2013-07-17_18:01:53.593578525$
exp             & RT coef        & WER         & SER        \\ 
\hline
tri3b-fmmi-b    & 1.36150426667  & (13.35, 9)  & (33.89, 9) \\ 
tri2b-mpe       & 0.20711125     & (14.84, 16) & (35.58, 18)\\ 
mono            & 0.3436106      & (40.83, 15) & (64.81, 19)\\ 
tri3b-mmi       & 0.173916626667 & (13.79, 11) & (34.54, 10)\\ 
tri1            & 0.2700173      & (20.63, 20) & (44.5, 20) \\ 
tri2b-mmi       & 0.2609084      & (14.49, 13) & (34.93, 15)\\ 
tri2a           & 0.2824097      & (20.05, 20) & (44.5, 19) \\ 
tri2b           & 0.2155103      & (19.71, 20) & (43.86, 20)\\ 
tri3b           & 0.14386492     & (19.59, 17) & (43.08, 17)\\ 
tri3b-fmmi-c    & 1.5112627      & (13.37, 9)  & (33.76, 9) \\ 
tri2b-mmi-b0.05 & 0.2807327      & (14.47, 13) & (34.28, 15)\\ 
\end{tabular}
\caption{Experiment with generated LM and dictionary "\ac{HTK}-like"\todo{tag2like}\todo{reorder}}
\end{table}  

\clearpage

% section introduction_to_training_acoustic_models (end)

\section[Kaldi and \ac{HTK}]{Kaldi and previous \ac{HTK} results comparison} 
\label{sec:compare}

Let us remind that we use OpenJulius decoder together with HTK models decoder in our dialog system.
The \ac{HTK} scripts were written by Filip Jurčíček and Matěj Korvas. No discriminative training was performed,
only experiments with mixing with different \acl{LM} settings were performed. 
The best obtained results are listed in~Table~\ref{tab:res}.

\begin{table}[!htp]\label{tab:htk_res}\centering\begin{tabular}{l|rr}
Experiment      & \ac{WER} & \ac{SER} \\
\hline
    Zerograms      & 50.18  & 94.34  \\
    Bigrams        & 33.67  & 68.55  \\
    Trigrams       & 20.31  & 45.10  \\
\end{tabular}
\caption{Best \ac{HTK} results}
\end{table}  

We tried to simulate the~\ac{HTK} settings in the~Kaldi experiment~\ref{tab:htk_like}.
We choose for all experiments the~parameters according the~\ac{HTK} scripts.
The most important parameters are the~maximum Gausians number and the~number of \acl{PDF}.
\todo{pdf numbers and max gausians why 19200? and how did I compute it?} 

In the experiment~\ref{tab:htk_like} we used the available options for reproducing the~\ac{HTK} like generation
of \ac{MFCC} features.

The most comparable settings for \ac{HTK} and Kaldi are the $trigram$ \ac{HTK} results
and the $tri2a$ model in "htk-like" experiment~\ref{tab:htk_like}. 
The~\ac{WER} for this settings can be found in Table~\ref{tab:compare}
together with best 
\begin{table}[!htp]\label{tab:compare}\centering\begin{tabular}{l|rrr}
    Experiment   & \ac{RTF} & \ac{WER} & \ac{SER} \\
\hline
\hline
\ac{HTK}-Trigrams   & Unknown  & 20.31  & 45.10  \\
Kaldi~\ref{tab:htk_like} - tri2a & todo & todo   & todo \\
\hline
Kaldi - best and fast descrimintive training & todo  & todo   & todo \\
\end{tabular}
\caption{Best result comparison between Kaldi and \ac{HTK}}
\end{table}  

To conclude, we find out that Kaldi recognition toolkit is capable of training acoustic models with comparable quality like \ac{HTK} toolkit. In addition, Kaldi has rich set of tools for discriminative training.
With the~Kaldi acoustic models good enough ASR hypothesis can be decoded. 
In Chapter~\ref{cha:decoder} we will describe Kaldi decoders, which are convenient for real-time usage
, and which also does do loose the ability to produce quality ASR hypothesis.

% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)



% chapter model_training_modelsmodels (end)
