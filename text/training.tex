% !TEX root = main.tex
\chapter{Acoustic model training}
\label{cha:train}

This chapter describes the~acoustic model training.

Having a good acoustic model is the first necessary step for~a~quality real-time decoder.
In this chapter, we focus on how to train good acoustic model.

\todo{
At first the~Section~\ref{sec:methods} introduces the used methods. 
The chapter continues by~presenting results in~Section~\ref{sec:exps}. 
Later, in~Section~\ref{sec:compare} we compare results obtained by Kaldi toolkit with results
trained with \ac{HTK} toolkit by Matěj Korvas\cite{lrec2014}.
The details about training scripts and its file system organization
is described in Appendix~\ref{cha:setup}.  
}

The Figure~\ref{fig:am-deps} list all acoustic models trained in our recipe and described in Section~\ref{sec:methods}.
It also shows that some sophisticated models need to be trained before other simpler one. 
The sophisticated model is initialized with aligned simpler \ac{AM}.
For example, The Figure show that the basic triphone \ac{AM} is initialized with aligned monophone model.

\begin{figure}[!htp]
    \begin{center}
    \input{images/am-deps}

    \small{\begin{tabular}{lll}
    \hline
    Training method name & Script shortcut \\
    \hline
    Monophone & mono \\
    Triphone  & tri1 \\
    $\Delta + \Delta\Delta$ & tri2a  \\
    \acs{LDA}+\acs{MLLT} & tri2b  \\
    \acs{LDA}+\acs{MLLT}+\acs{MMI} & tri2b\_mmi \\
    \acs{LDA}+\acs{MLLT}+\acs{bMMI} & tri2b\_bmmi \\
    \acs{MPE} & tri2b\_mpe \\
    \hline
    \end{tabular}}
    \end{center}
    \caption{Training partial order among \ac{AM} in our training recipe}
    \label{fig:am-deps} 
\end{figure}

The individual training methods and feature transformation are explained
in~Sections~\ref{sec:methods} and~\ref{sec:transform}.

\section{Training methods} 
\label{sec:methods}
We use two types of \ac{HMM} \ac{AM} training in Vystadial recipe. 
The \acl{ML} training and discriminative training.
The discriminative training is applied on top of \ac{ML} training represented by Viterbi training.

The Viterbi training, described in~Subsection~\ref{sub:trainhmm}, approximates \ac{MLE} in setting parameters to \ac{HMM}s in \ac{AM}.
The \ac{MLE} is general attitude to setting statistical model parameters.
The maximum likelihood estimation search for best parameters $\theta^*$ 
in order to maximize the likelihood function $f$ for \ac{IID} training data illustrated in Equation~\ref{eq:maxlikelihood}.
For \ac{IID} training data holds Equation~\ref{eq:lmjoint} describing data joint probability. 
\begin{equation}\label{eq:lmjoint}
    f(x_1, x_2, x_3, \ldots, x_n | \theta) = f(x_1 | \theta) * f(x_2 | \theta) * \ldots * f(x_n | \theta)
\end{equation}
The likelihood function can be derived from Equation~\ref{eq:lmjoint} assuming training data fixed
and parameter $\theta$ free as described in Equation~\ref{eq:likelihood}.
\begin{equation}\label{eq:likelihood}
    \mathcal{L}(\theta\,|\,x_1,\ldots,x_n) = \sum_{i=1}^n f(x_i|\theta)
\end{equation}
\begin{equation}\label{eq:maxlikelihood}
    \theta^* = argmax_{\theta} \mathcal{L}(\theta\,|\,x_1,\ldots,x_n)
\end{equation}

\ml{Discriminative training}
On the other hand the discriminative training does not optimize the likelihood function.
It uses the generative \ac{AM} and posterior probability to optimize 
other function and change \ac{AM} parameters.
In our training recipe we use two discriminative functions.
\begin{itemize}
    \item \acl{MMI}\cite{chow1990maximum} 
        Note the \ac{MMI} function is implemented as \acs{bMMI} with boosted parameter set to 0.\todo{CHECK}
    \item \acl{bMMI}\cite{povey2008boosted}
    \item \acl{MPE}\cite{povey2003mmi}
\end{itemize}

\ml{Generative training}
In our case the acoustic models {\it mono}, {\it tri1}, {\it tri2a}\/
and {\it tri2b}\/ are trained generatively.
Meaning the models are trained according \ac{MLE} criterion.
On the other hand the acoustic models {\it tri2b\_mmi}, {\it tri2b\_bmmi}\/
and {\it tri2b\_mpe}\/ are trained discriminatively.

The discriminative models usually yield better results than generative models, 
if enough data is available. 



\section{Feature and model space transformations}
\label{sec:transform}

Kaldi toolkit has support for number of feature transformations, which are applied on top
of speech parametrization of \ac{MFCC}(See Subsection~\ref{sub:param}).
Note that we will describe only transformation which are not speaker specific.
Namely we will describe only $\Delta + \Delta\Delta$, \ac{LDA} and \ac{MLLT},
because we use the above mentioned methods in our scripts 
as state of the art set up for speaker independent speech recognition\cite{state_of_art_TODO}.

\subsubsection*{The $\Delta+\Delta\Delta$ features}
The $\Delta + \Delta\Delta$ feature transformation 
just adds new features to speech parametrization.
We use \ac{MFCC} as speech parametrization method. See Subsection~\ref{sub:param} for details.
The $\Delta + \Delta\Delta$ method adds features to \ac{MFCC} feature vector, which approximates 
first and second derivative of \ac{MFCC} features.
See Figure~\ref{fig:delta} for typical setup of $\Delta + \Delta\Delta$ for \ac{MFCC},
where from 13 \ac{MFCC} features 13 $\Delta$ and another 13 $\Delta\Delta$ features are extracted.

\begin{figure}
    \begin{center}
    \input{images/mfcc-delta}
    \caption{Typical setup with 39 features using \ac{MFCC}.}
    \label{fig:delta} 
    \end{center}
\end{figure}


\subsubsection*{\acl{LDA} and \ac{MLLT}}
\ac{LDA} is used as linear classifier for acoustic features.
Based on vectors of length 13 representing static \ac{MFCC} 
spliced together in for each context window the \ac{LDA} tries to 
find the best dynamic information, replacing the $\Delta+\Delta\Delta$ coefficients.

The combination of \ac{LDA} and \ac{MLLT} reduce the feature dimension and
rotates the feature space so the classes after \ac{LDA} has more diagonal covariance in two steps.\cite{TODO_LDA_MLLT}
The \ac{HLDA} performs dimension reduction and space transformation in one step.\cite{TODO_HLDA}
The combination \ac{HLDA} and \ac{MLLT} perform very similar feature transformation
and obtain similar improvements over $\Delta+\Delta\Delta$ transformation 
as \ac{HLDA}\cite{TODO_HLDA}\cite{TODO_LDA_PLUS_MLLT}.
We choose to use \ac{LDA} and \ac{MLLT}, because Kaldi toolkit offers scripts 
for this combination and we gain substantial improvement over $\Delta+\Delta\Delta$ transformation.
See Table~\ref{tab:TODO_results}

In our experiments we use the context window with the default length of 9 frames, 
four frames from left context and four frames from right context.
The \ac{LDA} transforms uses and $\Delta$ and $\Delta\Delta$ dynamic features as prior.



\section{Experimental design and results} 
\label{sec:exps}
\todo{Starting from flat start monophone system we train basic triphone models}
\todo{Comment on 2 WER difference generative vs discriminative training}
\todo{Mention that HTK scripts are really good}
\todo{Show that we can get similar results with Kaldi as Matej did with HTK}
The experiments focus on comparing the quality of ASR hypothesis measured by~\ac{WER}.
The speed of the decoding is not important at this moment.  The {\it gmm-latgen-faster}\/ decoder is used for evaluation on testing data. 
\todo{Je to spatne - napsat presne-However, many of the methods from~Table~\ref{tab:disc_train}
require specific feature transformations, which also needs to applied before using the decoder on testing data.}

Note, that other Kaldi decoders may not support some of the trained models.
In general, if a method is slow for the {\it gmm-latgen-faster}\/ decoder, it is slow for other decoders too.
We measure the speed in terms of \ac{RTF}, because the {\it gmm-latgen-faster}\/ decoder is designed for~batch 
processing, so the measuring latency is not convenient.


\ml{datasets}
The experiments were performed on dataset called {\it Vystadial}\footnote{\todo{Add link to Vystadial dataset.}}.
However, in the~experiments we use the pretrained \ac{LM}.
The pretrained \acl{LM} was trained by our colleagues in Cambridge for the~same domain.
The pretrained \ac{LM} was also used in the~\ac{HTK} experiments and we use the~model in experiments 
for comparing comparing Kaldi and \ac{HTK} results.\footnote{The Vystadial recipe can train simple \ac{LM} from the audio transcriptions}.
The pretrained \ac{LM} contains $2802$ unigrams, $109315$ bigrams and
$118217$ trigrams. 


\begin{table}[!htp]\label{tab:exp_best}
\begin{tabular}{lrr}
\hline
\parbox[t]{6.0cm}{\ac{LM} and dictionary / Experiment} & Generated \acs{LM}  & Classic \ac{LM} \\
\hline
\ac{HTK} like parameters    &  $tri3b$-(19.59,17) & $tri2b$-(18.39,16) \\
Best with OOV               & todo & todo \\
Best without OOV            & todo & todo \\
\hline
\end{tabular}
\caption{Kaldi experiments, best Maximum Likelihood method}
\end{table}

\todo{Delete the table? The summary of best results is described~Table~\ref{tab:exp_best}}

In~the detail tables 
The tables~\ref{tab:htk_like},~\ref{tab:htk_like_gen_lm},~\ref{tab:best_oov} 
and~\ref{tab:best_nooov}\footnote{All the tables are in~Appendix~\ref{cha:results}.} presents complete overview of the results. 
Note, that the decoding on test data was performed with different \ac{LM} weights for each method and experiment. 
The we tried \ac{LM} weights $9, 10, 11, ...$ to $20$. 
The best \ac{LM} weight is always the second number in bracket.
\todo{na jakych datech jsi tu  nejlepsi wahu urcil? na test nebo devel. Pokud na test. Pak to musis nejak omluvit protoze to neni idealni
Muzes rict ze to delas aby jsi eliminoval vliv vahy LM na vysledku  a vzhledem k tomu ze netestujem kvalitu LM tak to nevadi jak ze tunis na test datech.}

Similarly to \ac{WER}, we also measured \ac{SER}. It is computed as ratio of sentences at least with one incorrectly decoded word dived by the total number of sentences. Note that \ac{SER} largely depends on length of sentences. 

\begin{table}[!htp]\label{tab:htk_like}\centering\begin{tabular}{l|rrr}
exp             & RT coef       & WER         & SER        \\ 
\hline
mono            & 0.766032      & (41.68, 12) & (67.79, 18)\\ 
tri1            & 0.6366977     & (21.97, 14) & (50.32, 20)\\ 
tri2a           & 0.4987811     & (22.34, 18) & (49.94, 18)\\ 
tri2b           & 0.4434147     & (18.39, 16) & (44.5, 19) \\ 
tri2b-mmi       & 0.50897815    & (14.25, 13) & (39.07, 13)\\ 
tri2b-mmi-b0.05 & 0.49218765    & (14.08, 14) & (38.29, 15)\\ 
tri2b-mpe       & 0.43555075    & (15.62, 16) & (41.01, 15)\\ 
tri3b           & 0.3002624     & (19.15, 17) & (47.22, 18)\\ 
tri3b-mmi       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
tri3b-fmmi-b    & 2.46556       & (14.25, 12) & (38.55, 15)\\ 
tri3b-fmmi-c    & 2.4882825     & (14.2, 14)  & (38.03, 14)\\ 
tri3b-fmmi-d    & 1.84923083333 & (14.47, 14) & (38.42, 14)
\end{tabular}
\caption{Experiment with pretrained LM and dictionary "\ac{HTK}-like".}
\end{table}  

% section introduction_to_training_acoustic_models (end)

\section[Kaldi and \acs{HTK} comparison]{Kaldi and previous \ac{HTK} results comparison} 
\label{sec:compare}

Let us remind, that we use OpenJulius decoder together with HTK models decoder in the~Alex dialog system.
The \ac{HTK} scripts were written by Filip Jurčíček and Matěj Korvas. No discriminative training was performed,
only experiments with mixing with different \acl{LM} settings were performed. 
The best obtained results are listed in~Table~\ref{tab:res}.

\begin{table}[!htp]\label{tab:htk_res}\centering\begin{tabular}{l|rr}
Experiment      & \ac{WER} & \ac{SER} \\
\hline
    Zerograms      & 50.18  & 94.34  \\
    Bigrams        & 33.67  & 68.55  \\
    Trigrams       & 20.31  & 45.10  \\
\end{tabular}
\caption{Best \ac{HTK} results}
\end{table}  

\todo{Tady jende o reportovani vysledky kete jsme dostali z HTK. 
Ale o ukazani ze pro stejnem nastaveni maji jak KALDI a HTK podobne vysledku.
Ze to je ocekavane, protoze pouzite metody jsou stejne/podobne. 
Ze to slouzi k overeni ze nove KALDI modely jsou dobre!
Idealne tady bude jedna tabulka KALDI vs. HTK}

We tried to simulate the~\ac{HTK} settings in the~Kaldi experiment~\ref{tab:htk_like}.
We choose for all experiments the~parameters according the~\ac{HTK} scripts.
The most important parameters are the~maximum Gausians number and the~number of \acl{PDF}.
\todo{pdf numbers and max gausians why 19200? and how did I compute it?} 

In the experiment~\ref{tab:htk_like} we used the available options for reproducing the~\ac{HTK} like generation
of \ac{MFCC} features.

The most comparable settings for \ac{HTK} and Kaldi are the $trigram$ \ac{HTK} results
and the $tri2a$ model in "htk-like" experiment~\ref{tab:htk_like}. 
The~\ac{WER} for this settings can be found in Table~\ref{tab:compare}.
Note we also added a discriminative model from the same "\ac{HTK}-like" experiment.
It is both more accurate and faster than current Kaldi set up.

\begin{table}[!htp]\label{tab:compare}\centering\begin{tabular}{l|rrr}
    Experiment   & \ac{RTF} & \ac{WER} & \ac{SER} \\
\hline
\hline
\ac{HTK} best, Trigrams   & Unknown  & 20.31  & 45.10  \\
Kaldi, "htk like"-$tri2a$, Table~\ref{tab:htk_like} - tri2a & 0.4987811 & 22.34 & 49.94\\
\hline
\ac{HTK}-$tri3b-mmi$       & 0.3152644     & (14.71, 13) & (39.33, 17)\\ 
\end{tabular} \caption{Result comparison between Kaldi and \ac{HTK}} \end{table}  

To conclude, we find out that Kaldi recognition toolkit is capable of training acoustic models with comparable quality 
like \ac{HTK} toolkit using maximum likelihood training. In addition, Kaldi has rich set of tools for discriminative training, 
which outperforms the maximum likelihood methods.
In Chapter~\ref{cha:decoder} we will describe Kaldi decoders, which are convenient for real-time usage, 
and which also does not loose the ability to produce quality ASR hypothesis.

\todo{Discuze proc se nevenujeme LM}
We do not focus on the language modeling since \ac{LM} is usually domain specific.
We want to use the~Alex dialog system and also the Kaldi recognizer in multiple domains with minimum
setup changes. 
\todo{move to better place: We focus on AM, there two ways a) use very general LM b) train LM from transcriptions}
In future, we would like to use very general \ac{LM} 
\todo{Tohle by mohlo byt vypichnuto nekde jinde, a lepe. Delame jenom AM, LM nas nezajima, ale abychom mohli testovat nejaky potrebujeme. Jsou dva zpusoby. Primo z trenovacich dat, nebo nejaky obcnejsi.}
% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)







% chapter model_training_modelsmodels (end)
