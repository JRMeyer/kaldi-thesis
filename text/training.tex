% !TEX root = main.tex
\chapter{Model training}
\label{cha:training}


\section{The script structure and training methods used} 
\label{sec:the_experiments_setup}

The experiments described further in this chapter were performed on two data sets.
The first dataset can not be published due to licensing reason, 
the~second dataset will be published for free use in autumn.
\ml{classic data}\ml{vystadial data}
Let us name the first one {\it classic data} and the second one {\it vystadial data}.
All the results presented in the~thesis were obtain using vystadial data.
However, for some experiments we use the bigger language model trained 
on~classic data.\footnote{\todo{SHOULD WE PUBLISH THEM?}. The results classic data experiments 
can be found in the enclosed source code.}

\small{The Kaldi toolkit has codified form of training scripts with predefined file system structure. 
In my recipe directory I have subdirectory "s5" where all the files are located. All the recipes in Kaldi have 
 one or more subdirectories named s1, s2, s3, s4 or s5 with predefined structure.

The most important file in the recipe is the "run.sh" file. It launches the training and its evaluation.
It uses other scripts. The scripts special to vystadila recipe are located in subdirecory "local".
The tools shared among all the Kaldi recipes are stored in symbolically linked directories "steps" and "utils".
The files "cmd.sh", "path.sh" and files in "conf" directory stores the short settings.

The directories "data" and "exp" are generated by running the "run.sh" script.
The "exp" directory stores the trained models and evaluation results the "data" 
directory is used for transformed input data.}

\tiny\begin{verbbox}
kaldi
|-- egs
|   |-- kaldi-vystadial-recipe
|   |   |-- s5
|   |   |   |-- cmd.sh
|   |   |   |-- conf
|   |   |   |   |-- decode.config
|   |   |   |   |-- mfcc.conf
|   |   |   |   |-- train_conf.sh
|   |   |   |-- data
|   |   |   |-- exp
|   |   |   |-- local
|   |   |   |   |-- backup.sh
|   |   |   |   |-- results.py
|   |   |   |   |-- save_check_conf.sh
|   |   |   |   |-- score.sh
|   |   |   |   |-- vystadial_data_prep.sh
|   |   |   |   |-- vystadial_format_data.sh
|   |   |   |   |-- vystadial_prepare_dict.sh
|   |   |   |-- path.sh
|   |   |   |-- run.sh
|   |   |   |-- steps -> ../../wsj/s5/steps
|   |   |   |-- utils -> ../../wsj/s5/utils
\end{verbbox}
\normalsize

\begin{figure}[!htp]
\centering \theverbbox \caption{\small{Kaldi Vystadial recipe file system structure}}
\label{fig:ascii-box}
\end{figure}

If we say that we run one experiment, it means we train and evaluate from ten to twelve training methods 
for the same data and the same settings.\footnote{The list of methods we choosed to evaluate is based Vassil Panayotov's Voxforge recipe}.

The maximum likelihood training of generative models is described in~Section~\ref{sec:train_ml}.
The overview of methods for discriminative training used in vystadial recipe for training Kaldi models
can be found below.

Let us remind that discriminative model can describe the probability $P(audio|ASR hypothesis)$.
In contrast to generative model it is not capable to generate samples from join probability $P(audio, ASR hypothesis)$. However, the discriminative models usually yield better results than generative models. 

As you can see in~Table~\ref{tab:disc_train}, we combine various discriminative training methods
and feature transformations in order to obtain better results. 
Let us briefly introduce improvements and alternatives to maximum likelihood training methods,
which are used in vystadial script.

\begin{itemize}
    \item \ac{LDA} is used as linear classifier for acoustic features. 
        It transforms the acoustic features to another feature space with reduced dimension.
    \item \ac{MLLT}\cite{psutka2007benefit} clusters and transforms full covariance matrices for Gaussian distributions used in \ac{HMM}s. Using the full covariance matrices instead of the diagonal one improves the \ac{WER} and the sharing of the covariance matrices makes the technique reasonable efficient.
    \item \ac{MMI} is different criterium to maximum likelihood. The key idea is that maximum likelihood is optimal only under almost never met conditions. \ac{MMI} optimize maximum mutual information objective function\cite{chow1990maximum}.
    \item \ac{UBM}\cite{povey2010subspace} training is the base system for \acl{SAT}. 
        By changing small number of parameters in vector of short length (50) \ac{UBM} controls all the mean and 
        weight parameters of the speech-state-specific mixture model. In addition, it introduce 
        a~"speaker vector," which can be adapted by discriminative \ac{SAT} training.
    \item \acl{SAT}\cite{povey2011kaldi} uses \ac{fMLLR} technique. Note that in our experiments we have 
        the~training data without speaker labels, so we suppose that each utterance is spoken by different user.
        In fact, it makes the \ac{SAT} methods hopeless in improving the \ac{WER}, 
        but we include the results for completeness and future comparison.
    \item \ac{MPE}\cite{povey2007evaluation} is an objective function for discriminative models in \ac{ASR}. In \ac{MPE} experiment the Kaldi toolkit uses modified \ac{MMI} and extended Baum-Welsch algorithm together with the \ac{MPE} objective function to train \ac{HMM} parameters.
    \item \cite{povey2005fmpe} \ac{fMMI} uses \ac{MPE} objective functions to the acoustic features.
\end{itemize}
\begin{table}[!htp]\label{tab:disc_train}
\small{\begin{tabular}{lll}
\hline
Training method name & Script shortcut & Note\\
\hline
Monophone & mono & Base for all other systems.\\
Triphone  & tri1 &  Alignments from mono. Base for others.\\
delta+delta-delta & tri2a & Alignments from tri1 \\
\ac{LDA}+\ac{MLLT} & tri2b &  Base for other experiments \\
\ac{LDA}+\ac{MLLT}+\ac{MMI} & tri2b-mmi & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{MMI}(boost) & tri2b-mmi-b0.05  & Alignments from tri2b \\
\ac{MPE} & tri2b-mpe & Alignments from tri2b \\
\ac{LDA}+\ac{MLLT}+\ac{SAT} & tri3b & Alignments from tri2b. Base for others. \\
\ac{LDA}+\ac{MLLT}+\ac{SAT}+\ac{MMI} & tri3b-mmi  & Alignments from tri3b \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-b & Alignments from \ac{UBM}(based on tri3b) \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-c & tri3b-fmmi-b with changed parameters \\
\ac{UBM}+\ac{fMMI}+\ac{MMI} & tri3b-fmmi-d & Alignments from \ac{UBM} + \ac{MPE} on features.\\ 
\end{tabular}}
\caption{Training methods overview. All experiments train triphones except the~monophone experiment. Script shortcut is refers to names of the experiments in the "run.sh" script.}
\end{table}

% section the_experiments_setup (end)

\section{Introduction to training acoustic models} 
\label{sec:introduction_to_training_acoustic_models}
We will use a basic language model, because we want to compare \ac{HTK} and Kaldi 
toolkit acoustic models accuracy.  In addition, we can reuse the already built 
language model for \ac{HTK}. Other reason is that a language model is very domain 
dependent. % FIXME add citation LM are domain dependent
We want to use our Alex dialog system in other domains except Prague public 
transport, so we do not focus on language modeling. 

During the phase of~training acoustic models, we focus mainly on the quality 
of~the decoded output rather than on the speed of~decoding itself. 
We would like to achieve at least as accurate decoding with Kaldi as we have 
achieved with \ac{HTK}. In this thesis we will set up and evaluate following 
experiments using Kaldi:
\begin{itemize}
    \item Standard generative training
    \item Learning linear transformations (e.g. \ac{HLDA})
    \item Discriminative training 
\end{itemize}

% section introduction_to_training_acoustic_models (end)

\section[Kaldi vs \ac{HTK}]{Kaldi vs \ac{HTK} training scripts and its capabilities} 
\label{sec:kaldi_htk}

% section kaldi_vs_htk_training_scripts_and_its_capabilities (end)

\section{Kaldi Experiment Results} % (fold)
\label{sec:exp_results}

The quality is measured on the decoder output for each acoustic model used. 
The measures used depends on the decoder output. 
On the other hand, all of the decoders used are capable of decoding 
the acoustic signal into textual representation. 

Corresponding \ac{HTK} results could be found at
\href{https://redmine.ms.mff.cuni.cz/projects/vystadial/wiki/Acoustic_models/}{Vystadial Redmine Wiki}

Shortcuts used:
\begin{itemize}
    \item \bf \ac{RTF} = Minimum over number of jobs for each tasks
    \item \bf \ac{WER}, \ac{SER} = Minimum over different LM weights rescoring,
        where LM weight ranges from 9 to 20. 
\end{itemize}

\subsection*{Data classic}

\begin{table}[!htp]\centering\begin{tabular}{cccc}
exp             & \ac{RTF}       & \ac{WER}         & \ac{SER} \\
tri2b-mpe       & 0.88688125     & (23.54, 20) & (51.73, 20)\\
mono            & 3.2617725      & (50.3, 20)  & (75.07, 20)\\
tri3b-mmi       & 0.807879083333 & (22.73, 19) & (50.53, 15)\\
tri1            & 1.94605        & (33.34, 20) & (66.4, 20) \\
tri3b-denlats   & 0.77643725     & None        & None       \\
tri2b-mmi       & 1.72681375     & (23.03, 15) & (50.8, 20) \\
tri2a           & 1.962175       & (33.19, 20) & (66.0, 20) \\
tri2b           & 1.8105575      & (30.08, 19) & (63.47, 19)\\
tri3b           & 0.748003125    & (31.37, 19) & (63.47, 19)\\
tri3b-fmmi-c    & 11.8128208333  & (22.24, 16) & (49.2, 15) \\
tri3b-fmmi-b    & 8.22911708333  & (21.67, 17) & (48.27, 14)\\
tri2b-mmi-b0.05 & 1.146794625    & (22.29, 17) & (49.87, 17)\\
tri2b-denlats   & 1.02629525     & None        & None       \\
tri3b-fmmi-d    & 6.60894458333  & (22.51, 18) & (49.6, 15)
\end{tabular}
\caption{1.\ experiment kaldi-vystadial-recipe commit cf40202263450c72b4f8ef9aa8d6b7f03ac59814.
Initial Kaldi setup according Voxforge recipe.}
\end{table}  


\begin{table}[htp] \centering{
\begin{tabular}{cccc}
exp             & \ac{RTF}      & \ac{WER}         & \ac{SER}        \\
-ri3b-fmmi-b    & 10.422085     & (19.64, 14) & (45.07, 11)\\
tri2b-mpe       & 2.09137125    & (22.06, 15) & (49.6, 14) \\
mono            & 3.4773675     & (48.69, 18) & (74.4, 18) \\
tri3b-mmi       & 1.25892633333 & (20.11, 18) & (47.73, 11)\\
tri1            & 4.459925      & (26.92, 16) & (56.27, 20)\\
tri3b-denlats   & 1.2851525     & None        & None       \\
tri2b-mmi       & 1.68427875    & (20.88, 17) & (46.67, 14)\\
tri2a           & 2.3842725     & (26.09, 17) & (55.07, 17)\\
tri2b           & 1.7085825     & (26.16, 20) & (57.73, 20)\\
tri3b           & 1.073698375   & (27.57, 20) & (58.8, 16) \\
tri3b-fmmi-c    & 12.1484916667 & (19.52, 15) & (45.47, 11)\\
tri2b-mmi-b0.05 & 1.79180375    & (20.63, 14) & (46.4, 14) \\
tri2b-denlats   & 1.196845      & None        & None       \\
tri3b-fmmi-d    & 9.047925      & (19.42, 15) & (45.07, 12)
\end{tabular}
\caption{2.\ experiment kaldi-vystadial-recipe commit 10fe1128519aa263ab805216e381d15378d74821.
Adjust according \ac{HTK} parameters in order to compare to \ac{HTK} models.}
}\end{table}  


\clearpage
\subsection*{Data vystadial}


\begin{table}[htp] \centering{
\begin{tabular}{cccc}
exp             & \ac{RTF}       & \ac{WER}         & \ac{SER}        \\
-ri3b-fmmi-b    & 8.49685875     & (11.32, 15) & (32.6, 20) \\
tri2b-mpe       & 1.042053       & (12.7, 19)  & (36.61, 19)\\
mono            & 2.46068        & (43.51, 13) & (64.17, 15)\\
tri3b-mmi       & 0.921234416667 & (12.21, 14) & (33.89, 18)\\
tri1            & 1.6237775      & (16.87, 19) & (43.73, 19)\\
tri3b-denlats   & 1.03578075     & None        & None       \\
tri2b-mmi       & 1.197875       & (11.52, 11) & (32.86, 17)\\
tri2a           & 1.5251075      & (17.09, 12) & (44.24, 19)\\
tri2b           & 1.060909       & (16.06, 20) & (43.08, 20)\\
tri3b           & 0.77341175     & (17.02, 17) & (44.11, 20)\\
tri3b-fmmi-c    & 8.77094958333  & (12.28, 16) & (34.02, 16)\\
tri2b-mmi-b0.05 & 1.119341       & (11.2, 16)  & (32.08, 17)\\
tri2b-denlats   & 0.9729795      & None        & None       \\
tri3b-fmmi-d    & 6.58549958333  & (11.86, 13) & (33.25, 12)
\end{tabular}
\caption{3.\ experiment kaldi-vystadial-recipe commit 585de385922b431881dfd2508567917045a104ab.
{\bf CHEATING! TEST DATA ARE SUBSET OF TRAIN DATA} on data /ha/projects/vystadial/data/asr/en/voip/{train.old, test.old}}
}\end{table}  


\begin{table}[htp] \centering{
\begin{tabular}{cccc}
exp             & \ac{RTF}      & \ac{WER}         & \ac{SER}        \\
-ri3b-fmmi-b    & 10.5736754167 & (16.87, 16) & (40.62, 16)\\
tri2b-mpe       & 1.2229925     & (17.34, 14) & (41.53, 16)\\
mono            & 2.81766       & (41.59, 13) & (65.72, 15)\\
tri3b-mmi       & 1.07604041667 & (18.23, 14) & (42.69, 15)\\
tri1            & 2.12002       & (23.19, 18) & (50.97, 18)\\
tri3b-denlats   & 0.94768325    & None        & None       \\
tri2b-mmi       & 1.59091125    & (17.49, 14) & (41.27, 15)\\
tri2a           & 1.91198       & (21.44, 15) & (48.9, 15) \\
tri2b           & 1.54646       & (20.62, 18) & (48.38, 18)\\
tri3b           & 0.9433825     & (21.56, 17) & (52.13, 17)\\
tri3b-fmmi-c    & 11.175855     & (17.29, 14) & (41.27, 17)\\
tri2b-mmi-b0.05 & 1.5890125     & (17.22, 15) & (41.01, 15)\\
tri2b-denlats   & 1.00629875    & None        & None       \\
tri3b-fmmi-d    & 8.39958958333 & (16.9, 13)  & (40.62, 17)
\end{tabular}
\caption{4.\ experiment kaldi-vystadial-recipe commit 585de385922b431881dfd2508567917045a104ab.
\ac{HTK} like parameters.
(/ha/projects/vystadial/data/asr/en/voip/{train, test}).
}
}\end{table}  


\begin{table}[htp] \centering{
\begin{tabular}{cccc}
exp             & \ac{RTF}      & \ac{WER}         & \ac{SER}        \\
-ri3b-fmmi-b    & 14.6555625    & (15.57, 19) & (37.9, 14) \\
tri2b-mpe       & 2.4290175     & (16.25, 16) & (41.4, 20) \\
mono            & 4.68498       & (43.02, 14) & (65.59, 13)\\
tri3b-mmi       & 2.5989        & (19.63, 20) & (46.57, 16)\\
tri1            & 4.00148       & (22.61, 18) & (50.84, 17)\\
tri3b-denlats   & 2.3944975     & None        & None       \\
tri2b-mmi       & 3.11491875    & (16.03, 18) & (41.27, 18)\\
tri2a           & 3.5336175     & (22.02, 15) & (49.55, 20)\\
tri2b           & 2.6743175     & (19.88, 20) & (47.48, 20)\\
tri3b           & 1.807084875   & (20.54, 17) & (47.22, 17)\\
tri3b-fmmi-c    & 14.7587791667 & (15.74, 15) & (38.03, 14)\\
tri2b-mmi-b0.05 & 3.23086       & (15.88, 17) & (40.88, 20)\\
tri2b-denlats   & 2.2504575     & None        & None       \\
tri3b-fmmi-d    & 10.4809795833 & (15.62, 14) & (38.03, 16)
\end{tabular}
\caption{5.\ experiment kaldi-vystadial commit 4e96c6704785b2476daf26e0b39eec95f3157f1e.
Data vystadial (/ha/projects/vystadial/data/asr/en/voip/{train, test})
Removed \ac{OOV} words from \ac{LM} $\longrightarrow$ decoder is not trained for \ac{OOV}s and does not produce
any symbol for \ac{OOV}.}
}\end{table}  

\begin{table}[htp] \centering{
\begin{tabular}{cccc}
    exp             & \ac{RTF}    & \ac{WER}         & \ac{SER}        \\
    -ri3b-fmmi-b    & 11.33336625 & (19.42, 15) & (45.07, 13)\\
    tri2b-mpe       & 1.2307175   & (21.22, 19) & (48.8, 15) \\
    mono            & 2.8952      & (49.88, 18) & (74.93, 17)\\
    tri3b-mmi       & 1.24198625  & (20.01, 16) & (46.67, 13)\\
    tri1            & 2.398165    & (28.13, 18) & (58.27, 18)\\
    tri3b-denlats   & 1.20614     & None        & None       \\
    tri2b-mmi       & 1.65662125  & (20.34, 17) & (46.93, 11)\\
    tri2a           & 2.15838     & (28.43, 16) & (58.0, 15) \\
    tri2b           & 1.451105    & (27.96, 20) & (58.93, 20)\\
    tri3b           & 1.0596075   & (27.79, 20) & (58.93, 20)\\
    tri3b-fmmi-c    & 11.4361875  & (19.13, 15) & (45.47, 16)\\
    tri2b-mmi-b0.05 & 2.0089575   & (19.64, 13) & (45.6, 12) \\
    tri2b-denlats   & 1.20083     & None        & None       \\
    tri3b-fmmi-d    & 16.1227625  & (19.5, 18)  & (46.0, 11)
\end{tabular}
\caption{6.\ experiment kaldi-vystadial commit aa7263b3f5c151409a87e3d845d58e39335a4f0c.
Data classic (todo)
Removed \ac{OOV} words from \ac{LM} $\longrightarrow$ decoder is not trained for \ac{OOV}s and does not produce
any symbol for \ac{OOV}.}
}\end{table}  

% section Experiment results (end)

% chapter model_training_modelsmodels (end)
