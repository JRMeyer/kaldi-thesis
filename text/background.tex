% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

% TODO FIND THE BEST INTRODUCTION FROM:
%   Spoken language understanding
%   Discriminative Training and Acoustic Modeling for Speech Recognition
%   Markov Models for Pattern Recognition -Fink
% 
% TODO Watch NLP again on http://coursera.com check it for references for the good explanations
% 
% TODO one goal of the EXPECTED goals of writing thesis 
%       Show that you understand given topic
%       Show that you can write and talk about given topic

Research on field of speech recognition began more than \todo{30} years ago. The statistical approach based on \ac{HMM} established itself very early and nowadays it is de facto standard for continuous speech recognition. If we speak about speech recognition in~this thesis we mean continuous speech recognition based on Hidden Markov Models. 

Section~\ref{sec:general_introduction} provides an~introduction into classical speech recognition. It concisely describes components of speech recognition system. Algorithms for training acoustic and language models as well for decoding are explained. In~Section~\ref{sec:fst} the speech recognition with Finite State Transducers is motivated.

\section{Speech recognition based on \ac{HMM}} 
\label{sec:general_introduction}
Let us gently introduce speech recognition. We will focus on the algorithms and approaches used in next chapters. For simplicity,
let us suppose that the~output of speech recognition is the textual transcription of the input audio. In~Subsection~\ref{sub:lattice} we will define
lattices as other possible output format.

\subsection*{Speech recognition in a world of Artificial intelligence}
\label{sub:intro_ml}
Speech recognition is a classic machine learning technique and has many in common with other machine learning task. There are many algorithms models which are used both in speech recognition and in other machine learning tasks.
Let us name a few examples:
\begin{itemize}
    \item Markov models are used by Viterbi algorithm as well as in \ac{DNA} multiple alignment problem.
    \item Neural Networks are getting popular in speech recognition and are classic solution for hand written text recognition.
    \item Language models are used in machine translation and also in speech recognition. 
\end{itemize}

In machine learning there are usually distinguished two types of learning algorithms, supervised and unsupervised.

\begin{figure}[!htp]
    \begin{center}
    \input{images/supervised-general}
    \input{images/supervised-baum-welsh}
    \caption{Supervised learning idea and example}
    \label{fig:supervised} 
    \end{center}
\end{figure}
{\it Supervised learning}\/ is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
The inferred function is often called {\it model}. For speech recognition tasks the models, namely acoustic models and the language models, are stored in persistent memory before using them. 

Acoustic models are trained using supervised learning. Language models are trained using unsupervised training. 

\begin{figure}[!htp]
    \begin{center}
    \input{images/unsupervised-general}
    \input{images/unsupervised-lm}
    \caption{Unsupervised learning idea and example}
    \label{fig:unsupervised} 
    \end{center}
\end{figure}
{\it Unsupervised learning}\/ can be thought of as finding patterns in the input data beyond what would be considered pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application of statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}
% subsection speech_recognition_as_machine_learning_task (end)

\subsection*{Statistical approach}
\label{sub:statistical_approach}
In the statistical approach we can describe speech recognition as classification problem where for a given acoustic observations $a$ we search for the most probable sequence of~words $w^*$.
\begin{equation}\label{eq:asr}
    w^* = argmax_{w}\{P(w|a)\} 
\end{equation}

Note that an ideal acoustic model would exactly determined the probabilities for every $w$ and $a$ in~Equation~\ref{eq:acoustic}.
\begin{equation}\label{eq:acoustic}
    P(w|a)
\end{equation}
Similarly, an ideal language model will perfectly determined probability for any sequence of words $w$
\begin{equation}
    P(w)
\end{equation}

Having the ideal acoustic and language model we can find the most probable transcription $w*$ for given acoustic observations $a$ by using Bayes rule.
\begin{equation}
    P(w \mid a) = \frac{P(a \mid w) * P(w)}{P(a)}
\end{equation}

By realizing that the probability $P(a)$ of acoustic observations is constant we finish in~Equation~\ref{eq:ideal}.
\begin{align}\label{eq:ideal}
    argmax_w\{P(w \mid a)\} &= argmax_w \{\frac{P(a \mid w) * P(w)}{P(a)}\}\\
                            &= \{P(a \mid w) * P(w)\}
\end{align}
From Equation~\ref{eq:ideal} we know the probability for every transcription $w$. However, in order to obtain the most probable transcription $w*$ we still need to search the possibly infinite set of sequences of words, which represent valid transcriptions $w$. The~{\it search}\/ done by the $armax$ function in~Equation~\ref{eq:ideal} is in speech recognition called {\it decoding}\/ and is realized by the {\it decoder}\/ program. The~introduced components are depicted in~Figure~\ref{fig:components}.
\begin{figure}[!htp]
    \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognition system\cite{ney1990acoustic} \todo{check that the diagram is from there}}
    \label{fig:components} 
    \end{center}
\end{figure}
% subsection statistical_approach (end)



\subsection{Signal processing}
\label{sub:signal}
Signal processing is an~important first step for speech recognition. However, we cover signal processing very briefly, because the methods described in this subsection are de facto standard and most of the toolkits use exactly the same algorithms. The~output of the~signal processing are typically vectors of \ac{MFCC} coefficients. The toolkits used in our dialog system, Kaldi and \ac{HTK} toolkit, produce almost exactly the same \ac{MFCC} coefficients for given audio input.\footnote{The~subtle differences are caused by implementation approaches, but does not effect the quality of \ac{MFCC} coefficients in~significant way.}

An interested reader can found comprehensive introduction into signal processing in~the fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.

Let us introduce the major components of signal processing. 
\begin{enumerate}
    \item The raw audio signal is at first converted from continuous to discrete signal by {\it sampling}.  % TODO describe sampling https://en.wikipedia.org/wiki/Sampling_(signal_processing)
    \item The~discrete signal is transformed into {\it frequency domain}\/ by~\ac{DFT} in overlapping frames.
    \item The frequency spectrum obtained in previous step is transformed onto the mel scale\footnote{\href{https://en.wikipedia.org/wiki/Mel_scale}{Mel scale on Wikipedia}}, using triangular overlapping filters.
    \item From the mel frequencies the logs of the powers are taken from each of the mel frequencies.
    \item At the end the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
\end{enumerate}
The {\it sampling frequency}\/ is typically 8000, 16000, 32000 or 44100 Hz and each of the samples is usually encoded in 8, 16 or 32 bits. In our experiments we use 16 kHz sampling frequency for 16 bit samples.  

The overlapping frames are used because \ac{DFT} is not accurate at the edges of frames. 
Windowing function like Hanning function applied on the signal removes the edge effect of~\ac{DFT}. 

Let us give us example from Kaldi settings. In one of the Kaldi decoders the default frame length is $25ms$ and the frames are shifted by $10ms$ for 16kHz sampling frequency. By multiplying the sample rate with frame length in seconds we find out that there are $16000 * 0.025 = 400$ samples in one frame. The frames are overlapping with $ 16000 * (0.025 - 0.01) = 240$ samples.

In Kaldi there is used by default 25 filters for 25 overlapping bins for frequencies between 20 and $Sampling frequency / 2 = 8000$, which is according the~Nyquist theorem\cite{jerri1977shannon} the highest frequency that can be exactly determined in \ac{DFT}. 

It is common to use only eight to twelve lowest-order \ac{MFCC} coefficients. In Kaldi we are using twelve of them.
Note that the zero-order coefficients just measure the total frame energy and are often discarded.

\todo{pictures and formulas}

% subsection signal_processing (end)
\subsection{Decoding with \ac{HMM}}
\label{sub:decoding_with_hmm}
Let us return to high level point of view to speech recognition from~Figure~\ref{fig:components}.
We briefly described the signal processing unit and now we will focus on the search unit. The key part of speech recognition system.
In order to describe the decoding algorithms we introduce the \acl{HMM}. 

The~\acl{HMM} is a~very powerful statistical method of~characterizing the~observed data samples of a~discrete-time series. Chapter 8, \cite{huang2001spoken}.
We will at first introduce {\it Markov Chain}, then \ac{HMM} will be defined and later we will explain how are the acoustic and language models used in \ac{HMM} framework. In~the~Subsection~\ref{sub:viterbi_algorithm} we finally present the search algorithm for speech recognition.

\subsubsection*{Markov Chain}
\label{ssub:markov_chain}
\todo{Spoken Language Processing page 366}


% subsubsection markov_chain (end)


\subsection*{Viterbi algorithm}
\label{sub:viterbi_algorithm}
The Viterbi algorithm is the~basic search algorithm for \ac{HMM}. 

% subsection viterbi_algorithm (end)


% subsection decoding_with_hmm (end)
\subsection{Baum-Welsh algorithm}
\label{sub:baum_welsh_algorithm}



% subsection baum_welsh_algorithm (end)


\subsection{Other criteria}
\label{sub:other_criteria}

% subsection other_criteria (end)

\subsection{Decoding with lattices}
\label{sub:lattice}



% subsection lattice (end)


% section general_introduction (end)
\section{Finite State Transducers} 
\label{sec:fst}
% OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
% OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
% SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri

\subsection{Description}
\label{sub:description}

% subsection description (end)

\subsection{Kaldi implementation} % (fold)
\label{sec:kaldi}


\subsubsection*{Sources} % (fold)

\begin{itemize}
    \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
    \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
\end{itemize}

\subsubsection*{Decoding graph construction} % (fold)
Kaldi uses \ac{FST} as underlaying representation for all models, which are used to decoding. Consequently, training and decoding of models in Kaldi can be expressed as sequence of operations above \acp{FST}.

Decoding is performed using a final result of training, so called {\it decoding graph}. 
From the high level point of view,
during training we are constructing the decoding graph 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
Namely, the transducers appearing in~Equation~\ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the \ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding graph. 
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}
Let us explain the shortcuts in the list below. Note that the operation are described in detail
at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% The source code of these operations is in fstext and corresponding command-line program are in fstbin/
\begin{itemize}
    \item asl - Add self loops to \ac{FST}
    \item rds - Remove disambiguation symbols from \ac{FST}
    \item H' is \ac{FST} H without self loops
    \item min -\ac{FST} minimization
    \item $A\circ B$  - Composition of \ac{FST} $A$ and $B$.
    \item det - Determinization of \ac{FST}
\end{itemize}

{\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.


\subsubsection*{Kaldi decoders} % (fold)
\begin{itemize}
    \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
    \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$

\end{itemize}
% subsection Kaldi Framework (end)

% section finite_state_automata (end)


% chapter background (end)
