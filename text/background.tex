% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

%{{{
% TODO FIND THE BEST INTRODUCTION FROM:
%   Spoken language understanding
%   Discriminative Training and Acoustic Modeling for Speech Recognition
%   Markov Models for Pattern Recognition -Fink
% 
% TODO Watch NLP again on http://coursera.com check it for references for the good explanations
% 
% TODO one goal of the EXPECTED goals of writing thesis 
%       Show that you understand given topic
%       Show that you can write and talk about given topic
%}}}

The statistical methods for continuous speech recognition were established more than 30 years ago. 
The most popular statistical methods are based on \ac{HMM} \acl{AM} and n-grams \ac{LM},
which are also used in Kaldi, the toolkit of our choice.

The first Section~\ref{sec:back_asr} introduces speech preprocessing, \ac{AM} and \ac{LM} training
and explains the principle of speech decoding.

Next three sections describe decoding or training models for particular software.
The Kaldi toolkit is described in Section~\ref{sec:back_kaldi}, 
the \ac{HTK} toolkit in Section~\ref{sec:back_htk} and 
the OpenJulius decoder in Section~\ref{sec:back_julius}.

\begin{figure}[!htp]
    \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognizer\cite{ney1990acoustic}}
    \label{fig:components} 
    \end{center}
\end{figure}

\section{Automatic speech recognition}
\label{sec:back_asr}

The goal of statistical \ac{ASR} is to decode 
the best speech transcription for given speech recording.
Formally, we search for the most probable sequence of~words $w^*$ given the acoustic observations $a$.
See Equation~\ref{eq:best_seq}, which can be simplified to Equation~\ref{eq:best_fix},
because we want decode $w^*$ for fixed speech $a$.

\begin{equation}\label{eq:best_seq}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{\frac{P(a \mid w) * P(w)}{P(a)}\}
\end{equation}

\begin{equation}\label{eq:best_fix}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{P(a \mid w) * P(w)\}
\end{equation}

The task of acoustic modeling is to estimate the probability $P(a \mid w)$,
which we describe in Section~\ref{sub:am}. 
Similarly, the \ac{LM} represents the probability $P(w)$.
We describe language modeling in Section~\ref{sub:lm}.

Improving the accuracy of speech recognition engine is dependent
mainly on improving the \ac{AM} and also the \ac{LM}.
The \ac{AM} and \ac{LM} are trained separately 
and stored on hard disc.

Typically it is easier to model preprocessed speech, 
so various feature and model space transforms are applied before \ac{AM} training.
The speech recognizer has to use the same speech preprocessing,
which is used for \ac{AM} model training.
See Figure~\ref{fig:components}.


% {\it Supervised learning}\/ is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. 
%Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
% The inferred function is often called {\it model}. 
% 
% {\it Unsupervised learning}\/ can be thought of as finding patterns in the input data beyond what would be considered 
% pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application 
% of~statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}


\subsection{Speech parameterization}
\label{sub:param}
% \section{Signal processing}
% FIXME consult kaldi website with what I have written
% http://kaldi.sourceforge.net/feat.html
The goal of speech parameterization is to reduce the negative environmental influences on speech recognition.
The speech varies in number of aspects. Some of them are listed below:
\begin{itemize}
    \item Differences among speakers pronunciation depends on gender, dialect, voice, etc.
    \item Environmental noises. In application for a dialog system the typical speech is
        recorded on noisy street. 
    \item The recorded channel. 
        For example the telephone signal is reduced to frequency band between 300 to 3000Hz\cite{TODO}.
        The quality of mobile phone signal also influence the quality of the audio signal.
\end{itemize}

Speech parametrization improves robustness of the speech recognition in different recording conditions.
Note that parametrization of speech does not require to know the recording conditions in advance,
which is significant advantage to other audio preprocessing methods.

\ml{acoustic features}
Speech parametrization extracts speech-distinctive acoustic features from raw waveform.
The two most successful methods for speech parametrization in last decades are 
\ac{MFCC}\cite{davis1980comparison} and \ac{PLP}\cite{hermansky1990perceptual}.
\cite{TODO_most_succesful} 
% change ti to something else than 
% SPEECH SIGNAL PARAMETERIZATION TECHNIQUES FOR SPEECHUNDERSTANDING SYSTEM Gaurav K. LEEKHA, Mrs. Meenakshi MEHLA, Shrilekha SAGWAL
The methods are very computationally effective and significantly improves the quality of recognized speech.


The toolkits used in our dialog system, Kaldi and \ac{HTK} toolkit,
compute \ac{MFCC} coefficients for given audio input similarly.\footnote{The~subtle differences are caused by implementation approaches, but does not effect the quality of~\ac{MFCC} coefficients in~significant way.}
We choose \ac{MFCC} as speech parametrization technique for both the toolkits,
so we can compare them.

% An interested reader can find comprehensive introduction into signal processing 
% in~the~fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.
Both \ac{MFCC} and \ac{PLP} transformations are applied on sampled and quantized audio signal.
The sampling frequency is typically 8000, 16000 or 32000 Hz.
Each sample is usually encoded to 8, 16 or 32 bits. 
In our experiments we use 16 kHz sampling frequency for 16 bit samples.  

\ml{feature window}
The \ac{MFCC} or \ac{PLP} statistics are computed for signal for overlapping windows.
In Figure~\ref{fig:mfcc_window} there are 7 windows 
for audio of length 85 ms and window shift and length 10ms and 25 ms.

\begin{figure}[!htp]
    \begin{center}
    \input{images/mfcc_window}
    \caption{\ac{PLP} or \ac{MFCC} features are computed every 10 ms seconds in 25 ms windows.
    Audio length is $(frames-1)*shift + win\_len = 85ms$}
    \label{fig:mfcc_window} 
    \end{center}
\end{figure}

For each window \ac{MFCC} or \ac{PLP} efficiently computes statistics with much lower dimension. 
Let us describe \ac{MFCC} computation for 25 ms window shifted by 10 ms and 16kHz audio sampling frequency. 
The $16000 * 0.025 = 400$ samples in one window are reduced to 13 static cepstral coefficients.

The \ac{MFCC} or \ac{PLP} static features are usually extended 
by time derivatives delta and delta-delta features \cite{psutka2001comparison},
so it results to $13 + 13 + 13 = 39$ \ac{MFCC} delta \& delta-delta features 
per 400 samples in one window.

The \ac{MFCC} features are computed by following steps:

\small{\begin{enumerate}
    \item The~audio samples are transformed into {\it frequency domain}\/ by~\ac{DFT} in the window.
    \item The frequency spectrum from previous step is transformed onto the mel scale, 
        using triangular overlapping filters.
    \item From the mel frequencies the logs of the powers are taken from each of the mel frequencies.
    \item At the end the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
    \item The delta and delta-delta coefficients are computed from the current and previous static features.
\end{enumerate}


\subsection*{Feature space transformations}
Feature space transformations are usually applied in addition to \ac{MFCC} or \ac{PLP} preprocessing.
\ml{frame}
The feature space transformations are applied per frame. 
Each feature vector extracted from \ac{MFCC}
or \ac{PLP} window - frame, is projected to another space.

The linear or affine transforms are expressed by matrix multiplications $Ax$ respectively $Ax^+$.
The matrix $A$ represents the transformation. 
The $x$ is the input vector and $Ax$ are the transformed features.
The affine transformations uses extended vector $(x^+)^T = (x_1, .., x_n, 1)$ and matrix $A: (n+1)*(n+1)$.

There is large variety of available transforms and 
dependent on your acoustic data you should choose the most appropriate one.
The transforms differ according how they are estimated.
Namely for linear transforms how the matrix $A$ is trained.

Some transforms are trained on all acoustic data, some per speaker.
Some transforms are estimated discriminatively, some use generative models.

Let us give overview of feature space transforms available in Kaldi toolkit
and their typical training set up: 

% use and city http://kaldi.sourceforge.net/transform.html#transform_cmllr_global
\begin{itemize}
    \item LDA \todo{describe} \cite{TODO}
    \item HLDA \todo{describe} \cite{TODO}
    \item STC/MLLT\todo{describe} \cite{TODO}
    \item Linear VTLN\todo{describe} \cite{TODO}
    \item Exponential Transform (ET)\todo{describe} \cite{TODO}
    \item Cepstral mean and variance normalization\todo{describe} \cite{TODO}
\end{itemize}


\todo{To conclude, the signal processing samples the continuous audio at regular interval intervals. 
Fixed number of consecutive samples forms a window. The~windows are usually overlapping.
The~acoustic features are computed for each of the overlapping window. 
}

% section signal_processing (end)

\subsection{Acoustic modeling}
\label{sub:am}
Acoustic modeling is arguably the heart of speech recognition.
More realistic modeling of acoustic features directly affects the speech recognition quality 
as seen in Equation~\ref{eq:best_fix}. 
The \ac{AM} estimates the probability $P(a|w)$ of generating acoustic features $a$
for given word $w$.

The acoustic modeling has only partial information available for training \ac{AM}.
Let us remind that acoustic features $a$ are collected every 10 ms.
However, the corresponding textual transcription is time-unaligned.
The hidden information of words time alignment in sentence
makes acoustic model training more challenging.

All modern speech recognition toolkits use \acl{HMM}\cite{TODO}
for modeling uncertainty between acoustic features and corresponding transcription. 

\subsubsection*{Choice of training units}
The most successful acoustic modeling methods does not estimates the $P(a|w)$ directly,
but estimates probability $P(a|f_{b}f_{c}f_{c})$ of generating acoustic features $a$ for triphone $f_{b}f_{c}f_{c}$.

\ml{phoneme}
Phone is the smallest contrastive unit of speech. 
Let see few examples of words and their phonetic transcriptions according CMU dictionary\cite{TODO}.
\begin{itemize}
    \item {\it youngest} \& {\it  Y AH1 NG G AH0 S T }
    \item {\it youngman} \& {\it  Y AH1 NG M AE2 N }
    \item {\it earned} \& {\it ER1 N D}
    \item {\it ear}\/ with two transcribed pronunciations {\it IY1 R}\/ and {\it IH1 R}
\end{itemize}
The CMU dictionary distinguishes among several variations for each vowel e.g. {\it AH1}\/ and {\it AH0}.
It distinguishes pronunciation for {\it earned}\/ and {\it ear}\/
despite the same transcription in first three letters.
It also stores two possible pronunciations for the word {\it ear}.

The acoustic features for a phone significantly depends on context.
The previous and following phone strongly influence the sound of the phone in the middle.

\ml{triphone}
The triphone is sequence of three phonemes and captures the context of single phone.
See~\ref{fig:hmm_words}.
The triphones variate much less according context than phonemes, so can be easily trained.
Let us note that some combination of prefixes has the same effect on the central phoneme.
E.g. {\it q}\/ and {\it k} has the same effect on {\it i}. % cite??
In order to reduce the number of triphones, such triphones are clustered together.

The advantage of training triphone-\ac{AM} over word-\ac{AM} 
is due to words sparsity. 
There is typically less clustered triphones and many words for training data,
so a triphone model can be trained from much more examples than a word model. 

% TODO mentioned curse of dimensionality for words -> more parameters than triphones for less examples

\begin{figure}[!htp]
    \begin{center}
    \input{images/hmm-words}
    \caption{\todo{Triphone models for three words}}
    \label{fig:hmm_words} 
    \end{center}
\end{figure}


\todo{Picture Hmm 3 short words for word HMM}
\todo{Picture Hmm 3 short words for 3phonems}

% \begin{figure}[!htp]
%     \begin{center}
%     \input{images/supervised-general}
%     \input{images/supervised-baum-welsh}
%     \caption{Supervised learning idea and example}
%     \label{fig:supervised} 
%     \end{center}
% \end{figure}

\subsubsection*{Hidden Markov Models}

The~\ac{HMM} is a~very powerful statistical method for~characterizing observed data samples
of~a~discrete-time series with an unknown state. Chapter 8, \cite{huang2001spoken}.
In our case the hidden states represents triphones and we observe samples of  acoustic features.

\todo{define Markov Model}
\todo{define HMM}

\todo{Mention EM-Baum Welsch} 
% EM in Kaldi

\todo{acoustic modeling in Kaldi \url{http://kaldi.sourceforge.net/model.html}}



\subsection{Language modeling}
\label{sub:lm}

Language models are trained using unsupervised training. 

\begin{figure}[!htp]
    \begin{center}
    % \input{images/unsupervised-general}
    \input{images/unsupervised-lm}
    \caption{Unsupervised learning idea and example}
    \label{fig:unsupervised} 
    \end{center}
\end{figure}

\subsection{Speech decoding}
\label{sub:decode}

\subsection{Evaluating \ac{ASR} quality}
\label{sub:eval}

\subsubsection*{The metrics in speech recognition}
\label{sub:the_metrics_in_speech_recognition}
In this thesis we are concerned about three aspects of speech decoder.
The speed, universality and quality of~\ac{ASR} hypothesis.

The universality of a decoder is not very well measurable and we will discuss it 
in~Chapter~\ref{cha:implementation} as an implementation property.

\ml{\acl{RTF}}
We will measure the speed of~the decoders by \acl{RTF} metric or we will express it by latency.
\todo{RTF malo srozumitelne}
\ac{RTF} is defined as $\ac{RTF} = \frac{P}{D}$, where $P$ is time of~the processing the audio by a~decoder on input with length $D$. 

\ml{latency}
The latency is a measure of delay. The delay is measured for each utterance in seconds. 
The delay is measured between time of submitting the last audio for given utterance to the decoder and 
the~time of returning the \ac{ASR} hypothesis from decoder.

\ml{\acl{WER}}
The quality of decoded hypothesis will be measured by \ac{WER}.
Note, that \ac{WER} is a minimum edit operation distance on words between two texts, the textual hypothesis
and its reference.
We will compute it by $\frac{S+D+I}{N}$, where $S$ is number of substitutions,
$D$ is number of deletions, $I$ number of insertions to the reference. 
The $N$ denotes the number of words in reference.

The other formats of textual hypothesis like n-best lists and lattice
will be converted to textual hypothesis, because the reference for our training data is only available  
in~form of text audio transcriptions.
From n-best list we take the best textual hypothesis directly as the first item. From lattice
we extract the transcriptions by search.  

% subsection the_metrics_in_speech_recognition (end)

\section{Kaldi}
\label{sec:back_kaldi}

\section{\ac{HTK}}
\label{sec:back_htk}
\todo{very brief}

\section{OpenJulius}
\label{sec:back_julius}
\todo{jake byly metody sumu ,  triphonem}


\subsection*{Decoding algorithm}
\label{sub:dec_algorithm}
The Viterbi algorithm is the~basic search algorithm for \ac{HMM}. 
Briefly
However

\todo{how online decoder works}

\subsubsection{Decoding with lattices}
\label{sub:lattice}
\todo{what are lattices, why is they are good, depth of lattices for dialog system,
problem of backward search for lattices}
In~Subsection~\ref{sub:lattice} we will describe lattices as another output format convenient for dialog systems.


% subsection lattice (end)

% subsection decoding_algorithm (end)


\section{Training acoustic models} 
\label{sec:train_ml}

This section describes the acoustic training in general and introduces the~maximum likelihood training methods.
The~maximum likelihood training is a base for discriminative methods for acoustic modeling, which are briefly
described in~Chapter~\ref{ref:training}.
In~Chapter~\ref{cha:training}, the focus is on comparing results obtained by numerous of training methods
and not on~explaining the methods.

\todo{Describe generative vs discriminative model}

\todo{clustering phones, trees, monophone and triphone training 
    \url{http://kaldi.sourceforge.net/tree_externals.html}
Shortly from Dans thesis extended Baum-Welsh}

% section training_acoustic_models (end)






% section general_introduction (end)

\section{Finite State Transducers} 
\label{sec:fst}
\todo{Introduction from:
 OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
 OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
 SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri}



\subsubsection*{\todo{Sources - cite them!}} % (fold)

\begin{itemize}
    \item \todo{Consult Vassil blogpost}
    \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
    \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
\end{itemize}

\subsection{Decoding graph construction in~Kaldi} % (fold)
% FIXME add source http://kaldi.sourceforge.net/graph.html
% FIXME consult Vassil blogpost
Kaldi uses \ac{FST} as underlaying representation for all models, which are used to decoding. Consequently, 
training and decoding models in Kaldi can be expressed as sequence of operations above \acp{FST}.

Decoding is performed using a final result of training, so called {\it decoding graph}. 
From the high level point of view,
during training we are constructing the decoding graph 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
Namely, the transducers appearing in~Equation~\ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the \ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding graph. 
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}
Let us explain the shortcuts in the list below. Note that the operation are described in detail
at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% The source code of these operations is in fstext and corresponding command-line program are in fstbin/
\begin{itemize}
    \item asl - Add self loops to \ac{FST}
    \item rds - Remove disambiguation symbols from \ac{FST}
    \item H' is \ac{FST} H without self loops
    \item min -\ac{FST} minimization
    \item $A\circ B$  - Composition of \ac{FST} $A$ and $B$.
    \item det - Determinization of \ac{FST}
\end{itemize}

{\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.


\subsubsection*{Kaldi decoders} % (fold)
\begin{itemize}
    \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
    \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$

\end{itemize}
% subsection Kaldi Framework (end)

% section finite_state_automata (end)


% chapter background (end)
