% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

The statistical methods for continuous speech recognition were established more than 30 years ago. 
The most popular statistical methods are based on \ac{HMM} \acl{AM} and n-grams \ac{LM},
which are also used in Kaldi, the toolkit of our choice.

Section~\ref{sec:back_asr} introduces speech preprocessing, \ac{AM} and \ac{LM} training
and explains the principle of speech decoding.

The~three sections describe decoding or training models for a~particular software.
The Kaldi toolkit is described in Section~\ref{sec:back_kaldi}, 
the \ac{HTK} toolkit in Section~\ref{sec:back_htk} and 
the Julius decoder in Section~\ref{sec:back_julius}.

\begin{figure}[!htp]
    \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognizer\cite{ney1990acoustic}}
    \label{fig:components} 
    \end{center}
\end{figure}

\section{Automatic speech recognition}
\label{sec:back_asr}

The goal of statistical \ac{ASR} is to decode 
the best speech transcription for given speech recording.
Formally, we search for the most probable sequence of~words $w^*$ given the acoustic observations $a$.
See Equation~\ref{eq:best_seq}, which can be simplified to Equation~\ref{eq:best_fix},
because we want decode $w^*$ for fixed speech $a$.

\begin{equation}\label{eq:best_seq}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{\frac{P(a \mid w) * P(w)}{P(a)}\}
\end{equation}

\begin{equation}\label{eq:best_fix}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{P(a \mid w) * P(w)\}
\end{equation}

The task of acoustic modeling is to estimate the probability $P(a \mid w)$,
which we describe in Section~\ref{sub:am}. 
Similarly, the \ac{LM} represents the probability $P(w)$.
We describe language modeling in Section~\ref{sub:lm}.

Improving the accuracy of speech recognition engine is dependent
mainly on improving the \ac{AM} and also the \ac{LM}.
The \ac{AM} and \ac{LM} are trained separately 
and stored on a~hard disc.

Typically it is easier to model preprocessed speech, 
so various feature and model space transforms are applied before \ac{AM} training.
The speech recogniser has to use the same speech preprocessing,
which is used for \ac{AM} model training.
See Figure~\ref{fig:components}.


% {\it Supervised learning}\/ is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. 
%Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
% The inferred function is often called {\it model}. 
% 
% {\it Unsupervised learning}\/ can be thought of as finding patterns in the input data beyond what would be considered 
% pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application 
% of~statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}


\subsection{Speech parameterisation}
\label{sub:param}
% \section{Signal processing}
% FIXME consult kaldi website with what I have written
% http://kaldi.sourceforge.net/feat.html
The goal of speech parameterization is to reduce the negative environmental influences on speech recognition.
The speech varies in a~number of aspects. Some of them are listed below:
\begin{itemize}
    \item Differences among speakers pronunciation depends on gender, dialect, voice, etc.
    \item Environmental noises. In the~application for a dialog system the typical speech is
        recorded on a~noisy street. 
    \item The recorded channel. 
        For example the telephone signal is reduced to frequency band between 300 to 3000Hz.
        The quality of mobile phone signal also influences the quality of the audio signal.
\end{itemize}

Speech parametrisation improves robustness of the speech recognition in different recording conditions.
Note that parametrisation of speech transforms the acoustic signal based on few numerical parameters,
which are constant for all recording conditions during training and decoding.
Simply put, the acoustic signal is transformed by boosting frequencies, which are informative for speech.

\ml{acoustic features}
Speech parametrisation extracts speech-distinctive acoustic features from raw waveform.
The two most successful methods for speech parametrisation in last decades are 
\ac{MFCC}\cite{davis1980comparison} and \ac{PLP}\cite{hermansky1990perceptual}.
% change ti to something else than 
% SPEECH SIGNAL PARAMETERIZATION TECHNIQUES FOR SPEECHUNDERSTANDING SYSTEM Gaurav K. LEEKHA, Mrs. Meenakshi MEHLA, Shrilekha SAGWAL
The methods are very computationally effective and significantly improve the quality of recognised speech.


The toolkits used in our dialog system, Kaldi and \ac{HTK} toolkit,
compute \ac{MFCC} coefficients for given audio input similarly.\footnote{The~subtle differences are caused by implementation approaches, but does not effect the quality of~\ac{MFCC} coefficients in~significant way.}
We choose \ac{MFCC} as speech parametrisation technique for both toolkits,
so we can compare them.

% An interested reader can find comprehensive introduction into signal processing 
% in~the~fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.
Both \ac{MFCC} and \ac{PLP} transformations are applied on a~sampled and quantized audio signal.
The sampling frequency is typically 8000, 16000 or 32000 Hz.
Each sample is usually encoded to 8, 16 or 32 bits. 
In our experiments we use 16 kHz sampling frequency for 16 bit samples.  

\ml{feature window}
The \ac{MFCC} or \ac{PLP} statistics are computed for signal for overlapping windows.
In Figure~\ref{fig:mfcc_window} there are 7 windows 
for audio of length 85 ms and window shift and length 10ms and 25 ms.

\begin{figure}[!htp]
    \begin{center}
    \input{images/mfcc_window}
    \caption{\ac{PLP} or \ac{MFCC} features are computed every 10 ms seconds in 25 ms windows.
    Audio length is $(frames-1)*shift + win\_len = 85ms$}
    \label{fig:mfcc_window} 
    \end{center}
\end{figure}

For each window \ac{MFCC} or \ac{PLP} efficiently computes statistics with a~reduced dimension. 
Let us describe the~\ac{MFCC} computation for 25 ms window shifted by 10 ms and 16kHz audio sampling frequency. 
The $16000 * 0.025 = 400$ samples in one window are reduced to 13 static cepstral coefficients.

The \ac{MFCC} or \ac{PLP} static features are usually extended 
by time derivatives delta and delta-delta features \cite{psutka2001comparison},
so it results to $13 + 13 + 13 = 39$ \ac{MFCC} delta \& delta-delta features 
per 400 samples in one window.

The \ac{MFCC} features are computed by the~following steps:

\small{\begin{enumerate}
    \item The~audio samples are transformed into {\it frequency domain}\/ by~\ac{DFT} in the window.
    \item The frequency spectrum from the~previous step is transformed onto the mel scale, 
        using triangular overlapping filters.
    \item From the mel frequencies the logs of the powers are taken from each of the mel frequencies.
    \item At the end the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
    \item The delta and delta-delta coefficients are computed from the current and previous static features.
\end{enumerate}

\subsection*{Feature space transformations}
Feature space transformations are usually applied in addition to \ac{MFCC} or \ac{PLP} preprocessing.
\ml{frame}
The feature space transformations are applied per frame. 
Each feature vector extracted from \ac{MFCC}
or \ac{PLP} window - frame, is projected to another space.
Some transformation take into account context of several 
preceding (left context) and consecutive frames (right context).

The linear or affine transforms are expressed by matrix multiplications $Ax$ respectively $Ax^+$.
The matrix $A$ represents the transformation. 
The $x$ is the input vector and $Ax$ are the transformed features.
The affine transformations uses extended vector $(x^+)^T = (x_1, \ldots, x_n, 1)$ and matrix $A: (n+1)*(n+1)$.

There is large variety of available transforms and 
dependent on your acoustic data you should choose the most appropriate one.
The transforms differ according how they are estimated.
Namely for linear transforms how the matrix $A$ is trained.

Some transforms are trained on all acoustic data, some per speaker.
Some transforms are estimated discriminatively, some use generative models.

We list some of Kaldi transforms in order to illustrate
rich choice of feature transforms in Kaldi toolkit.
In Section~\ref{sec:transform} we describe in more detail the transforms,
which we use in our training scripts.

% use and city http://kaldi.sourceforge.net/transform.html#transform_cmllr_global
\begin{itemize}
    \item \acl{LDA}\cite{gopinath1998maximum}
    \item \acl{HLDA}\cite{gales1999semi}
    \item \acl{MLLT} also known as \acl{STC}\cite{gopinath1998maximum}
    \item \acl{ET}Exponential Transform (ET)\cite{povey2011exponential}
    \item \acl{CMVN}\cite{molau2003feature}
\end{itemize}


% To conclude, the signal processing samples the continuous audio at regular interval intervals. 
% Fixed number of consecutive samples forms a window. The~windows are usually overlapping.
% The~acoustic features are computed for each of the overlapping window. 

% section signal_processing (end)

\subsection{Acoustic modeling}
\label{sub:am}
Acoustic modeling is arguably the heart of speech recognition.
More realistic modeling of acoustic features directly affects the speech recognition quality 
as seen in Equation~\ref{eq:best_fix}. 
The \ac{AM} estimates the probability $P(a|w)$ of generating acoustic features $a$
for given word $w$.

The acoustic modeling has only partial information available for training \ac{AM}.
Let us remind that acoustic features $a$ are collected every 10 ms.
However, the corresponding textual transcription is time-unaligned.
The hidden information of words time alignment in sentence
makes acoustic model training more challenging.

Modern speech recognition toolkits use \acl{HMM}
for modeling uncertainty between acoustic features and corresponding transcription. 

\subsubsection*{Choice of training units}
The most successful acoustic modeling methods does not estimates the $P(a|w)$ directly,
but estimates probability $P(a|f_{b}f_{c}f_{c})$ of generating acoustic features $a$ for triphone $f_{b}f_{c}f_{c}$.

\ml{phoneme}
Phone is the smallest contrastive unit of speech. 
Let see few examples of words and their phonetic transcriptions according CMU dictionary\cite{weide1998cmu}.
\begin{itemize}
    \item {\it youngest} \& {\it  Y AH1 NG G AH0 S T }
    \item {\it youngman} \& {\it  Y AH1 NG M AE2 N }
    \item {\it earned} \& {\it ER1 N D}
    \item {\it ear}\/ with two transcribed pronunciations {\it IY1 R}\/ and {\it IH1 R}
\end{itemize}
The CMU dictionary distinguishes among several variations for each vowel e.g. {\it AH1}\/ and {\it AH0}.
It distinguishes pronunciation for {\it earned}\/ and {\it ear}\/
despite the same transcription in first three letters.
It also stores two possible pronunciations for the word {\it ear}.

The acoustic features for a phone significantly depends on context.
The previous and following phone strongly influence the sound of the phone in the middle.

\ml{triphone}
The triphone is sequence of three phonemes and captures the context of single phone.
See~\ref{fig:hmm_words}.
The triphones variate much less according context than phonemes, so can be easily trained.
Let us note that some combination of prefixes has the same effect on the central phoneme.
E.g. {\it q}\/ and {\it k} has the same effect on {\it i}. % cite??
In order to reduce the number of triphones, such triphones are clustered together.

The advantage of training triphone-\ac{AM} over word-\ac{AM} 
is due to words sparsity. 
There is typically less clustered triphones and many words for training data,
so a triphone model can be trained from much more examples than a word model. 

% TODO mentioned curse of dimensionality for words -> more parameters than triphones for less examples

\begin{figure}[!htp]
    \begin{center}
    \input{images/hmm-words}
    \caption{Markov Triphone model for three words. Only alignments need to learned.}
    \label{fig:hmm_words} 
    \end{center}
\end{figure}


\begin{figure}[!htp]
    \begin{center}
    \input{images/hmm-alt}
    \caption{Markov Triphone model representing four alternatives.}
    \label{fig:hmm_alt} 
    \end{center}
\end{figure}


% \begin{figure}[!htp]
%     \begin{center}
%     \input{images/supervised-general}
%     \input{images/supervised-baum-welsh}
%     \caption{Supervised learning idea and example}
%     \label{fig:supervised} 
%     \end{center}
% \end{figure}

\subsubsection*{Hidden Markov Models}
The~\ac{HMM} is a~very powerful statistical method for~characterizing observed data samples
of~a~discrete-time series with an unknown state. Chapter 8, \cite{huang2001spoken}.
In our case the hidden states represents triphones and we observe samples of  acoustic features.

\ml{transition probability}
Hidden Markov Models has two type of parameters {\it transition probabilities among states}\/
and {\it probabilistic distribution for generating observation in given state}.
The transition probability is probability of changing state $q$ to state $u$.
Each transition can be represented as arc $e=qu$ between the states $q$ and $u$, see~\ref{fig:hmm_alt} and~\ref{fig:hmm_words}.
The probability is often represented as the weight $w_e$ of arc $e$.
For every node in a Markov model must hold that the sum weights of outgoing arcs is one.

The Markov model emits deterministically in each node an observation during traversal over its arcs.
The \acl{HMM} emits the observation stochastically based on the probabilistic distribution related
with~the~visited state.

\ml{Gaussian HMM}
For speech recognition the multivariate Gaussian distribution is used in all states of \ac{HMM}. 
Typically the Gaussian distribution in each state models acoustic features $a$.
Ideally the estimated \ac{HMM} \ac{AM} would generate in each state features belonging to one triphone.

\subsubsection*{Training \ac{HMM}}
\label{sub:trainhmm}

The Kaldi as well as \ac{HTK} toolkit uses \acl{EM} algorithms to train \ac{HMM} \acl{AM}.
The toolkits models the observation probabilities using multivariate Gaussian distribution 
with dimension of the acoustic features $a$.
The \ac{EM} algorithm starts with \ac{HMM} with initial values. 

Typically, the~transition probabilities follow uniform distribution.
The observation probabilities are usually initialized by multivariate Gaussian distribution
with $\mu$ and $\Sigma$ set to global mean and global covariance matrix 
estimated on all training acoustic data.

\ml{EM}
Let us describe how the \ac{EM} algorithm operates for one pair
of training data consisting of acoustic features $a$ extracted from speech
and corresponding text speech transcription $t$.
We create \ac{HMM} $t'$, where each state represent one triphone. 
The triphones are extracted from transcription $t$ using pronunciation dictionary.
In Figure~\ref{fig:hmm} the transcription {\it how do you do} was expanded \ac{HMM} model representing triphone sequence.
It should be obvious that only parameters of HMM states occurring in $t'$ can be updated from pair $(a, t)$.

The \ac{EM} algorithm iterates following steps in order to update parameters of transition and observation probabilities:
\begin{itemize}
    \item The observation probabilities are computed using \ac{HMM} $t'$. 
    \item {\bf E-step}: Based on the observation probabilities the observation are assigned to states of \ac{HMM} $t'$. 
    \item {\bf M-step}: Based on assignment of observation to states the $t'$ parameters are re-estimated. 
\end{itemize}

\ml{Baum-Welsch}
The acoustic model training conceptually differs for \ac{HTK} and Kaldi only in the {\bf expectation (E) step}.
The \ac{HTK} toolkit in {\bf E-step} finds the most probable distribution 
which map acoustic observation to \ac{HMM} states using \ac{MLE}. \todo{MLE fill few words and cite}. 
Using the most probable distribution in {\bf M-step} leads to Baum-Welsch equations. \cite{huang2001spoken}
This variation of \ac{EM} algorithm for estimating \ac{HMM} parameters is called Baum-Welsch algorithm.

\ml{Viterbi}
On the other hand the Kaldi toolkit applies the Viterbi 
criterion in assigning the acoustic observation to \ac{HMM} states.
Using the Viterbi criterion Kaldi just finds 
the most probable assignment $\phi$ of observations to states.\cite{buthpitiya2012parallel}
Using only the single best assignment $\phi$ the {\bf M-step} is also simpler than
Baum-Welch algorithm case.
We simply refer to \ac{EM} algorithm with Viterbi criterion as Viterbi training.

\subsubsection*{Viterbi training of acoustic models}
We focus on Viterbi training because it is used in Kaldi toolkit and is simpler.
Latest work suggest that Viterbi training is just as effective for continuous
speech recognition as Baum-Welch algorithm \cite{rodriguez2003comparative}.
In addition Viterbi training needs much less computational resources. 

% \todo{Should I add equations from buthpitiya2012parallel ?}

\subsection{Language modeling}
\label{sub:lm}

\ac{LM} effectively reduces and more importantly priorities the \ac{AM} hypothesis.
The statistical \ac{LM} assigns given word sequence its probability.
The probability of a word transcription from \ac{AM} is combined with
the probability of the word transcription from \ac{LM}.

The invalid words sequences or the sequences not frequent in training data
are estimated with low probability. On the other hand the frequent sequences
are assigned with high probability.

The word sequence "{\it are few born new york}" should be rewarded 
with lower probability from \ac{LM} than he alternative with similar phonetic representation
"{\it are you from new york}".

Note that in our settings the \ac{AM} already limits the possible triphone
sequences to sequences present in lexicon words - the words in training data.
% The lexicon bridges the \ac{AM} and \ac{LM}
So the mapping from acoustic features $a$ to triphone sequences
are restricted to sequences, which form probably word sequences.

In practice it is unfeasible to compute the probability
of $k$ word sequence $W$ according Equation~\ref{eq:lm}.
In addition it would require enormous amount of data for estimating
probability for $k>10$.


\begin{equation} \label{eq:lm}
    P(W)=P(w_i,  w_{i-1}, w_{i-2}, ..., w_1)=\prod_{i=1}^{k}{P(w_i|w_{i-1}, w_{i-2}, w_1)}
\end{equation}

\ml{\ac{LM} order}
We call the number $k$ order of \ac{LM}.
In speech recognition we typically use the order of \ac{LM} limited to $k<4$.
The Equation~\ref{eq:ngram} describes how should be the trigram \ac{LM} estimated.
The n-gram is sequence of n consecutive words. The probability of n-gram can be estimated by language models
of order n and greater.

The \ac{LM} estimates the probability of an n-gram $t$ by counting the frequency in text corpus of $t$ and 
also (n-1), (n-2), .. n-grams, which are subsequences of $t$.
The text corpus is typically chosen, so it corresponds to domain, in which the \ac{ASR} system 
with the \ac{LM} should be used.

\ml{LM smoothing}
The technique of estimating probability of unseen n-grams $t$ 
based on n-grams with lower order which form the original $t$ is called {\it smoothing}.
The technique is widely used, because there is much greater chance that lower order n-grams are
present in training data than n-gram with high order.
For example, in our training scripts if no external \ac{LM} is supplied,
we train the \ac{LM} only on text transcriptions from the training data
and we use Witten-Bell smoothing.\cite{witten1991zero}

\begin{equation} \label{eq:ngram}
    P(w_i \mid w_{i-1}, w_{i-2}, ..., w_1) = P(w_i \mid w_{i-1}, w_{i-2}) * P(w_{i-1} \mid w_{i-2}) * P(w_{i-2})
\end{equation}

\subsection{Speech decoding}
\label{sub:decode}
The \ac{CSR} is a pattern recognition task as well as search problem.
In speech recognition, making a search decision is also referred to as decoding.\cite{huang2001spoken}

Combining AM and LM -> LM weight
Formats
Beam search - forward decoding.
Backward decoding only for lattices.


Hypothesis in one-best format can extracted from all presented hypothesis output formats.

\subsection{Evaluating \ac{ASR} quality}
\label{sub:eval}
\ml{WER}
The quality of speech recognizer is typically measured using \acl{WER}.
The \ac{WER} measure is computed on pairs $(decoded(a),t)$ for acoustic recoding $a$ and their human transcription $t$.
The expression $decoded(a)$  in this section denotes \ac{ASR} hypothesis in one-best format 
as described in Subsection~\ref{sub:decode}.

The pair $(a,t)$ is not supposed to be used in \ac{AM} training,
because we are testing the ability to decode unknown speech.
We should also measure the \ac{ASR} quality on speech from speaker,
which was not use for \ac{AM} training, 
because we usually want to decode speech of unheard speaker.

The \ac{WER} is computed as minimum edit distance on words between 
pair $decoded(a), t$ typically using edit operations {\it substitution, deletion, insertion}\/ as described in~\ref{eq:edit_dist}.
The effective implementation for computing WER uses dynamic programing and is not computationally intensive,
because \ac{ASR} hypothesis are typically quite short.
\begin{equation} \label{eq:edit_dist}
    WER = 100* \frac{min\_dist(decoded_{AM, LM}(a), t, edit\_operation=\{Subs, Del, Ins\})}{\#\ words\ in\ t}
\end{equation}
\ml{reference}
Note that \ac{WER} is an error function so the ideal value is 0, meaning the hypothesis $decoded(a)$ and 
the reference transcription $t$ are identical. The \ac{WER} 100 for $decoded(a)$ and reference $t$ 
with equal number of words shows that every single word is different between $decoded(a)$ and reference $t$.
Despite the fact that \ac{WER} resembles percentage format, it can be bigger than 100. See Figure~\ref{fig:wer400}.
\begin{figure}[!htp]
    \begin{center}
    \begin{verbatim}
        decoded(a) = 'hi hi hi hi'
        t='hello'
        WER = 100 * ( 4 / 1) = 400
    \end{verbatim}
    \caption{The \acs{WER} measure can be greater than 100.}
    \label{fig:wer400} 
    \end{center}
\end{figure}

\subsubsection*{Alternative measures}
The \acl{SER} measures how many decoded utterances $decoded(a)$ match exactly its reference $t$
for all pairs $(a, t)$ in test set $T$.
\begin{equation}
    SER = \frac{\sum_{\{(a, t) \in T; decoded(a) = t\}}{1}}{|T|}
\end{equation}

If we are using alternative output format to one best hypothesis as n-best list or lattice,
we are extracting one-one best hypothesis format for measuring \ac{WER} or \ac{SER}.
On the other hand we are using n-best lists or lattice,
because the one-best hypothesis might be wrong and the alternative
hypothesis may be closer to reference.

We do not evaluate any other measurement except \ac{WER} and \ac{SER},
but note that richness and correctness of alternatives are desired qualities.
The alternatives may contain additional information and are for example used
in a dialog system \acl{SLU} unit which parses \ac{ASR} output.

\subsubsection*{Measuring speed}
\label{sub:the_metrics_in_speech_recognition}
In this thesis we are especially concerned about speed of speech decoding,
because the implemented decoder is used in a \acl{SDS}.

\ml{\acl{RTF}}
Very natural measure of speed for speech decoding is \acl{RTF},
which express how much the recognizer decodes faster than the user speaks.
We measure the \ac{RTF} for each recording described in~Equation~\ref{eq:rtf}.
\begin{equation}\label{eq:rtf}
    RTF = \frac{time(decode(a))}{length(a)}
\end{equation}
For real time decoding in a dialog system we need $RTF < 1.0$ for all tested utterances $a$.
In other words the decoding of each utterance $a$ should takes less time than
playing the utterance in a music player.
With $RTF < 1.0$ we can decode faster than the user speaks, 
so the hypothesis is ready immediately after the user finishes the speech. 

Note that in our decoder we have two phases of decoding.
The forward decoding is performed as user speaks, but
the backward decoding is triggered at the very end of the speech
as described in~Subsection~\ref{sub:decode}.
The users have to wait at least the time when backward decoding is performed,
so we measure \ac{FWRTF} for forward decoding.

\ml{latency}
In real time \ac{SDS} the critical measure is a delay how long the user
has to wait for its answer.
The latency measures the time between the end of the user speech and
the time when a decoder returns the hypothesis, 
which is the most important speed measure for \ac{ASR} component in \ac{SDS}.
Note if $FWRTF < 1.0 $ in our decoder then the latency is the time of backward decoding.


\section{\ac{HTK}}
\label{sec:back_htk}
The \ac{HTK} toolkit is set of command line tools, sample scripts and library
for training and decoding \ac{HMM} focused on speech recognition.
\ac{HTK} uses Baum-Welch algorithm for \ac{HMM} parameters estimation.
With the toolkit are distributed two decoders {\it HVite} and {\it HDecode},
which are not designed for real time applications.
{\it HVite} can be used only with unigram or bigram \ac{LM}. 
The {\it HDecode} decoder handles also a trigram \ac{LM}.

The functionality of the library is wrapped by command line programs.
The programs are typically combined in training scripts to train acoustic and language models.
In the Figure~\ref{fig:htk_tools} the acoustic models are labelled as "HMMs" 
and the language models are in \ac{HTK} represented in "Networks".
The trained models are used in one of \ac{HTK} decoders e.g. {\it HVite}\/ for decoding
transcriptions, which can be evaluated using {\it HResults}.

\begin{figure}[!htp]
    \begin{center}
    \input{images/htk_tools}
    \caption{Figure 2.2 from HTK Book 3.4\cite{young2006htk}}
    \label{fig:htk_tools} 
    \end{center}
\end{figure}

The \ac{HTK} use Baum-Welch algorithm to train acoustic models.
The {\it HVite} decoder uses token passing algorithm and Viterbi criterion.
\cite{HTKBook3.4} % chapter 2
Only bigram \ac{LM} can be used with {\it HVite}.
The \{\it HDecode} decoder can handle bigram or trigram language models. 


Let us to stress that we use high quality Bash and Perl scripts for training \ac{HTK} \ac{AM}
from Vertanen improved by Matěj Korvas.\cite{vertanen_baseline_2006}\cite{korvas_2014}

The \ac{HTK} toolkit is licensed under special license\footnote{\url{http://htk.eng.cam.ac.uk/docs/license.shtml}}.
The {\it HDecode} has very similar license condition but can be only used for research purposes.
\footnote{You need to register even to see the license: \url{http://htk.eng.cam.ac.uk/prot-docs/hdecode_register.shtml}}

\section{Julius decoding engine}
\label{sec:back_julius}

% http://julius.sourceforge.jp/juliusbook/en/desc_overview.html
Julius is large vocabulary continuous speech engine, which can use \ac{AM} in \ac{HTK} format for decoding.\cite{lee2009julius}
Julius is BSD licensed\footnote{\url{http://www.linfo.org/bsdlicense.html}} and performs almost real-time decoding.

Julius is two pass decoder. In first pass the forward decoding is perform as
a technique of time synchronous beam search.
The second pass reranks and further prunes the extracted hypothesis from pass one.
The second pass is also known as backward decoding.
Bigram \ac{LM} is used for forward decoding and more complex trigram \ac{LM} is used for backward decoding.

Before the implementation of this thesis was finished 
the team of Alex was interested in Julius, because its ability of real-time decoding 
and confusion 
network\footnote{Confusion network is approximation of lattice described in~Subsection~\ref{sub:lattice}} 
output format.

The Alex team abandon the Julius decoder for software issues with crashes of the decoder. 
The crashes appeared during backward decoding and extracting
the confusion network from Julius. 
In addition the crashes were hard to detect,
because Julius used to run in separate process and the dialog system Alex were waiting for the lattice output of dead process,
which should had been passed through sockets.

\section{Kaldi}
\label{sec:back_kaldi}

Kaldi is speech recognition toolkit consisting of library, command line programs, scripts for focus on acoustic modeling.
Kaldi deploys several decoders for evaluation Kaldi \ac{AM}.
Kaldi uses Viterbi training for estimating \ac{AM}. 
Only in special cases of speaker adaptive discriminative training is also used extended Baum-Welsch algorithm\cite{povey2011kaldi}.

The architecture of Kaldi toolkit could be separated to Kaldi library and training scripts.
The scripts access the functionality of Kaldi library through command line programs.
The C++ Kaldi library is based {\it OpenFST}\cite{allauzen2007openfst} library and 
it uses optimized libraries for linear algebra such as BLAS and LAPACK.
Related functionality is usually grouped in one namespace in C++ code, which usually also correspond
to one directory on file system. The examples of the namespaces or directories can be seen in Figure~\ref{fig:kaldi_arch}

\begin{figure}[!htp]
    \begin{center}
        \includegraphics[width=25em]{images/kaldi-lib}
        \caption{Kaldi toolkit architecture\cite{povey2011kaldi}}
        \label{fig:kaldi_arch} 
    \end{center}
\end{figure}

The Figure~\ref{fig:kaldi_prog} list programs used for speech parametrisation, feature transforms, acoustic model
training, decoding and other utilities. Kaldi provides very useful standardized scripts which wraps
Kaldi programs or add useful functionality. The scripts are located in {\it utils} and {\it steps}
directories and are used in many training script recipes for different corpus data.
In this thesis we created new training recipe using Kaldi infrastructure and
Czech and English training corpus \cite{korvas_2014}.
The recipe, the data and acoustic modeling scripts are described in~Chapter~\ref{cha:train}.

\subsection{Finite State Transducers} 
\label{sec:fst}
In this section we would like to introduce \acl{FST} framework and its
implementation OpenFST, on which the Kaldi library is build on. 
Kaldi uses \ac{FST} as underlaying representation for \ac{LM}, partially for \ac{AM}, lexicon and 
also for representing transformation between text, pronunciation and triphones.

The \ac{FST} framework provides well studied graph operations\cite{mohri2002weighted},
which can be effectively use for acoustic modeling.
Using the \ac{FST} framework the speech decoding task is expressed as
beam search in a graph, which is well studied problem.

The OpenFST library implements memory efficient representation of \ac{FST} and
provides standardized efficient operations.
As stated in \cite{mohri2002weighted} and \cite{povey2011kaldi} the operations can be effectively used
for speech decoding. 

\subsubsection*{OpenFST datastructures in Kaldi}
The \ac{FST} graphs used for \ac{AM} model training and speech decoding
can be constructed as sequence of standardized OpenFST operations.
We would like to introduce toy examples of construction such data structures which are used in Kaldi.

Note that proper introduction to \ac{FST} framework for speech recognition we recommend\cite{mohri2002weighted}.
The Kaldi decoding graph construction is very well described in \href{http://kaldi.sourceforge.net/graph\_recipe\_test.html}{Kaldi documentation}.
We feel that few figures of toy speech recognition problem will help us explain algorithms,
which we describe in Chapters~\ref{cha:train} and~\ref{cha:decoder}.
The inspiration for such attitude was \href{http://vpanayotov.blogspot.cz/2012/06/kaldi-decoding-graph-construction.html}{Vassil Panayotov post}.

\todo{Introduction from:
 OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
 OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
 SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri}


\subsubsection*{\todo{Sources - cite them!}} % (fold)

\begin{itemize}
    \item \todo{Consult Vassil blogpost}
    \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
    \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
\end{itemize}

\subsection{Decoding graph construction in~Kaldi} % (fold)
% FIXME add source http://kaldi.sourceforge.net/graph.html
% FIXME consult Vassil blogpost

Decoding is performed using a final result of training, so called {\it decoding graph}. 
From the high level point of view,
during training we are constructing the decoding graph 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
Namely, the transducers appearing in~Equation~\ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the \ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding graph. 
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}
Let us explain the shortcuts in the list below. Note that the operation are described in detail
at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% The source code of these operations is in fstext and corresponding command-line program are in fstbin/
\begin{itemize}
    \item asl - Add self loops to \ac{FST}
    \item rds - Remove disambiguation symbols from \ac{FST}
    \item H' is \ac{FST} H without self loops
    \item min -\ac{FST} minimization
    \item $A\circ B$  - Composition of \ac{FST} $A$ and $B$.
    \item det - Determinization of \ac{FST}
\end{itemize}

{\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.


\subsubsection*{Kaldi decoders} % (fold)
\begin{itemize}
    \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
    \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$

\end{itemize}

\subsection*{Decoding algorithm}
\label{sub:dec_algorithm}
The Viterbi algorithm is the~basic search algorithm for \ac{HMM}. 
Briefly
However

\todo{how online decoder works}

\subsubsection{Decoding with lattices}
\label{sub:lattice}
\todo{what are lattices, why is they are good, depth of lattices for dialog system,
problem of backward search for lattices}
In~Subsection~\ref{sub:lattice} we will describe lattices as another output format convenient for dialog systems.


% subsection lattice (end)



