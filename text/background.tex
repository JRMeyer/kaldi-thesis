% !TEX root = main.tex
\chapter{Background}
\label{cha:background}
This chapter introduces the basics of speech recognition related to this work. Section \ref{sec:back_asr} introduces speech preprocessing, \acf{AM} and \acf{LM} training, and explains important aspects of speech decoding. Following sections describe specific speech recognition software implementations. The Kaldi toolkit is described in Section \ref{sec:back_kaldi}, the \ac{HTK} toolkit in Section \ref{sec:back_htk} and the Julius decoder in Section \ref{sec:back_julius}.

The statistical methods for continuous speech recognition were established more than 30 years ago. The most popular statistical methods are based on acoustic modelling using \acp{HMM} and n-gram \acp{LM}, which are also used in Kaldi, the toolkit of our choice. We introduce principles of speech recognition and present techniques which are used in Kaldi. 

\begin{figure}[!htp]
  \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognizer\cite{ney1990acoustic}}
    \label{fig:components} 
  \end{center}
\end{figure}

\section{Automatic speech recognition}
\label{sec:back_asr}

The goal of statistical \ac{ASR} is to decode the most likely word sequence given some speech. The term \term{decoding} finds its origin in \acs{HMM} terminology. In speech recognition it is equivalent to \term{recognizing} the word sequence from the speech. Formally, we search for the most probable sequence of words $w^*$ given the acoustic observations $a$ as described in Equation \ref{eq:best_fix}. The best word sequence $w^*$ does not depend on probability of the acoustic features $P(a)$ so it can be eliminated as shown on the second row of the equation.

\begin{equation}
  \label{eq:best_fix}
  \begin{split}
    w^* = argmax_{w}\{P(w \mid a)\} &= argmax_{w}\{\frac{P(a \mid w) * P(w)}{P(a)}\} \\
                                   &= argmax_{w}\{P(a \mid w) * P(w)\}
  \end{split}
\end{equation}

The task of acoustic modelling is to estimate the parameters $\theta$ of a model so that the probability $P(a \mid w ; \theta)$ is as accurate as possible.\footnote{Acoustic modelling is described in Section~\ref{sub:am}.} Similarly, the \ac{LM} represents the probability $P(w)$. \footnote{We describe language modelling in Section~\ref{sub:lm}.}

\ml{frames}
The Figure \ref{fig:components} illustrates the process of decoding the most probable word hypotheses $w^*$ for a given speech utterance. 
First, the~sampled audio signal is processed by speech parametrisation and feature transformations, so the decoding itself takes acoustic features $a$ as input. The acoustic features $a$ are computed on small overlapping windows of audio signal. The acoustic signal in one window is known as a frame.

The decoding itself is performed time synchronously frame by frame using beam search. The beam search expands hypotheses from the previous step by taking into account the new frame features, and it computes probabilities of the expanded hypotheses using the \ac{AM} and the \ac{LM}. If the number of hypotheses exceeds the beam, the low probable hypotheses are pruned. % Pruning only the least probable hypotheses may lead to discarding globally optimal hypotheses.

After decoding the last audio frame, all hypotheses represent the whole utterance. The word labels $w^*$ are extracted from the most probable hypothesis which survived the beam search.

Improving the accuracy of a speech recognition engine depends mainly on improving the \ac{AM} and the \ac{LM} and also on parameters of the beam search such as threshold how many hypotheses are allowed at maximum.


\subsection{Speech parameterisation}
\label{sub:param}
The goal of speech parameterization is to reduce the negative environmental influences on speech recognition. Speech varies in a number of aspects. Some of them are listed below:

\begin{itemize}
\item Differences among speakers' pronunciation depends on gender, dialect, voice, etc.
\item Environmental noises. In the dialogue system Alex (where our \ac{ASR} implementation is used) the speech is typically recorded in a noisy street environment.
\item The recorded channel.
  For example the telephone signal is reduced to frequency band between 300 to 3000Hz.
  The quality of mobile phone signal also influences the quality of the audio signal.
\end{itemize}

Different speech parametrisation may improve robustness of speech recognition for different recording conditions. % Note that speech parametrisation transforms the acoustic signal based on few numerical parameters, which are constant for all recording conditions during training and decoding. Simply put, the acoustic signal is transformed by boosting frequencies, which are informative for speech.

\ml{acoustic features}

Speech parametrisation extracts speech-distinctive acoustic features from the raw waveform. The two most successful methods for speech parametrisation in the last decades are \ac{MFCC}\cite{davis1980comparison} and \ac{PLP}\cite{hermansky1990perceptual}.
% change it to something else than SPEECH SIGNAL PARAMETERIZATION TECHNIQUES FOR SPEECHUNDERSTANDING SYSTEM Gaurav K. LEEKHA, Mrs. Meenakshi MEHLA, Shrilekha SAGWAL

Both \ac{MFCC} and \ac{PLP} transformations are applied on a sampled and quantized audio signal.\footnote{In our experiments we use 16 kHz sampling frequency and 16 bit samples.} For each window \ac{MFCC} or \ac{PLP} efficiently computes statistics with a reduced dimension. % The~sampling frequency is typically 8000, 16000 or 32000 Hz. Each sample is usually encoded to 8, 16 or 32 bits. 
The methods are very computationally effective and significantly improve the quality of recognised speech.

The toolkits used in our dialogue system, Kaldi and \ac{HTK}, compute \ac{MFCC} coefficients for given audio input in a similar way.\footnote{The subtle differences are caused by implementation approaches, but do not affect the quality of \ac{MFCC} coefficients in a significant way.} Therefore, we choose \ac{MFCC} as the speech parametrisation technique for both toolkits, so we can compare them.

% An interested reader can find comprehensive introduction into signal processing in~the~fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.

The \ac{MFCC} statistics are computed for each frame. In Figure \ref{fig:mfcc_window} there are 7 windows --- frames with length of 25 ms and frame shift of 10 ms. The whole utterance lasts 85 ms.

\begin{figure}[!htp]
  \begin{center}
    \input{images/mfcc_window}
    \caption{\ac{PLP} or \ac{MFCC} features are computed every 10 ms seconds in 25 ms windows.
      Audio length is $(frames-1)*shift + win\_len = 85ms$}
    \label{fig:mfcc_window} 
  \end{center}
\end{figure}

Let us describe the \ac{MFCC} computation for a 25 ms window shifted by 10 ms and 16kHz audio sampling frequency. 
The $16000 * 0.025 = 400$ samples in one window are reduced to 13 static cepstral coefficients.

The \ac{MFCC} static features are usually extended by time derivatives $\Delta+\Delta\Delta$ features \cite{psutka2001comparison}.
As a result, \ac{MFCC} $\Delta+\Delta\Delta$ extracts $13 + 13 + 13 = 39$ acoustic features for one frame. 
The original vector of 400 audio samples in one frame is reduced to vector of 39 \ac{MFCC} $\Delta+\Delta\Delta$ acoustic features.

The \ac{MFCC} features are computed by the following steps:

{\small \begin{enumerate}
    \item The audio samples are transformed into the \term{frequency domain} by \ac{DFT} in a window.
    \item The frequency spectrum from the previous step is transformed onto the mel scale, a perceptual scale of frequencies, using triangular overlapping filters.
    \item The logs of the powers are taken from each of the mel frequencies.
    \item At the end, the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
    \item The $\Delta+\Delta\Delta$ coefficients are computed from the current and previous static features. See Figure \ref{fig:delta}.
\end{enumerate}}

\begin{figure}
    \begin{center}
    \input{images/mfcc-delta}
    \caption{Typical setup with 39 features using \ac{MFCC}.}
    \label{fig:delta} 
    \end{center}
\end{figure}

\subsection*{Feature space transformations}
Feature space transformations are usually applied in addition to \ac{MFCC} or \ac{PLP} parametrisation.
\ml{frame}
The feature space transformations are also typically applied per frame, but they usually take into account context of several preceding (left context) and consecutive frames (right context).

The linear and affine transformations are expressed by the matrix multiplications $Ax$ and $Ax^+$, respectively. The matrix $A$ represents the transformation. The input vector is $x$ and $Ax$ are the transformed features. The affine transformation uses the extended vector $(x^+)^T = (x_1, \ldots, x_n, 1)$ and the matrix $A: (n+1)*(n+1)$.

There is a large variety of available transformations. Depending on the acoustic data, one should choose the most appropriate transformation. % The~transformations differ according to how they are estimated, namely for linear transformations how the~matrix $A$ is trained.
Some transformations are estimated discriminatively, some use generative models. Some are speaker dependent, some speaker independent.

We list some of Kaldi transformations in order to illustrate the rich choice of feature transformations in the Kaldi toolkit. % use and city http://kaldi.sourceforge.net/transform.html#transform_cmllr_global

\begin{itemize}
    \item \acf{HLDA}\cite{gales1999semi}.
    \item \acf{LDA}\cite{gopinath1998maximum} is typically used with \acs{MLLT} for speaker independent training.
    \item \acf{MLLT} also known as \acf{STC}\cite{gopinath1998maximum}
    \item \acf{ET}\cite{povey2011exponential} uses a small number of speaker specific parameters for adaptation to the speaker.
    \item \acf{CMVN}\cite{molau2003feature} typically normalises the cepstrum mean and variance per speaker.
\end{itemize}

In our acoustic modelling scripts, see Chapter \ref{cha:train},  we use two non-speaker adaptive feature transformations, which can be computed with very small context. The first transformation, $\Delta+\Delta\Delta$  for \ac{MFCC} coefficients, was already introduced. The second transformation, \ac{LDA} and \ac{MLLT}, is described briefly below.

\subsubsection*{\acl{LDA} and \ac{MLLT} feature transformation}
The \ac{LDA}+\ac{MLLT} is an alternative setup to $\Delta+\Delta\Delta$ transformation in our training scripts. We use it also on top of \ac{MFCC} coefficients. Using several spliced \ac{MFCC} vectors, the \ac{LDA}+\ac{MLLT} searches for the best dynamic transformation.

The combination of \ac{LDA} and \ac{MLLT} applies the feature transformation in two steps: \ac{LDA} reduces the feature dimension and \ac{MLLT} applies the linear simple transformation\cite{gopinath1998maximum}. Whereas, the \ac{HLDA} estimates dimension reduction and space transformation in one step.\cite{gales1999semi} The \ac{LDA} + \ac{MLLT} combination performs very a similar feature transformation to \ac{HLDA} and gains significant improvements over the $\Delta+\Delta\Delta$ transformation similarly as \ac{HLDA}\cite{gales1999semi}\cite{gopinath1998maximum}.

% To conclude, the signal processing samples the continuous audio at regular interval intervals. A fixed number of consecutive samples forms a window. The windows are usually overlapping. The acoustic features are computed for each of the overlapping windows. 

\subsection{Acoustic modelling}
\label{sub:am}
Acoustic modelling is arguably the heart of speech recognition. The \ac{AM} estimates the probability $P(a|w; \theta)$ of generating acoustic features $a$ for given words $w$ and thus directly affects speech recognition quality as seen in Equation~\ref{eq:best_fix}.

Acoustic modelling has only partial information available for training the \ac{AM} parameters $\theta$ because the corresponding textual transcription is time-unaligned. The hidden information of the word (time) alignment in an utterance makes acoustic model training more challenging. Modern speech recognition toolkits use \acl{HMM}s for modelling uncertainty between acoustic features and the corresponding transcription. 

\subsubsection*{Choice of training units}
The most successful acoustic modelling methods do not estimate the $P(a|w)$ directly, but estimate the probability $P(a|f_1f_2f_3f_4)$ of generating acoustic features $a$ for phones $w=f_1f_2f_3f_4$ which forms the pronunciation of the word $w$. Moreover, triphones are used even more successfully for estimating the probability of acoustic features given a word pronunciation.

\ml{phone}
The phone is the smallest contrastive unit of speech. Let us see a few examples of words and their phonetic transcriptions according CMU dictionary\cite{weide1998cmu}.

\begin{itemize}
    \item \term{youngest} \& \term{Y AH1 NG G AH0 S T}
    \item \term{youngman} \& \term{Y AH1 NG M AE2 N}
    \item \term{earned} \& \term{ER1 N D}
    \item \term{ear} with two transcribed pronunciations \term{IY1 R} and \term{IH1 R}
\end{itemize}

The CMU dictionary distinguishes among several variations for each vowel e.g. \term{AH1} and \term{AH0}. It also stores two possible pronunciations for the word \term{ear}.

The acoustic features for a phone significantly depend on its context. The preceeding and following phones strongly influence the sound of the middle phone.

\ml{triphone}
The triphone is a sequence of three phones and captures the context of single phone. As a result, the acoustic properties of triphones vary much less according to the context than phones. Let us note that certain combinations of prefixes have the same effect on the central phone, e.g. \term{q} and \term{k} have the same effect on \term{i}. In order to reduce the number of triphones for acoustic modelling, these triphones are clustered together.

% The~advantage of training triphone-\ac{AM} over word-\ac{AM} is due to word sparsity. There are typically less clustered triphones and many words for training data, so a triphone model can be trained from much more examples than a word model. 

% TODO mentioned curse of dimensionality for words -> more parameters than triphones for less examples

% \begin{figure}[!htp]
%     \begin{center}
%     \input{images/supervised-general}
%     \input{images/supervised-baum-welsh}
%     \caption{Supervised learning idea and example}
%     \label{fig:supervised} 
%     \end{center}
% \end{figure}

\subsubsection*{\acfp{HMM}}
The \ac{HMM} is a very powerful statistical method for characterizing observed data samples of a discrete-time series with an unknown state. \cite{huang2001spoken}. In the case of speech recognition, the hidden states typically represent monophones or triphones and we observe samples of the acoustic features.

\ml{transition probability}
Hidden Markov Models have two types of parameters: \term{transition probabilities among states} and \term{probabilistic distributions for generating an observation in given state}. These parameters need to be estimated in \ac{AM} training.\footnote{Both kind of parameters are denoted together as $\theta$ in Equation \ref{eq:best_fix}.}

The transition probability is the probability of changing state $q$ to state $u$. Each transition is represented as an arc $e=qu$ between the states $q$ and $u$, see \ref{fig:hmm_words}. The probability of the transition is typically represented as the weight $w_e$ of arc $e$.

Importantly, an \ac{HMM} uses a self loop arc $e=uu$ for each state to model acoustic features which are generated several times from the same state $u$. As a result, an \ac{HMM} is able to model variable length of phones. % For every node in a Markov model it must hold that the sum of the weights of outgoing arcs is one.

\ml{Gaussian HMM}
During traversal over one of its arcs, the Markov model emits an observation. The \acl{HMM} emits the observation stochastically based on the probabilistic distribution related to the visited state. In speech recognition, a multivariate Gaussian distribution is typically used to model observation probabilities of \ac{HMM} states. The Gaussian distribution models the probability of emitting acoustic features from a given state. The parameters of the Gaussian distribution are estimated for each state individually. However, states are usually clustered during \ac{AM} training and the states within a cluster share the same parameters for the Gaussian distribution.


\subsubsection*{\ac{HMM} Training}
\label{sub:trainhmm}

Kaldi uses Viterbi training and \ac{HTK} uses \acl{EM} algorithms to train \ac{HMM} \acl{AM}s. Both toolkits model observation probabilities using multivariate Gaussian distributions with the dimension of the acoustic features $a$. % Both Viterbi training and the \ac{EM} algorithm initialize the \ac{HMM} with initial values. 

Typically, the transition probabilities are initialised with values uniformly distributed. The observation probabilities are usually initialized by multivariate Gaussian distributions with $\mu$ and $\Sigma$ set to the global mean and global covariance matrix estimated on all training acoustic data.

\ml{EM}

Let us describe how the \ac{EM} algorithm operates for one pair of training data consisting of (1) the acoustic features $a$ and (2) the corresponding text speech transcription $t$. We create an \ac{HMM} $t'$, where each state represents one monophone.\footnote{We describe the identical training procedure for simplicity on monophones. State-of-the-art \acp{AM} use triphones.} The monophones are extracted from the transcription $t$ using the pronunciation dictionary. In Figure \ref{fig:hmm_words} the utterance \term{how do you do} was expanded to a monophone \ac{HMM} model. Given the \ac{HMM} model for transcription $t$ and the acoustic features $a$, the parameters of the model are estimated. It should be obvious that only states representing phones in the transcription can be trained by the training pair $(a, t)$. Consequently, one needs a lot of training data to robustly estimate the parameters of every state.

The \ac{EM} algorithm iterates over the following steps in order to update the parameters of transition and observation probabilities:

\begin{itemize}
    \item The observation probabilities are computed using the \ac{HMM} $t'$. 
    \item {\bf E-step}: Based on the observation probabilities, the observations are aligned to the states of \ac{HMM} $t'$. 
    \item {\bf M-step}: Based on the alignment of observations to states, the parameters of $t'$ are re-estimated. 
\end{itemize}

\ml{Baum-Welch}
The {\bf E-step} finds a distribution for the alignment between the \ac{HMM} $t'$ and transcription $t$ using \ac{MLE}\cite{gopinath1998maximum} and observation probabilities. \ac{MLE} takes into account all possible alignments and their probabilities to compute the resulting distribution. The Baum-Welch equation can be derived from the fact that the \ac{MLE} criterion is also used for finding the most probable distribution in the {\bf M-step}.\cite{huang2001spoken} % This variation of \ac{EM} algorithm for estimating \ac{HMM} parameters is called Baum-Welch algorithm.

\begin{figure}[!htp]
    \begin{center}
    \input{images/hmm-words}
    \caption{Markov monophone model for four words. Such an \ac{HMM} is constructed for monophone Viterbi training and reference transcriptions \textit{how do you do}. The parameters of the \ac{HMM} model are updated according Equation \ref{eq:mean}, \ref{eq:var} and \ref{eq:weights}.}
    \label{fig:hmm_words} 
    \end{center}
\end{figure}

\subsubsection*{\acl{MLE} method}
\label{sub:mle_method}
The \ac{MLE} is a general approach to setting statistical model parameters. It searches for the best parameters $\theta^*$ in order to maximize the likelihood function $f$ for \ac{IID} training data, as illustrated in Equation \ref{eq:maxlikelihood}. For \ac{IID} training data Equation \ref{eq:lmjoint} holds, describing the joint probability of the data.
 
\begin{equation}\label{eq:lmjoint}
    f(x_1, x_2, x_3, \ldots, x_n | \theta) = f(x_1 | \theta) * f(x_2 | \theta) * \ldots * f(x_n | \theta)
\end{equation}

The likelihood function can be derived from Equation \ref{eq:lmjoint} assuming fixed training data and the free parameter $\theta$, as described in Equation \ref{eq:likelihood}.

\begin{equation}\label{eq:likelihood}
    \mathcal{L}(\theta\,|\,x_1,\ldots,x_n) = \sum_{i=1}^n log(f(x_i|\theta))
\end{equation}

\begin{equation}\label{eq:maxlikelihood}
    \theta^* = argmax_{\theta} \mathcal{L}(\theta\,|\,x_1,\ldots,x_n)
\end{equation}

\subsubsection*{Viterbi training of acoustic models}
On the other hand, the Kaldi toolkit applies the Viterbi criterion in assigning the acoustic observations to \ac{HMM} states. Viterbi training approximates the \ac{EM} algorithm by choosing the single best alignment and maximizing the posterior probability for the chosen alignment. Latest work suggests that Viterbi training is just as effective for continuous speech recognition as the Baum-Welch algorithm \cite{rodriguez2003comparative}. Moreover, Viterbi training needs much fewer computational resources. 

We detail the Viterbi training since it is used in the Kaldi toolkit for acoustic model training, and a very similar algorithm is used for Viterbi decoding.

Given a set of training observations $O^r, 1 \le r \le R$ and an \ac{HMM} state sequence $1<j<N$ the observation sequence is aligned to the state sequence via Viterbi alignment.\cite{buthpitiya2012parallel} The best alignment $T$ results from maximising Equation \ref{eq:vit_align} for $1<i< N$.

\begin{equation}\label{eq:vit_align}
    \phi_N(T)= max_i[\phi_i(T)a_{iN}] 
\end{equation}

% \todo{describe T and T versus $o_t$}
% \todo{Should I cite every equation?}

The $\phi_i(o_t)$ from Equation \ref{eq:vit_align} is computed recursively according to Equation \ref{eq:state_prob}.

\begin{equation}\label{eq:state_prob}
  \phi_i(o_t) = b_j(o_t) max \left\{
  \begin{array}{lr}
    \phi_j(t-1)a_{jj}\\
    \phi_{j-1}(t-1)a_{(j-1)j}
  \end{array}
  \right.
\end{equation}

The initial conditions are $\phi_1(1)=1$ and $\phi_j(1)= a_{1j}b_j(o_1)$, for $ 1 < j < N$. In our case the likelihoods are modeled as mixture Gaussian densities, so the output probability $b_j(o_t)$ is defined as in Equation \ref{eq:observ_prob}.

\begin{equation}\label{eq:observ_prob}
  b_j(o_t) = \sum_{m=1}^{M_j}{c_{jm}\mathcal{N}(o_t; \mu_{jm}, \Sigma_{jm})}
\end{equation}

The $M_j$ represents the number of mixture components in state $j$ $c_{jm}$ is the weight of the $m^{th}$ component, and $\mathcal{N}(o_t; \mu_{jm}, \Sigma_{jm})$ is a multivariate Gausian with mean vector $\mu$ and covariance $\Sigma$.

Firstly, model parameters are updated based on the single-best alignment of individual observations to states and Gaussian components within states. Secondly, transition probabilities are estimated from the relative frequencies from Equation \ref{eq:freq}, where $A_{ij}$ denotes the number of transitions from state $i$ to state $j$.

\begin{equation}\label{eq:freq}
  \hat{a}_{ij} = \frac{A_{ij}}{\sum_{k=2}^{N}{A_{ik}}}
\end{equation}

The indicator function $\psi^r_{jm}(t)$ is used for updating the means and the covariance matrix from statistics. It returns one if $o^r_t$ is associated with mixture component $m$ of state $j$ and zero otherwise. The mean vector and covariance matrix are updated according to Equations \ref{eq:mean} and \ref{eq:var}.

\begin{equation}\label{eq:mean}
  \hat{\mu_{jm}} = \frac{\sum_{r=1}^{R}{\sum_{t=1}^{T_r}{\psi^r_{jm}(t)o^r_t}}}  {\sum_{r=1}^{R}{\sum_{t=1}^{T_r}{\psi^r_{jm}(t)}}}
\end{equation}

\begin{equation}\label{eq:var}
  \hat{\Sigma_{jm}} = \frac{\sum_{r=1}^{R}{\sum_{t=1}^{T_r}{\psi^r_{jm}(t)(o^r_t - \hat{\mu_{jm}})(o^r_t - \hat{\mu_{jm}})'}}}  {\sum_{r=1}^{R}{\sum_{t=1}^{T_r}{\psi^r_{jm}(t)}}}
\end{equation}

Finally, the mixture weights are computed based on the number of observations allocated to each component.\footnote{The Viterbi equation has the same notation as in \cite{buthpitiya2012parallel}.}

\begin{equation}\label{eq:weights}
  c_{jm} = \frac{\sum_{r=1}^{R}{\sum_{t=1}^{T_r}{\psi^r_{jm}(t)}}} {\sum_{r=1}^{R}{\sum_{t=1}^{T_r}\sum_{l=1}^{M}{\psi^r_{jl}(t)}}}
\end{equation}

\ml{generative training}
To conclude, \ac{AM}s are trained using \ac{MLE} or Viterbi training, which approximates the theoretically optimal \ac{MLE} Baum-Welch training; however, in practice Viterbi training performs as well as \ac{MLE} modelling. Baum-Welch and Viterbi training aim at modelling the likelihood of a spoken utterance and perform so called generative training. However, discriminative methods, which re-estimate generative \acp{AM}, perform better.

\subsubsection*{Discriminative training}
\label{sub:subsection_name}

\ml{discriminative training}

Discriminative training uses an objective function and likelihoods from a generative model to discriminate -- i.e. to boost differences -- between high probable and low probable hypotheses. Discriminative training is typically initialised by an acoustic generative model from Baum-Welch or Viterbi training. Then, the likelihood from the generative model is boosted according an objective function, and the \ac{AM} is re-estimated. The following objective functions and accordingly named discriminative training methods are used in our training scripts:

\begin{itemize}
\item \acl{MMI}\cite{chow1990maximum} % Note the~\ac{MMI} function is implemented as \acs{bMMI} with boosted parameter set to 0.
\item \acl{bMMI}\cite{povey2008boosted}
\item \acl{MPE}\cite{povey2003mmi}
\end{itemize}

For details on how the methods are initialized and their usage in Kaldi, see Chapter \ref{cha:train}.

\subsection{Language modelling}
\label{sub:lm}

A \acl{LM} effectively reduces and more importantly prioritises the \ac{AM} hypothesis. The probability of acoustic features given a transcription of words $P(a|w)$ (estimated by the \ac{AM}) is combined with the probability of the transcription of words $P(w)$ (estimated by the \ac{LM} for a given domain) in order to compute the posterior probability of the transcription given the features $P(w|a) = \frac{P(a|w)*P(w)}{P(a)}$.

The statistical \ac{LM} assigns a given word sequence its probability according Equation \ref{eq:lm}. The most used n-gram \ac{LM} computes the probability of a length $k$ word sequence $W$ according to Equation \ref{eq:lm}.\cite{brants2007large} The Markov assumption approximates the probability by assuming that only the most recent $n-1$ words are relevant when predicting next word.

\ml{\ac{LM} order}

We call the number $n$ the order of the \ac{LM}.

\begin{equation} \label{eq:lm}
  P(W)=P(w_k,  w_{k-1}, w_{k-2}, ..., w_1) \approx \prod_{i=1}^{k}{P(w_i|w^{i-1}_{i-n+1})}
\end{equation}

The probability $P(w_i|w^{i-1}_{i-n+1})$ for each word $w_i$ is estimated using relative frequencies of the n-grams, (n-1)-grams, (n-2)-grams, \ldots etc, on training data. The \ac{MLE} is used for estimating relative frequencies $r$ according to Equation \ref{eq:freq}.

\begin{equation} \label{eq:freq}
  r(w_i|w^{i-1}_{i-n+1}) = \frac{f(w^i_{i-n+1})}{f(w^{i-1}_{i-n+1})}
\end{equation}

Equation \ref{eq:freq} is intuitive but many valid and even reasonable utterances are missing or too few. Consequently, the denominator might be zero and the relative frequency may be undefined.

\ml{sparse data}

This is known as sparse data problem.

\ml{LM smoothing}

Smoothing techniques are often used to estimate the higher n-gram relative frequencies based on lower frequencies \cite{goodman2001bit}. In principle, the predictive accuracy of the language model can be improved by increasing the order of the n-gram. However, doing so further exacerbates the sparse data problem.\cite{brants2007large}

The \ac{LM} estimates probabilities by counting relative frequencies from a text corpus which is typically chosen according to the targeted \ac{ASR} domain. For example, in the training scripts which are described in Chapter \ref{cha:train} we train the \ac{LM} only on text transcriptions from the training data using Witten-Bell smoothing.\cite{witten1991zero}

\subsection{Speech decoding}
\label{sub:decode}
Speech \ac{HMM} decoders find the most probable word sequence by searching phone sequences which correspond to words. Phones are typically represented as triphones in an \ac{AM}.

Using a combination of \ac{AM} and \ac{LM} probabilities as described in Equation \ref{eq:best_fix} does not produce the most accurate speech transcriptions. Typically a \ac{LMW} $w_{lm}$ is used to improve speech recognition accuracy. It is tuned on the development set and balances the impact of the two models. Using the \ac{LMW}, the best word sequence is found according to Equation \ref{eq:am_lm}.

\begin{equation}\label{eq:am_lm}
  w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{P(a \mid w) * P(w)^{w_{lm}}\}
\end{equation}

\ac{ASR} is a pattern recognition task as well as a search problem. In speech recognition, making a search decision is also referred to as decoding.\cite{huang2001spoken}

For word recognition, the \ac{AM} limits the possible phone sequences to only words in the lexicon --- the words in the training data. Word recognition is nowadays the most successful form of \ac{ASR}. An \ac{HMM} sequence represents the phone sequence which forms a word, as illustrated on Figures \ref{fig:hmm_lm} and \ref{fig:hmm_alt}. Words are connected via \ac{HMM} states which represent inter-word silence.

For isolated word recognition the \acp{HMM} are evaluated for each word possibility. Using the forward algorithm for each \ac{HMM} $h_w$, we are able to compute the probability of every word $w$ given the acoustic observations. Isolated word recognition becomes a simple recognition problem, where we select the most probable \ac{HMM} $h^*$ from a finite set of word \acp{HMM}.

Note that \ac{HMM} training is identical for continuous \ac{ASR} and isolated word recognition, but decoding is more complicated for continuous \ac{ASR} where we aim to decode word sequences.

\begin{figure}[!htp]
  \begin{center}
    \input{images/hmm-lm}
    \caption{Diagram of how a \ac{LM} is combined with \acp{HMM}.}
    \label{fig:hmm_lm} 
  \end{center}
\end{figure}

Let us introduce a simple example of continuous word \ac{ASR}. Imagine an \ac{LM} of order 1 modelling only two words - \term{one} and \term{two} each uniformly distributed\footnote{If a \ac{LM} of order 1 assigns an equal probability to every word, we say it has order 0}. We want to decode any possible sequence of words \term{one}, \term{two}. The $\epsilon$ transition at the end of each word's \acp{HMM} to the final state $e$ allows us to introduce an \ac{HMM} silence model which connects the final state $e$ with the start state $s$. Consequently, words are chained using a silence \ac{HMM} model as illustrated in Figure~\ref{fig:hmm_lm}. An expanded monophone \ac{HMM} for words \term{one} and \term{two} is shown in Figure \ref{fig:hmm_alt}. Note that the \ac{LM} weight $P(w)$ can be stored on the $\epsilon$ transition at the beginning.

\begin{figure}[!htp]
  \begin{center}
    \input{images/hmm-alt}
    \caption{Expanded \acp{HMM} for the words \term{one} and \term{two}. 
      The arrows at \ac{HMM} states illustrate that every observation of acoustic features 
      can be generated according to the statistical distribution. 
      Note that if a speaker says \term{two}, a \ac{HMM} model with well trained parameters should output higher probability
      for the \ac{HMM} representing \term{two}. For longer speaker sequences e.g. \term{two one one two ...} the \acp{HMM} are connected over the $\epsilon$ transitions, and a search is used for selecting the most probable sequence.}
    \label{fig:hmm_alt} 
  \end{center}
\end{figure}

Even the simple \ac{HMM} network in Figure \ref{fig:hmm_alt} can become a large search problem partially because the search space of words grows exponentially in the number of words in the utterance and partially because the word boundaries are unknown. Each word can begin at any moment and last with decreasing probability \textit{ad infinitum}, so the search space explodes. In addition, higher order \acp{LM} increase the decoding complexity even more.

One can see that the search space of a speech recognition problem is enormous and still has to be solved in very short time for real-time applications. % The~most famous \ac{AI} search algorithm $A^*$ for large search problems cannot be used, because during decoding while the user speaks we know little about what is he going to say in the~rest of the utterance, so we cannot design a good heuristic function. With the $A^*$ algorithm we could not exploit the time when the user speaks to perform any computation. The user expects a response around 200 ms after his speech, so we need to find the best path in a very short time frame.

A natural choice for the one-best hypothesis is Viterbi beam search\cite{huang2001spoken}. It uses a dynamic programming algorithm to compute the new best partial hypotheses for new audio data based on partial hypotheses from the previous step. 

% \todo{picture beam search}

The Viterbi algorithm is a breadth-first search algorithm where a beam is used to limit the number of nodes which are expanded from the current set of nodes to next iteration. We list a few alternatives of how to set up the beam for speech decoding.

\begin{itemize}
\item \term{Fixed beam} guarantees maximum size of memory footprint and fast decoding.
\item \term{Relative one-best hypothesis comparison} effectively discards most of the improbable hypotheses if the one-best hypothesis is significantly better than the alternatives but it keeps a lot of alternatives if the one-best hypothesis is weak. The relative one-best hypothesis comparison naturally broadens the beam in uncertain regions, but does not guarantee any hard limits e.g., maximum number of nodes expanded.
\item \term{Combination} of methods applies the strictest criteria on beam in each iteration.
\end{itemize}

\subsubsection*{Numerical stability}
The hypotheses which are represented as paths of states are typically rather long in the search graph and a lot of hypothesis are assigned tiny probabilities. In order to ensure numeric stability, probabilities are expressed in logarithmic scale. 

% \todo{Misleading: The task of searching for the most probable path in a graph is transformed to the well-studied shortest path problem.}

In order to use the shortest distance measure to find the most probable path we use formula \ref{eq:am_lm_log} derived from equation \ref{eq:am_lm}. Both $C(a \mid w)$ and $C(w)$ are costs with ranges between zero and one, where a cost of one corresponds to zero probability $C(1) \cong P(0)$ and a cost of infinity corresponds to a probability of one $C(\infty) \cong P(1)$.

\begin{equation}\label{eq:am_lm_log}
  \begin{split}
    w^* &= argmin_{w}\{log(\frac{1}{C(a \mid w) * C(w)^{w_{lm}}})\} \\
    &= argmin_{w}\{-log(C(a \mid w) * C(w)^{w_{lm}})\} \\
    &= argmin_{w}\{-log(C(a \mid w)) - w_{lm}*log( C(w))\} 
  \end{split}
\end{equation}

\subsubsection*{Decoding formats}
The one-best hypothesis outputs only a single sequence of words despite the fact that other sequences of words are often almost as probable as the best hypothesis. Formats which are able to represent alternative hypothesis provide better results for further processing than one-best hypothesis because the alternatives may cover almost all probable hypotheses. We present n-best list and lattice formats, which both are able to represent alternative hypothesis.

\ml{n-best list}

The n-best list is an extension to the one-best hypothesis format. The most probable word sequence, along with the second, third, \ldots, n-th most probable hypothesis are included in an n-best list. 

\begin{figure}[!htp]
  \begin{center}
\begin{verbatim}
    0.5 hi how are you
    0.2 hi where are you
    0.1 bey how are you
\end{verbatim}
\caption{Example of a 3-best list output with posterior probability for each path. An n-best list in Kaldi can be easily extracted from lattices. Corresponding example lattice is in Figure \ref{fig:toy_lat}.}
\label{fig:nbest} 
  \end{center}
\end{figure}

% The n-best list and lattice are extracted from the beam history. Note that the Kaldi toolkit always generates state level lattices \cite{povey2012generating}\footnote{State level lattices uses states very similar to triphone states}. The~word lattice and later word n-best list are extracted from the~state level lattice where the~states roughly corresponds to triphones. 

\begin{figure}[!htp]
  \begin{center}
    \includegraphics[width=30em]{images/toy_lattice.ps}
    \caption{Word posterior lattice. 
      Common parts of hypotheses are effectively represented. 
      All outgoing arcs for each node sum to 1.0. }
    \label{fig:toy_lat} 
  \end{center}
\end{figure}

\ml{lattice}

A lattice is a convenient type of \ac{ASR} output. It effectively represents the alternative hypotheses by sharing their common parts. An example of a word lattice in Figure \ref{fig:toy_lat} shows a word posterior lattice. Each hypothesis is represented as sequence of arcs from starts to final node. The words and their weights are associated with the arcs. The posterior probability of a hypothesis is computed as the product of posterior probabilities of each word in the hypothesis.

It is useful to capture how the quality of each hypothesis contrasts to its alternatives or even provide an absolute quality measure. Typically, likelihood and posterior probability are associated with each word sequence to express its quality. The likelihood measure can be used only for relative comparison, whereas posterior probability is a normalised absolute measure. For some applications the likelihood measure is sufficient, other applications (for example dialogue systems) prefer posterior probabilities. Note that posterior probabilities for n-best lists typically do not sum to one and may need to be renormalised, because n-best list omits some hypotheses which are used to compute the posterior probability in lattices. See Figure \ref{fig:nbest} for such an example of a 3-best list.

\subsection[Evaluating \acs{ASR} quality]{Evaluating \acl{ASR} quality}
\label{sub:eval}
\ml{WER}

The~accuracy of a~speech recognizer is typically measured using \acl{WER}.
The~\ac{WER} measure is computed on one-best \ac{ASR} hypotheses and their human transcriptions.
The~\ac{WER} is computed as a~minimum edit distance on words between the~\ac{ASR} output and reference transcription.
Following edit operations are used \term{substitution, deletion, insertion} to compute the~minimum edit distance as illustrated in~\ref{eq:edit_dist}.
The~effective implementation for computing WER uses dynamic programing and is not computationally intensive because \ac{ASR} hypotheses are typically quite short.
\begin{equation} \label{eq:edit_dist}
    WER = 100* \frac{min\_dist(decoded_{AM, LM}(a), t, edit\_operation=\{Subs, Del, Ins\})}{\#\ words\ in\ t}
\end{equation}
\ml{reference}
Note that \ac{WER} is an error function so the~ideal value is zero because for $WER=0$ the~one-best hypothesis $decoded(a)$ and the~reference transcription $t$ are identical.
The~\ac{WER} value of 100 show that every single word is different between $decoded(a)$ and reference $t$ if the~number of words in \ac{ASR} output and reference are equal.
Despite the~fact that \ac{WER} resembles percentage format, it can be bigger than 100. 
See the~third example in~Figure~\ref{fig:wer400}.
\begin{figure}[!htp]
    \begin{center}
    \begin{verbatim}
        decoded(a) = 'hi hi hi hi'
        t='hi hi ha ha'
        WER = 100 * ( 2 / 4) = 50 

        decoded(a) = 'how do you do'
        t='how do you do''
        WER = 100 * ( 0 / 4) = 0

        decoded(a) = 'hi hi hi hi'
        t='hello'
        WER = 100 * ( 4 / 1) = 400
    \end{verbatim}
    \caption{\acs{WER} captures the~\ac{ASR} one-best hypotheses accuracy.}
    \label{fig:wer400} 
    \end{center}
\end{figure}

Note that the~data used for evaluation should not be used in \ac{AM} training because we are evaluating the~ability to decode unknown speech.
We should also measure the~\ac{ASR} quality on speech from a~speaker who does not appear in speech training data because we usually want to decode speech of an unheard speaker.

\subsubsection*{Alternative measures}
\ml{SER}
The~\acl{SER} measures how many decoded utterances $decoded(a)$ match exactly its reference $t$ for all pairs $(a, t)$ in test set $T$.
\begin{equation}
    SER = \frac{\sum_{\{(a, t) \in T; decoded(a) = t\}}{1}}{|T|}
\end{equation}

\ml{oracle WER}
If the~n-best list or lattice is used the~one-best hypothesis is extracted to compute \ac{WER} or \ac{SER}.
On the~other hand, we are using n-best lists or lattice because the~one-best hypothesis might be wrong and the~alternative hypothesis may be closer to the~reference.
In order to evaluate quality of alternative hypotheses one may use oracle \ac{WER} which reports the~WER of the~best hypotheses in n-best list or in lattice.
The~lattices with rich alternatives gain much lower oracle \ac{WER} than short n-best lists or even one-best hypotheses.
The~rich alternatives contain additional information and for example a~dialogue system \acl{SLU} component may exploit the~alternatives.

\subsubsection*{Measuring speed}
In this thesis we are especially concerned about the~speed of speech decoding because the~implemented decoder is used in a~real-time \acl{SDS}.

\ml{\acl{RTF}}
A~very natural measure of a~speech decoding speed is \acl{RTF}, which expresses how much the~recognizer decodes slower than the~user speaks.
We measure the~\ac{RTF} for each recording as described in~Equation~\ref{eq:rtf}.
\begin{equation}\label{eq:rtf}
    RTF = \frac{time(decode(a))}{length(a)}
\end{equation}
For real-time decoding in a~dialogue system we need smaller than one $RTF < 1.0$.
In other words, the~decoding of an utterance should take less time than a~user needed for pronouncing the~utterance.
With $RTF < 1.0$ the~hypothesis is decoded immediately after the~user finishes the~speech. 

The~decoding is performed while user is speaking, but extracting the~\ac{ASR} hypothesis output is triggered at the~very end of the~speech.
The~users have to wait at least the~time when the~\ac{ASR} hypotheses is extracted.

\ml{latency}
In real-time \ac{SDS} the~critical measure is a~delay how long the~user has to wait for its answer.
The~latency measures the~time between the~end of the~user speech and the~time when a~decoder returns the~hypothesis, which is the~most important speed measure for \ac{ASR} component in \ac{SDS}.
Note if $RTF < 1.0$ then the~latency corresponds to time of \ac{ASR} hypotheses extraction.


\section{\ac{HTK}}
\label{sec:back_htk}
The~\ac{HTK} toolkit is a~set of command line tools, sample scripts and library for training and decoding \ac{HMM} focused on speech recognition.
With the~toolkit are distributed two decoders \term{HVite} and \term{HDecode}, which are not designed for real-time applications.

Functionality of the~core library can be accessed through command line executables.
The~command line programs are typically combined in training scripts to train acoustic and language models.
In Figure~\ref{fig:htk_tools} the~acoustic models are labelled as "HMMs" and the~language models in \ac{HTK} are represented in "Networks".
The~trained models are used in one of \ac{HTK} decoders e.g. \term{HVite} for decoding transcriptions, which can be evaluated using \term{HResults}.

\begin{figure}[!htp]
    \begin{center}
    \includegraphics[width=20em]{images/htk_tools.ps}
    \caption{Figure 2.2 from HTK Book 3.4\cite{young2006htk}}
    \label{fig:htk_tools} 
    \end{center}
\end{figure}

The~\ac{HTK} library use Baum-Welch algorithm to train acoustic models.
The~\term{HVite} decoder uses token passing algorithm and Viterbi criterion.\cite{young2006htk} % chapter 2
Only unigram and bigram \acp{LM} can be used with \term{HVite}.
The~\\term{HDecode} decoder can handle bigram or trigram language models. 


Let us stress that we use high quality Bash and Perl scripts for training \ac{HTK} \ac{AM} from Vertanen improved by Matěj Korvas.\cite{vertanen_baseline_2006}\cite{korvas_2014}

The~\ac{HTK} toolkit is licensed under a~special license\footnote{\url{http://htk.eng.cam.ac.uk/docs/license.shtml}}.
The~\term{HDecode} has very similar license condition but can be only used for research purposes.\footnote{You need to register even to see the~license: \url{http://htk.eng.cam.ac.uk/prot-docs/hdecode_register.shtml}}

\section{Julius decoding engine}
\label{sec:back_julius}

% http://julius.sourceforge.jp/juliusbook/en/desc_overview.html
Julius is a~large vocabulary continuous speech decoder which can use \acp{AM} in \ac{HTK} format for decoding.\cite{lee2009julius}
Julius is BSD licensed\footnote{\url{http://www.linfo.org/bsdlicense.html}} and performs almost real-time decoding.

Julius is a~two pass decoder. 
In the~first pass, the~decoding is performed using time synchronous beam search.
The~second pass re-ranks and further prunes the~extracted hypothesis from the~pass one.
Bigram \ac{LM} is used for the~first pass and more complex trigram \ac{LM} is used for re-ranking.

Before the~implementation of this thesis was finished the~Alex \ac{SDS} team had been interested in Julius because its ability of real-time decoding and confusion network\footnote{A~confusion network is approximation of a~lattice described in~Section~\ref{sub:decode}.} output format.

The~Alex team abandoned the~Julius decoder for software issues e.g., crashes of the~decoder. 
The~crashes appeared during extracting confusion networks from Julius. 
In addition, the~crashes were hard to detect because Julius used to run in a~separate process.
% and the~dialogue system Alex were waiting for the~lattice output of dead process, which should had been passed through sockets.

\section{Kaldi}
\label{sec:back_kaldi}

Kaldi is a speech recognition toolkit consisting of a library, command line programs, and scripts for acoustic modelling.
Kaldi deploys several decoders for evaluating Kaldi \acp{AM}.
Kaldi uses Viterbi training for estimating \acp{AM}. 
Only in special cases of speaker adaptive discriminative training the~extended Baum-Welch algorithm is also used\cite{povey2011kaldi}.

The architecture of the~Kaldi toolkit can be separated into two parts: (1) the Kaldi library and (2) training scripts.
The~scripts access the~functionality of the Kaldi library through command line programs.
The~C++ Kaldi library is based on the~\term{OpenFST}\cite{allauzen2007openfst} library, and it uses optimized libraries for linear algebra such as BLAS and LAPACK.
Related functionality is usually grouped in one namespace in C++ code, which corresponds to one directory on file system. 
The~examples of the~namespaces or directories can be seen in Figure~\ref{fig:kaldi_arch}

\begin{figure}[!htp]
    \begin{center}
        \includegraphics[width=25em]{images/kaldi-lib}
        \caption{Kaldi toolkit architecture\cite{povey2011kaldi}}
        \label{fig:kaldi_arch} 
    \end{center}
\end{figure}

Kaldi uses executables which load input from files and typically store results again to files.
Alternatively, the~output of one Kaldi program can be fed into next command using system pipes.
There are usually many alternatives for every speech recognition tasks as seen in list of executables below:
\begin{enumerate}
    \item Speech parametrisation
        \begin{itemize}
            \item \term{apply-mfcc}
            \item \term{compute-mfcc-feats}
            \item \term{compute-plp-feats}
            \item \ldots
        \end{itemize}
    \item Feature transformation
        \begin{itemize}
            \item \term{apply-cmvn}
            \item \term{compute-cmvn-stats}
            \item \term{acc-lda}
            \item \term{fmpe-apply-transform}
            \item \ldots
        \end{itemize}
    \item Decoders
        \begin{itemize}
            \item \term{gmm-latgen-faster}
            \item \term{gmm-latgen-faster-parallel}
            \item \term{gmm-latgen-biglm-faster}
            \item \ldots
        \end{itemize}
    \item Evaluation and utilities
        \begin{itemize}
            \item compute-wer
            \item show-alignments
            \item \ldots
        \end{itemize}
\end{enumerate}
In addition, Kaldi provides very useful standardized scripts which wrap Kaldi executables or add new functionality. 
The~scripts are located in \term{utils} and \term{steps} directories and are used in many training scripts recipes for different corpus data.
In this thesis we created a~new training recipe using the~Kaldi infrastructure and Czech and English training corpus \cite{korvas_2014}.
The~recipe, the~data and acoustic modelling scripts are described in~Chapter~\ref{cha:train}.

\subsection{Finite State Transducers} 
\label{sec:fst}
The~\acl{FST} framework and its implementation in OpenFST determines the~shape of Kaldi data structures.
Kaldi uses \acp{FST} as the underlaying representation for \acp{LM}, partially for \acp{AM}, the lexicon, and also for representing transformation between text, pronunciation and triphones.

The~\ac{FST} framework provides well studied graph operations\cite{mohri2002weighted} which can be effectively used for acoustic modelling.
Using the~\ac{FST} framework, the~speech decoding task is expressed as a~beam search in a~graph, which is well studied problem.
% The~OpenFST library implements memory efficient representation of \ac{FST} and provides standardized efficient operations.
% As stated in \cite{mohri2002weighted} and \cite{povey2011kaldi} the~operations can be effectively used for speech decoding. 

The~\ac{FST} graphs used for \ac{AM} model training and speech decoding can be constructed as sequence of standardized OpenFST operations.\cite{mohri2002weighted}.
Decoding is performed on so called \term{decoding graph} $HCLG$ which is constructed from simple \ac{FST} graphs as illustrated in~Equation~\ref{eq:hclg}. 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.
The~symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
We briefly explain the~functionality of the~transducers from~Equation~\ref{eq:hclg}:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the~grammar or language model.
    \item L represents the~lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the~relationship between context-dependent phones on input and phones on output.
    \item H contains the~\ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi decoding graph is created using standard \ac{FST} operations\footnote{Kaldi tutorial on building $HCLG$: \url{http://kaldi.sourceforge.net/graph_recipe_test.html}}.\cite{mohri2002weighted}
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}

\ml{Semiring}
Most of the~operations operate on paths in the~decoding graph.
A path is a~sequence of edges which have weights, an input, and an output label.
Based on the~weight type and weight path operations we distinguish several semirings. 

Formally, a~\term{semiring} $(\mathcal{K}, \oplus, \otimes, \bar{0}, \bar{1})$ is an algebraic structure on set $\mathcal{K}$ with operations $\oplus$ and $\otimes$.
The~binary operations multiplication $\oplus$ and addition $\otimes$ have identity element $\bar{0}$ respectively $\bar{1}$. 
The~$(\mathcal{K}, \oplus)$ forms commutative monoid and $(\mathcal{K}, \otimes)$ forms just a~monoid.
The~multiplication is left and right distributive over addition.
Moreover, multiplication by $\bar{0}$ annihilates any member of $\mathcal{K}$ to $zero$. 
Table~\ref{tab:semiring} shows useful semirings in OpenFST.

\begin{table}[!htp]\label{tab:semiring}
\begin{center}
\begin{tabular}{lrrrrr}
\hline
Name & $\mathcal{K}$ & $\oplus$ & $ \otimes$ & $\bar{0}$ & $\bar{1}$ \\ 
\hline
Real        & $[0,\infty)$        &  +                     &  * &  0        &  1  \\
Log         & $(-\infty, \infty)$ & $-log(e^{-x} + e^{-y})$ & + &  $\infty$ &  0  \\
Tropical    & $(-\infty, \infty)$ &  min                   &  + &  $\infty$ &  0  \\
\hline
\end{tabular}
\caption{Semirings used in speech recognition.\cite{openfst_web}}
\end{center}
\end{table}
