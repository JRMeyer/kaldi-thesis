% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

%{{{
% TODO FIND THE BEST INTRODUCTION FROM:
%   Spoken language understanding
%   Discriminative Training and Acoustic Modeling for Speech Recognition
%   Markov Models for Pattern Recognition -Fink
% 
% TODO Watch NLP again on http://coursera.com check it for references for the good explanations
% 
% TODO one goal of the EXPECTED goals of writing thesis 
%       Show that you understand given topic
%       Show that you can write and talk about given topic
%}}}
\todo{Tady by asi spise davalo smysl.
2.1 Automatic speech recognition 
- Speech parameterization
- Acoustic modeling
- Language modeling
- Speech decoding
- Evaluation of the ASR quality 
2.2 KALDI
2.3 HTK
- a very brief description
2.4 Julius}
\todo{jake byly metody sumu ,  triphonem}

Research on field of speech recognition began more than 40 years ago. The statistical approach based 
on~\ac{HMM} established itself very early and nowadays it is de facto standard for continuous speech recognition. 
If we speak about speech recognition in~this thesis we mean continuous speech recognition based on Hidden Markov Models. 

Section~\ref{sec:general_introduction} provides an~introduction into speech recognition with \ac{HMM}. 
It concisely describes components of speech recognition system. The~algorithms for training acoustic language models 
as well for decoding are introduced. In~Section~\ref{sec:fst} the~advantages of Finite State Transducers 
in~speech recognition are discussed.

\section{Introduction to Speech recognition} 
\label{sec:general_introduction}
\ml{ASR hypothesis}
Let us gently introduce speech recognition. Automatic speech recognition is a process
that produces an \ac{ASR} hypothesis for given audio input. 
For simplicity, let us for suppose that ASR hypothesis is the textual transcription of speech in the input audio. 
In~Subsection~\ref{sub:lattice} we will describe lattices as another output format convenient for dialog systems.

%{{{ somehow finished
\subsection*{Speech recognition in a world of Artificial intelligence}
\label{sub:intro_ml}
Speech recognition is a classic machine learning technique and has many in common with other machine learning task. 
There are many algorithms and models which are used both in speech recognition and in other machine learning technique.
Let us name a few examples:
\begin{itemize}
    \item Markov models are used in speech recognition as well as in \ac{DNA} multiple alignment problem.
    \item Neural Networks are getting popular in speech recognition and are classic solution for hand written text recognition.
    \item Language models are used in machine translation and also in speech recognition. 
\end{itemize}

In machine learning there are usually distinguished two kinds of learning algorithms, supervised and unsupervised.

\begin{figure}[!htp]
    \begin{center}
    \input{images/supervised-general}
    \input{images/supervised-baum-welsh}
    \caption{Supervised learning idea and example}
    \label{fig:supervised} 
    \end{center}
\end{figure}
{\it Supervised learning}\/ is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
The inferred function is often called {\it model}. For speech recognition tasks the models, namely acoustic models and the language models, are stored in persistent memory before using them. 

Acoustic models are trained using supervised learning. Language models are trained using unsupervised training. 

\begin{figure}[!htp]
    \begin{center}
    \input{images/unsupervised-general}
    \input{images/unsupervised-lm}
    \caption{Unsupervised learning idea and example}
    \label{fig:unsupervised} 
    \end{center}
\end{figure}
{\it Unsupervised learning}\/ can be thought of as finding patterns in the input data beyond what would be considered 
pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application 
of~statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}
% subsection speech_recognition_as_machine_learning_task (end)

\subsection*{Statistical approach}
\label{sub:statistical_approach}
In the statistical approach we can describe speech recognition as classification problem where 
for a~given acoustic observations $a$ we search for the most probable sequence of~words $w^*$.

\begin{equation}\label{eq:asr}
    w^* = argmax_{w}\{P(w|a)\} 
\end{equation}

Note that an ideal acoustic model would exactly determined the probabilities for every $w$ and $a$ in~Equation~\ref{eq:acoustic}.
\begin{equation}\label{eq:acoustic}
    P(w|a)
\end{equation}
Similarly, an ideal language model will perfectly determined probability for any sequence of words $w$
\begin{equation}
    P(w)
\end{equation}

Having the ideal acoustic and language model we can find the most probable transcription $w*$ for given acoustic observations $a$ by using Bayes rule.
\begin{equation}
    P(w \mid a) = \frac{P(a \mid w) * P(w)}{P(a)}
\end{equation}

By realizing that the probability $P(a)$ of acoustic observations is constant we finish in~Equation~\ref{eq:ideal}.
\begin{align}\label{eq:ideal}
    argmax_w\{P(w \mid a)\} &= argmax_w \{\frac{P(a \mid w) * P(w)}{P(a)}\}\\
                            &= \{P(a \mid w) * P(w)\}
\end{align}
From Equation~\ref{eq:ideal} we know the probability for every transcription $w$. However, in order to obtain the most probable transcription $w*$ we still need to search the possibly infinite set of sequences of words, which represent valid transcriptions $w$. The~{\it search}\/ done by the $armax$ function in~Equation~\ref{eq:ideal} is in speech recognition called {\it decoding}\/ and is realized by the {\it decoder}\/ program. The~introduced components are depicted in~Figure~\ref{fig:components}.
\begin{figure}[!htp]
    \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognition system\cite{ney1990acoustic}}
    \label{fig:components} 
    \end{center}
\end{figure}
% subsection statistical_approach (end)



From the~statistical point of view we do not care how do we obtain the probabilities.
It does not matter to if we use supervised or unsupervised learning. 

In reality, the decoded ASR hypothesis are not perfect. The ASR hypothesis, in our example the textual transcriptions, 
commonly contain errors. The causes of the errors can be classified into two categories. 
Firstly, the errors can be caused by inaccurate algorithms used in computing 
the~probabilities for language and acoustic models. Secondly, we silently supposed in the Equation~\ref{eq:ideal}, 
that the~data used for training and testing were equally distributed. 
In practice, both kinds of errors occur.

\subsubsection*{The metrics in speech recognition}
\label{sub:the_metrics_in_speech_recognition}
In this thesis we are concerned about three aspects of speech decoder.
The speed, universality and quality of~\ac{ASR} hypothesis.

The universality of a decoder is not very well measurable and we will discuss it 
in~Chapter~\ref{cha:implementation} as an implementation property.

\ml{\acl{RTF}}
We will measure the speed of~the decoders by \acl{RTF} metric or we will express it by latency.
\ac{RTF} is defined as $\ac{RTF} = \frac{P}{D}$, where $P$ is time of~the processing the audio by a~decoder on input with length $D$. 

\ml{latency}
The latency is a measure of delay. The delay is measured for each utterance in seconds. 
The delay is measured between time of submitting the last audio for given utterance to the decoder and 
the~time of returning the \ac{ASR} hypothesis from decoder.

\ml{\acl{WER}}
The quality of decoded hypothesis will be measured by \ac{WER}.
Note, that \ac{WER} is a minimum edit operation distance on words between two texts, the textual hypothesis
and its reference.
We will compute it by $\frac{S+D+I}{N}$, where $S$ is number of substitutions,
$D$ is number of deletions, $I$ number of insertions to the reference. 
The $N$ denotes the number of words in reference.

The other formats to textual hypothesis like n-best lists and lattice
will be converted to textual hypothesis, because the reference for our training data is only available  
in~form of text audio transcriptions.
From n-best list we take the best textual hypothesis directly as the first item. From lattice
we extract the transcriptions by search.  

% subsection the_metrics_in_speech_recognition (end)

\section{Signal processing}
\label{sub:signal}
% FIXME consult kaldi website with what I have written
% http://kaldi.sourceforge.net/feat.html
\ml{acoustic features}
Signal processing is an~important first step for speech recognition. However, we cover signal processing very briefly, 
because the methods described in this subsection are de facto standard and most of the toolkits use exactly the same 
algorithms. The~output of the~signal processing are acoustic feature vectors. 


The most used feature vectors are \ac{MFCC} coefficients and \ac{PLP} features. The toolkits used in our dialog system, 
Kaldi and \ac{HTK} toolkit, produce almost exactly the same \ac{MFCC} coefficients for given audio 
input.\footnote{The~subtle differences are caused by implementation approaches, but does not effect the quality 
    of~\ac{MFCC} coefficients in~significant way.}

An interested reader can find comprehensive introduction into signal processing 
in~the~fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.


Let us give a~concrete example with of signal processing steps: 
\small{\begin{enumerate}
    \item The raw audio signal is at first converted from continuous to discrete signal by {\it sampling}.  % TODO describe sampling https://en.wikipedia.org/wiki/Sampling_(signal_processing)
    \item The~discrete signal is transformed into {\it frequency domain}\/ by~\ac{DFT} in overlapping frames.
    \item The frequency spectrum obtained in previous step is transformed onto the \href{https://en.wikipedia.org/wiki/Mel_scale}{mel scale}, using triangular overlapping filters.
    \item From the mel frequencies the logs of the powers are taken from each of the mel frequencies.
    \item At the end the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
\end{enumerate}
The {\it sampling frequency}\/ is typically 8000, 16000, 32000 or 44100 Hz and each of the samples is usually encoded in 8, 16 or 32 bits. In our experiments we use 16 kHz sampling frequency for 16 bit samples.  

The overlapping frames are used because \ac{DFT} is not accurate at the edges of frames. 
Windowing function like Hanning function applied on the signal removes the edge effect of~\ac{DFT}. 

For example,  the online Kaldi decoder uses $25ms$  frame length and the frames are shifted by $10ms$ for 16kHz sampling frequency. By multiplying the sample rate with frame length in seconds we find out that there are $16000 * 0.025 = 400$ samples in one frame. The frames are overlapping with $ 16000 * (0.025 - 0.01) = 240$ samples.

In Kaldi there is used by default 25 filters for 25 overlapping bins for~frequencies 
between 20 and $Sampling frequency / 2 = 8000$, which is according the~Nyquist theorem\cite{jerri1977shannon} 
the~highest frequency that can be exactly determined in \ac{DFT}.

It is common to use only eight to twelve lowest-order \ac{MFCC} coefficients. In Kaldi we are using twelve of them. 
Note that the zero-order coefficients just measure the total frame energy and are often discarded.
} % \small

\ml{sampling}
To conclude, the signal processing samples the continuous audio at regular interval intervals. 
Fixed number of consecutive samples forms a window. The~windows are usually overlapping.
\ml{feature window}
The~acoustic features are computed for each of the overlapping window. 
% }}} Finished

% section signal_processing (end)
\section{Acoustic modeling}
\label{sub:decoding_with_hmm}
Let us return to high level point of view to speech recognition from~Figure~\ref{fig:components}.
We described the signal processing unit and now we will focus on modeling the phonemes, the basic
units of speech. We will use the~statistical \ac{HMM} approach to phonemes modeling.

The~\ac{HMM} is a~very powerful statistical method of~characterizing the~observed data samples 
of~a~discrete-time series. Chapter 8, \cite{huang2001spoken}.
We will at first introduce {\it Markov Chain}, then \ac{HMM} will be defined. Later,  we will explain 
how are the acoustic and language models used in \ac{HMM} framework. 
In~the~Subsection~\ref{sub:dec_algorithm} we present the search algorithms for speech recognition.

\todo{acoustic modeling in Kaldi \url{http://kaldi.sourceforge.net/model.html}}

\subsection*{Markov Chain}
\label{ssub:markov_chain}
\todo{Spoken Language Processing page 366}

\subsection{\acl{HMM}}
\label{ssub:hmm}
\todo{describe Hmm for phones}

% subsection hmm (end)



% subsection markov_chain (end)


\subsection*{Decoding algorithm}
\label{sub:dec_algorithm}
The Viterbi algorithm is the~basic search algorithm for \ac{HMM}. 
Briefly
However

\todo{how online decoder works}

\subsubsection{Decoding with lattices}
\label{sub:lattice}
\todo{what are lattices, why is they are good, depth of lattices for dialog system,
problem of backward search for lattices}


% subsection lattice (end)

% subsection decoding_algorithm (end)


\section{Training acoustic models} 
\label{sec:train_ml}

This section describes the acoustic training in general and introduces the~maximum likelihood training methods.
The~maximum likelihood training is a base for discriminative methods for acoustic modeling, which are briefly
described in~Chapter~\ref{ref:training}.
In~Chapter~\ref{cha:training}, the focus is on comparing results obtained by numerous of training methods
and not on~explaining the methods.

\todo{Describe generative vs discriminative model}

\todo{clustering phones, trees, monophone and triphone training 
    \url{http://kaldi.sourceforge.net/tree_externals.html}
Shortly from Dans thesis extended Baum-Welsh}

% section training_acoustic_models (end)






% section general_introduction (end)

\section{Finite State Transducers} 
\label{sec:fst}
\todo{Introduction from:
 OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
 OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
 SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri}



\subsubsection*{\todo{Sources - cite them!}} % (fold)

\begin{itemize}
    \item \todo{Consult Vassil blogpost}
    \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
    \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
\end{itemize}

\subsection{Decoding graph construction in~Kaldi} % (fold)
% FIXME add source http://kaldi.sourceforge.net/graph.html
% FIXME consult Vassil blogpost
Kaldi uses \ac{FST} as underlaying representation for all models, which are used to decoding. Consequently, 
training and decoding models in Kaldi can be expressed as sequence of operations above \acp{FST}.

Decoding is performed using a final result of training, so called {\it decoding graph}. 
From the high level point of view,
during training we are constructing the decoding graph 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
Namely, the transducers appearing in~Equation~\ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the \ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding graph. 
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}
Let us explain the shortcuts in the list below. Note that the operation are described in detail
at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% The source code of these operations is in fstext and corresponding command-line program are in fstbin/
\begin{itemize}
    \item asl - Add self loops to \ac{FST}
    \item rds - Remove disambiguation symbols from \ac{FST}
    \item H' is \ac{FST} H without self loops
    \item min -\ac{FST} minimization
    \item $A\circ B$  - Composition of \ac{FST} $A$ and $B$.
    \item det - Determinization of \ac{FST}
\end{itemize}

{\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.


\subsubsection*{Kaldi decoders} % (fold)
\begin{itemize}
    \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
    \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$

\end{itemize}
% subsection Kaldi Framework (end)

% section finite_state_automata (end)


% chapter background (end)
