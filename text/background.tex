% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

% TODO FIND THE BEST INTRODUCTION FROM:
%   Spoken language understanding
%   Discriminative Training and Acoustic Modeling for Speech Recognition
%   Markov Models for Pattern Recognition -Fink
% 
% TODO Watch NLP again on http://coursera.com check it for references for the good explanations
% 
% TODO one goal of the EXPECTED goals of writing thesis 
%       Show that you understand given topic
%       Show that you can write and talk about given topic

Research on field of speech recognition began more than \todo{30} years ago. The statistical approach based on {\it Hidden Markov Models (HMM)} established itself very early and nowadays it is de facto standard. If we speak about speech recognition in~this thesis we mean speech recognition based on Hidden Markov Models. 

Section~\ref{sec:general_introduction} provides an~introduction into classical speech recognition. It concisely describes components of speech recognition system. Algorithms for training acoustic and language models as well for decoding are explained. In~Section~\ref{sec:fst} the speech recognition with Finite State Transducers is motivated.

\section{Speech recognition based on HMM} 
\label{sec:general_introduction}
Let us gently introduce speech recognition. We will focus on the algorithms and approaches used in next chapters. For simplicity,
let us suppose that the~output of speech recognition is the textual transcription of the input audio. In~Subsection~\ref{sub:lattice} we will define
lattices as other possible output format.



\subsection*{Speech recognition in word of Artificial intelligence}
\label{sub:intro_ml}
Speech recognition is a classic machine learning technique and has many in common with other machine learning task. There are many algorithms models which are used both in speech recognition and in other machine learning tasks.
Let us name a few examples:
\begin{itemize}
    \item Markov models are used in Viterbi algorithm as well as in DNA substring matching problem.
    \item Neural Networks are getting popular in speech recognition and are classic solution for hand written text recognition.
    \item Language models are used in machine translation and also in speech recognition. 
\end{itemize}

In machine learning there are usually distinguished two types of learning algorithms, supervised and unsupervised.

\begin{figure}[!htp]
    \begin{center}
    \input{images/supervised-general}
    \input{images/supervised-baum-welsh}
    \caption{Supervised learning idea and example}
    \label{fig:supervised} 
    \end{center}
\end{figure}
{\it Supervised learning} is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
The inferred function is often called {\it model}. For speech recognition tasks the models, namely acoustic models and the language models, are stored in persistent memory before using them. 

Acoustic models are trained using supervised learning. Language models are trained using unsupervised training. 

\begin{figure}[!htp]
    \begin{center}
    \input{images/unsupervised-general}
    \input{images/unsupervised-lm}
    \caption{Unsupervised learning idea and example}
    \label{fig:unsupervised} 
    \end{center}
\end{figure}
{\it Unsupervised learning} can be thought of as finding patterns in the input data beyond what would be considered pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application of statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}
% subsection speech_recognition_as_machine_learning_task (end)


\todo{Bayes rules: acoustic model $P(text|give acoustic signal)$,  LM $P(text)$}





\subsection[MFCC coefficients]{Mel-frequency cepstrum coefficients}
\label{sub:mel_frequency_cepstrum_coefficients}



% subsection mel_frequency_cepstrum_coefficients (end)
\subsection{Baum-Welsh algorithm}
\label{sub:baum_welsh_algorithm}



% subsection baum_welsh_algorithm (end)

\subsection{Viterbi algorithm}
\label{sub:viterbi_algorithm}

% subsection viterbi_algorithm (end)

\subsection{Other criteria}
\label{sub:other_criteria}

% subsection other_criteria (end)

\subsection{Decoding with lattices}
\label{sub:lattice}



% subsection lattice (end)


% section general_introduction (end)
\section{Finite State Transducers} 
\label{sec:fst}
% OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
% OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
% SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri

\subsection{Description}
\label{sub:description}

% subsection description (end)

\subsection{Kaldi implementation} % (fold)
\label{sec:kaldi}


\subsubsection*{Sources} % (fold)

\begin{itemize}
    \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
    \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
\end{itemize}

\subsubsection*{Decoding graph construction} % (fold)
Kaldi uses Finite-State Transducers (FST) as underlaying representation for all models, which are used to decoding. Consequently, training and decoding of models in Kaldi can be expressed as sequence of operations above FSTs.

Decoding is performed using a final result of training, so called {\it decoding graph}. 
From the high level point of view,
during training we are constructing the decoding graph 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on FST.
Namely, the transducers appearing in equation \ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the HMM definitions, which takes as input id number of Probability Density function (PDF) and returns context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding graph. 
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}
Let us explain the shortcuts in the list below. Note that the operation are described in detail
at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% The source code of these operations is in fstext and corresponding command-line program are in fstbin/
\begin{itemize}
    \item asl - Add self loops to FST
    \item rds - Remove disambiguation symbols from FST
    \item H' is FST H without self loops
    \item min - FST minimization
    \item $A\circ B$  - Composition of FSTs $A$ and $B$.
    \item det - Determinization of FST
\end{itemize}

{\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.


\subsubsection*{Kaldi decoders} % (fold)
\begin{itemize}
    \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
    \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$

\end{itemize}
% subsection Kaldi Framework (end)

% section finite_state_automata (end)


% chapter background (end)
